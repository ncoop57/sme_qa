{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2219,
     "status": "ok",
     "timestamp": 1562072364186,
     "user": {
      "displayName": "Nathan Cooper",
      "photoUrl": "",
      "userId": "15284233239426922637"
     },
     "user_tz": 300
    },
    "id": "v-FFfIovWj1P",
    "outputId": "9e48829f-e15d-4adb-96d8-0d91a34c4fd6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.0.0-beta1'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import fire\n",
    "import json\n",
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tqdm\n",
    "from tensorflow.core.protobuf import rewriter_config_pb2\n",
    "import glob\n",
    "import pickle\n",
    "\n",
    "import model\n",
    "from encoder import get_encoder\n",
    "\n",
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/tf/src/data/gpt-2\n"
     ]
    }
   ],
   "source": [
    "cd /tf/src/data/gpt-2/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "A_rmLotVXbbw"
   },
   "source": [
    "# Sample from Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "45t7syAbXaPb"
   },
   "outputs": [],
   "source": [
    "def sample_sequence(*, hparams, length, start_token=None, batch_size=None, context=None, temperature=1, top_k=0):\n",
    "    if start_token is None:\n",
    "        assert context is not None, 'Specify exactly one of start_token and context!'\n",
    "    else:\n",
    "        assert context is None, 'Specify exactly one of start_token and context!'\n",
    "        context = tf.fill([batch_size, 1], start_token)\n",
    "\n",
    "    def step(hparams, tokens, past=None):\n",
    "        lm_output = model.model(hparams=hparams, X=tokens, past=past, reuse=tf.compat.v1.AUTO_REUSE)\n",
    "\n",
    "        logits = lm_output['logits'][:, :, :hparams.n_vocab]\n",
    "        presents = lm_output['present']\n",
    "        presents.set_shape(model.past_shape(hparams=hparams, batch_size=batch_size))\n",
    "        return {\n",
    "            'logits': logits,\n",
    "            'presents': presents,\n",
    "        }\n",
    "\n",
    "    def body(past, prev, output):\n",
    "        next_outputs = step(hparams, prev, past=past)\n",
    "        logits = next_outputs['logits'][:, -1, :]  / tf.cast(temperature, dtype=tf.float32)\n",
    "        logits = top_k_logits(logits, k=top_k)\n",
    "        samples = tf.random.categorical(logits=logits, num_samples=1, dtype=tf.int32)\n",
    "        return [\n",
    "            next_outputs['presents'] if past is None else tf.concat([past, next_outputs['presents']], axis=-2),\n",
    "            samples,\n",
    "            tf.concat([output, samples], axis=1)\n",
    "        ]\n",
    "\n",
    "    past, prev, output = body(None, context, context)\n",
    "\n",
    "    def cond(*args):\n",
    "        return True\n",
    "\n",
    "    _, _, tokens = tf.while_loop(\n",
    "        cond=cond, body=body,\n",
    "        maximum_iterations=length - 1,\n",
    "        loop_vars=[\n",
    "            past,\n",
    "            prev,\n",
    "            output\n",
    "        ],\n",
    "        shape_invariants=[\n",
    "            tf.TensorShape(model.past_shape(hparams=hparams, batch_size=batch_size)),\n",
    "            tf.TensorShape([batch_size, None]),\n",
    "            tf.TensorShape([batch_size, None]),\n",
    "        ],\n",
    "        back_prop=False,\n",
    "    )\n",
    "\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Unconditionally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================== SAMPLE 1 ========================================\n",
      "======================================== SAMPLE 2 ========================================\n",
      "======================================== SAMPLE 3 ========================================\n",
      "======================================== SAMPLE 4 ========================================\n",
      "======================================== SAMPLE 5 ========================================\n",
      "======================================== SAMPLE 6 ========================================\n",
      "======================================== SAMPLE 7 ========================================\n",
      "======================================== SAMPLE 8 ========================================\n",
      "======================================== SAMPLE 9 ========================================\n",
      "======================================== SAMPLE 10 ========================================\n",
      "======================================== SAMPLE 11 ========================================\n",
      "======================================== SAMPLE 12 ========================================\n",
      "======================================== SAMPLE 13 ========================================\n",
      "======================================== SAMPLE 14 ========================================\n",
      "======================================== SAMPLE 15 ========================================\n",
      "======================================== SAMPLE 16 ========================================\n",
      "======================================== SAMPLE 17 ========================================\n",
      "======================================== SAMPLE 18 ========================================\n",
      "======================================== SAMPLE 19 ========================================\n",
      "======================================== SAMPLE 20 ========================================\n",
      "======================================== SAMPLE 21 ========================================\n",
      "======================================== SAMPLE 22 ========================================\n",
      "======================================== SAMPLE 23 ========================================\n",
      "======================================== SAMPLE 24 ========================================\n",
      "======================================== SAMPLE 25 ========================================\n",
      "======================================== SAMPLE 26 ========================================\n",
      "======================================== SAMPLE 27 ========================================\n",
      "======================================== SAMPLE 28 ========================================\n",
      "======================================== SAMPLE 29 ========================================\n",
      "======================================== SAMPLE 30 ========================================\n",
      "======================================== SAMPLE 31 ========================================\n",
      "======================================== SAMPLE 32 ========================================\n",
      "======================================== SAMPLE 33 ========================================\n",
      "======================================== SAMPLE 34 ========================================\n",
      "======================================== SAMPLE 35 ========================================\n",
      "======================================== SAMPLE 36 ========================================\n",
      "======================================== SAMPLE 37 ========================================\n",
      "======================================== SAMPLE 38 ========================================\n",
      "======================================== SAMPLE 39 ========================================\n",
      "======================================== SAMPLE 40 ========================================\n",
      "======================================== SAMPLE 41 ========================================\n",
      "======================================== SAMPLE 42 ========================================\n",
      "======================================== SAMPLE 43 ========================================\n",
      "======================================== SAMPLE 44 ========================================\n",
      "======================================== SAMPLE 45 ========================================\n",
      "======================================== SAMPLE 46 ========================================\n",
      "======================================== SAMPLE 47 ========================================\n",
      "======================================== SAMPLE 48 ========================================\n",
      "======================================== SAMPLE 49 ========================================\n",
      "======================================== SAMPLE 50 ========================================\n",
      "======================================== SAMPLE 51 ========================================\n",
      "======================================== SAMPLE 52 ========================================\n",
      "======================================== SAMPLE 53 ========================================\n",
      "======================================== SAMPLE 54 ========================================\n",
      "======================================== SAMPLE 55 ========================================\n",
      "======================================== SAMPLE 56 ========================================\n",
      "======================================== SAMPLE 57 ========================================\n",
      "======================================== SAMPLE 58 ========================================\n",
      "======================================== SAMPLE 59 ========================================\n",
      "======================================== SAMPLE 60 ========================================\n",
      "======================================== SAMPLE 61 ========================================\n",
      "======================================== SAMPLE 62 ========================================\n",
      "======================================== SAMPLE 63 ========================================\n",
      "======================================== SAMPLE 64 ========================================\n",
      "======================================== SAMPLE 65 ========================================\n",
      "======================================== SAMPLE 66 ========================================\n",
      "======================================== SAMPLE 67 ========================================\n",
      "======================================== SAMPLE 68 ========================================\n",
      "======================================== SAMPLE 69 ========================================\n",
      "======================================== SAMPLE 70 ========================================\n",
      "======================================== SAMPLE 71 ========================================\n",
      "======================================== SAMPLE 72 ========================================\n",
      "======================================== SAMPLE 73 ========================================\n",
      "======================================== SAMPLE 74 ========================================\n",
      "======================================== SAMPLE 75 ========================================\n",
      "======================================== SAMPLE 76 ========================================\n",
      "======================================== SAMPLE 77 ========================================\n",
      "======================================== SAMPLE 78 ========================================\n",
      "======================================== SAMPLE 79 ========================================\n",
      "======================================== SAMPLE 80 ========================================\n",
      "======================================== SAMPLE 81 ========================================\n",
      "======================================== SAMPLE 82 ========================================\n",
      "======================================== SAMPLE 83 ========================================\n",
      "======================================== SAMPLE 84 ========================================\n",
      "======================================== SAMPLE 85 ========================================\n",
      "======================================== SAMPLE 86 ========================================\n",
      "======================================== SAMPLE 87 ========================================\n",
      "======================================== SAMPLE 88 ========================================\n",
      "======================================== SAMPLE 89 ========================================\n",
      "======================================== SAMPLE 90 ========================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================== SAMPLE 91 ========================================\n",
      "======================================== SAMPLE 92 ========================================\n",
      "======================================== SAMPLE 93 ========================================\n",
      "======================================== SAMPLE 94 ========================================\n",
      "======================================== SAMPLE 95 ========================================\n",
      "======================================== SAMPLE 96 ========================================\n",
      "======================================== SAMPLE 97 ========================================\n",
      "======================================== SAMPLE 98 ========================================\n",
      "======================================== SAMPLE 99 ========================================\n",
      "======================================== SAMPLE 100 ========================================\n"
     ]
    }
   ],
   "source": [
    "def main(\n",
    "    model_name='unconditional_experiment',\n",
    "    seed=None,\n",
    "    nsamples=100,\n",
    "    batch_size=1,\n",
    "    length=None,\n",
    "    temperature=1,\n",
    "    top_k=0,\n",
    "    models_dir='checkpoint',\n",
    "):\n",
    "    \"\"\"\n",
    "    Run the sample_model\n",
    "    :model_name=117M : String, which model to use\n",
    "    :seed=None : Integer seed for random number generators, fix seed to\n",
    "     reproduce results\n",
    "    :nsamples=0 : Number of samples to return, if 0, continues to\n",
    "     generate samples indefinately.\n",
    "    :batch_size=1 : Number of batches (only affects speed/memory).\n",
    "    :length=None : Number of tokens in generated text, if None (default), is\n",
    "     determined by model hyperparameters\n",
    "    :temperature=1 : Float value controlling randomness in boltzmann\n",
    "     distribution. Lower temperature results in less random completions. As the\n",
    "     temperature approaches zero, the model will become deterministic and\n",
    "     repetitive. Higher temperature results in more random completions.\n",
    "    :top_k=0 : Integer value controlling diversity. 1 means only 1 word is\n",
    "     considered for each step (token), resulting in deterministic completions,\n",
    "     while 40 means 40 words are considered at each step. 0 (default) is a\n",
    "     special setting meaning no restrictions. 40 generally is a good value.\n",
    "     :models_dir : path to parent folder containing model subfolders\n",
    "     (i.e. contains the <model_name> folder)\n",
    "    \"\"\"\n",
    "    models_dir = os.path.expanduser(os.path.expandvars(models_dir))\n",
    "    enc = get_encoder(\"117M\", \"models\")\n",
    "    hparams = model.default_hparams()\n",
    "\n",
    "    if length is None:\n",
    "        length = hparams.n_ctx\n",
    "    elif length > hparams.n_ctx:\n",
    "        raise ValueError(\"Can't get samples longer than window size: %s\" % hparams.n_ctx)\n",
    "\n",
    "    with tf.compat.v1.Session(graph=tf.Graph()) as sess:\n",
    "        np.random.seed(seed)\n",
    "        tf.compat.v1.set_random_seed(seed)\n",
    "\n",
    "        output = sample_sequence(\n",
    "            hparams=hparams, length=length,\n",
    "            start_token=enc.encoder['<|endoftext|>'],\n",
    "            batch_size=batch_size,\n",
    "            temperature=temperature, top_k=top_k\n",
    "        )[:, 1:]\n",
    "\n",
    "        saver = tf.compat.v1.train.Saver()\n",
    "        ckpt = tf.compat.v1.train.latest_checkpoint(os.path.join(models_dir, model_name))\n",
    "        saver.restore(sess, ckpt)\n",
    "\n",
    "        generated = 0\n",
    "        while nsamples == 0 or generated < nsamples:\n",
    "            out = sess.run(output)\n",
    "            for i in range(batch_size):\n",
    "                samples = enc.decode(out[i]).split(\"<|endoftext|>\")\n",
    "                for i, sample in enumerate(samples):\n",
    "                    generated += 1\n",
    "                    print(\"=\" * 40 + \" SAMPLE \" + str(generated) + \" \" + \"=\" * 40)\n",
    "                    with open(\"/tf/src/manifoldAnalysis/output_space/unconditional/samples/sample-\" + str(generated), 'w') as f:\n",
    "                        f.write(sample)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "gpt2_tf2_new.ipynb",
   "provenance": [],
   "toc_visible": true,
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
