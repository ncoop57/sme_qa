{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Easily export jupyter cells to python module\n",
    "https://github.com/fastai/course-v3/blob/master/nbs/dl2/notebook2script.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted evaluation.ipynb to exp/nb_evaluation.py\r\n"
     ]
    }
   ],
   "source": [
    "! python /tf/main/src/scripts/notebook2script.py evaluation.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "# imports\n",
    "from fastai.text import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append(\"../../\")\n",
    "from src.proc.exp.nb_proc import *\n",
    "from src.prep.exp.nb_prep import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "# Evaluation metrics for vulnerability detection - Accuracy, Precision, Recall\n",
    "def eval_vuln(mdl, tst, sp, task, max_toks):\n",
    "    tps, tns, fps, fns = 0, 0, 0, 0\n",
    "    tot = 0\n",
    "    for inpt, lbl in zip(tst[\"query\"], tst[\"res\"]):\n",
    "        pred = get_clas_res(mdl, \"xxbos \" + inpt, sp, task, n_toks = max_toks)\n",
    "        if lbl == \"yes\":\n",
    "            if pred == lbl:\n",
    "                tps += 1\n",
    "            else: fns += 1\n",
    "        else:\n",
    "            if pred == lbl:\n",
    "                tns += 1\n",
    "            else: fps += 1\n",
    "                \n",
    "        tot += 1\n",
    "        torch.cuda.empty_cache()\n",
    "            \n",
    "    acc   = (tps + tns) / tot\n",
    "    prec  = tps / (tps + fps) if (tps + fps) != 0 else 0.\n",
    "    recal = tps / (tps + fns) if (tps + fns) != 0 else 0.\n",
    "    \n",
    "    return acc, prec, recal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "# Dependency downloads\n",
    "import nltk\n",
    "# required for meteor to perform similarity score, etc by looking for synonyms, antonyms...\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# samples\n",
    "reference1 = 'It is a guide to action that ensures that the military will forever heed Party commands'\n",
    "reference2 = 'It is the guiding principle which guarantees the military forces always being under the command of the Party'\n",
    "reference3 = 'It is the practical guide for the army always to heed the directions of the party'\n",
    "exact_reference = 'It is a guide to action which ensures that the military always obeys the commands of the party'\n",
    "references = [reference1, reference2, reference3, exact_reference]\n",
    "hypothesis1 = 'It is a guide to action which ensures that the military always obeys the commands of the party'\n",
    "hypothesis2 = 'It is to insure the troops forever hearing the activity guidebook that party direct'\n",
    "hypotheses = [hypothesis1, hypothesis2]\n",
    "bad_reference = ['this is a cat']\n",
    "bad_sentence = 'non matching hypothesis'\n",
    "near_hypotheses = ['Here is cat','This is a dog', 'This is a dog.', 'this is a cat']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# comments\n",
    "comment_ref = ['''/**\n",
    "* This is a cat\n",
    "* @param cat is a cat\n",
    "*/''']\n",
    "comment_gen = '''/**\n",
    "* This is a cat\n",
    "* @param cat is a cat\n",
    "*/'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bleu Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "from typing import List\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "\n",
    "def _eval_bleu(reference_texts: List[str], generated_text: str, weights: List[int]):\n",
    "    tokenized_references = [tokenizer.tokenize(reference) for reference in reference_texts]\n",
    "    tokenized_generated_text = tokenizer.tokenize(generated_text)\n",
    "    return round(sentence_bleu(\n",
    "        tokenized_references, \n",
    "        tokenized_generated_text, \n",
    "        weights=weights),\n",
    "        4)\n",
    "    \n",
    "def eval_bleu1(reference_texts: List[str], generated_text: str):\n",
    "    return _eval_bleu(reference_texts, \n",
    "                      generated_text, \n",
    "                      weights = (1,0,0,0))\n",
    "    \n",
    "def eval_bleu2(reference_texts: List[str], generated_text: str):\n",
    "    return _eval_bleu(reference_texts, \n",
    "                      generated_text, \n",
    "                      weights = (0.5,0.5,0,0))\n",
    "\n",
    "def eval_bleu3(reference_texts: List[str], generated_text: str):\n",
    "    return _eval_bleu(reference_texts, \n",
    "                      generated_text, \n",
    "                      weights = (0.33,0.33,0.33,0))\n",
    "\n",
    "def eval_bleu4(reference_texts: List[str], generated_text: str):\n",
    "    return _eval_bleu(reference_texts, \n",
    "                      generated_text, \n",
    "                      weights = (0.25,0.25,0.25,0.25))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample Bleu Score\n",
    "bleu_ref = ['this is small test']\n",
    "bleu_cand = 'this is a test'\n",
    "print(eval_bleu1(bleu_ref, bleu_cand))\n",
    "print(eval_bleu2(bleu_ref, bleu_cand))\n",
    "print(eval_bleu3(bleu_ref, bleu_cand))\n",
    "print(eval_bleu4(bleu_ref, bleu_cand))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.75\n",
      "0.7071\n",
      "0.6329\n",
      "0.0\n"
     ]
    }
   ],
   "source": [
    "# Sample Bleu Score for case\n",
    "bleu_ref = ['this is small test']\n",
    "bleu_cand = 'this is small Test'\n",
    "print(eval_bleu1(bleu_ref, bleu_cand))\n",
    "print(eval_bleu2(bleu_ref, bleu_cand))\n",
    "print(eval_bleu3(bleu_ref, bleu_cand))\n",
    "print(eval_bleu4(bleu_ref, bleu_cand))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Meteor Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "from nltk.translate.meteor_score import meteor_score\n",
    "def eval_meteor(reference_texts: List[str], generated_text: str):\n",
    "    return round(meteor_score(reference_texts, generated_text, preprocess=str.lower), 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#good hypotheses\n",
    "[eval_meteor(references, hypothesis) for hypothesis in hypotheses]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bad hypothesis\n",
    "round(meteor_score(bad_reference, bad_sentence), 4),  eval_meteor(bad_reference, bad_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9922"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# case test meteor\n",
    "eval_meteor(['This is a cat'], 'this is a cat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_meteor(bad_reference, 'this is a cat * * \\n   \\t  ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# comment_ref\n",
    "eval_meteor(comment_ref, comment_gen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# near hypotheses\n",
    "[eval_meteor(bad_reference, near_hypothesis) for near_hypothesis in near_hypotheses]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rouge-L Metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#setup \n",
    "!pip uninstall -y py-rouge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "import rouge\n",
    "import pandas as pd\n",
    "\n",
    "def _eval_rougeL_single_ref(reference_text: str, generated_text: str):\n",
    "    evaluator = rouge.Rouge(metrics=['rouge-l'],\n",
    "                           max_n=4,\n",
    "#                            limit_length=True,\n",
    "#                            length_limit=100,\n",
    "#                            length_limit_type='words',\n",
    "                           apply_avg=0,\n",
    "                           apply_best=0,\n",
    "                           alpha=0.5, # Default F1_score\n",
    "                           weight_factor=1.2,\n",
    "                           stemming=True)\n",
    "    # scores = evaluator.get_scores(all_hypothesis, all_references)\n",
    "    # watch out, it takes hypothesis first and then references.\n",
    "    score = evaluator.get_scores(generated_text, reference_text)['rouge-l'][0]\n",
    "    score_p = score['p'][0]\n",
    "    score_f = score['f'][0]\n",
    "    score_r = score['r'][0]\n",
    "    return [score_p, score_r, score_f]\n",
    "\n",
    "def eval_rougeL_single_ref(reference_text: List[str], generated_text: str):\n",
    "    evaluator = rouge.Rouge(metrics=['rouge-l'],\n",
    "                           max_n=4,\n",
    "#                            limit_length=True,\n",
    "#                            length_limit=100,\n",
    "#                            length_limit_type='words',\n",
    "                           apply_avg=0,\n",
    "                           apply_best=0,\n",
    "                           alpha=0.5, # Default F1_score\n",
    "                           weight_factor=1.2,\n",
    "                           stemming=True)\n",
    "    # scores = evaluator.get_scores(all_hypothesis, all_references)\n",
    "    # watch out, it takes hypothesis first and then references.\n",
    "    score = evaluator.get_scores(generated_text, reference_text[0])['rouge-l'][0]\n",
    "    score_p = score['p'][0]\n",
    "    score_f = score['f'][0]\n",
    "    score_r = score['r'][0]\n",
    "    return (score_p, score_r, score_f)\n",
    "\n",
    "def eval_rougeL(reference_texts: List[str], generated_text: str):\n",
    "    scores = [\n",
    "        _eval_rougeL_single_ref(\n",
    "            reference, \n",
    "            generated_text) \n",
    "        for reference in reference_texts]\n",
    "#     return scores\n",
    "    result_df = pd.DataFrame(scores)\n",
    "    # be extra careful, mislabeling is going to be really damaging.\n",
    "    result_df.columns=['p', 'r', 'f']\n",
    "    return result_df\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1.0, 1.0, 1.0)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# rouge L case test\n",
    "\n",
    "eval_rougeL_single_ref([\"This is a cat\"], \"this is a cat\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'eval_rougeL_single_ref' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-65290ec8c7e8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0meval_rougeL_single_ref\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"This is a cat\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"this is a cat\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'eval_rougeL_single_ref' is not defined"
     ]
    }
   ],
   "source": [
    "eval_rougeL_single_ref([\"This is a cat\"], \"this is a cat\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def eval_txt(mdl, ds, sp, task, max_toks):\n",
    "    b1, b2, b3, b4 = [], [], [], []\n",
    "    meteor = []\n",
    "    rouge_l = []\n",
    "    levenshtein = []\n",
    "    cosine = []\n",
    "    jaccard = []\n",
    "    preds = []\n",
    "    tokenizer = Tokenizer()\n",
    "    for inpt, lbl in zip(ds[\"query\"], ds[\"res\"]):\n",
    "        pred = get_seq_res(mdl, \"xxbos \" + inpt, sp, task, n_toks = max_toks)\n",
    "        \n",
    "        lbl = ' '.join(lbl.split())\n",
    "        preds.append((pred, lbl))\n",
    "        \n",
    "        # bleu 1-4\n",
    "        b1.append(eval_bleu1([lbl], pred))\n",
    "        b2.append(eval_bleu2([lbl], pred))\n",
    "        b3.append(eval_bleu3([lbl], pred))\n",
    "        b4.append(eval_bleu4([lbl], pred))\n",
    "        \n",
    "        # meteor\n",
    "        meteor.append(eval_meteor([lbl], pred))\n",
    "        \n",
    "        # Levenshtein\n",
    "        levenshtein.append(levenshtein_distance_score(lbl, pred))\n",
    "        \n",
    "        # Similarities\n",
    "        cosine.append(get_cosine_sim(lbl, pred))\n",
    "        jaccard.append(get_jaccard_sim(lbl, pred))\n",
    "        \n",
    "        # rouge\n",
    "        rouge_l.append(eval_rougeL_single_ref([lbl], pred))\n",
    "        \n",
    "    return b1, b2, b3, b4, meteor, rouge_l, levenshtein, cosine, jaccard, preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "# Grabs entire model's response up until special xxbos token,\n",
    "# i.e. once model begins a new sentence we consider the model finished with its answer.\n",
    "def get_res(mdl, inpt, sp, task, n_toks = 1_000, greedy = False):\n",
    "    if greedy:\n",
    "        res = mdl.beam_search(inpt, n_words = n_toks, beam_sz = 1, top_k = 1).split(\" \")\n",
    "        res = sp.DecodePieces(res).split(\" \")[1:]\n",
    "    else:\n",
    "        res = mdl.predict(inpt, n_toks, temperature=0.75).split(\" \")\n",
    "        res = sp.DecodePieces(res).split(\" \")\n",
    "    \n",
    "    try:\n",
    "        end_res = res.index(\"xxbos\")\n",
    "    except:\n",
    "        end_res = len(res) - 1\n",
    "    \n",
    "    res = \" \".join(res[:end_res])\n",
    "    res = res[res.find(task) + len(task):]\n",
    "    \n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "# Grabs entire model's response up until special xxbos token for a sequence task,\n",
    "# i.e. once model begins a new sentence we consider the model finished with its answer.\n",
    "def get_seq_res(mdl, inpt, sp, task, n_toks = 1_000):\n",
    "    res = mdl.predict(inpt, n_toks, temperature=0.75).split(\" \")\n",
    "    res = sp.DecodePieces(res).replace(task, \" \").split(\" \")[1:]\n",
    "    \n",
    "    try:\n",
    "        end_res = res.index(\"xxbos\")\n",
    "    except:\n",
    "        end_res = len(res) - 1\n",
    "        \n",
    "    res = decode_spec_tokens(res[:end_res])\n",
    "    res = \" \".join(res[:end_res])\n",
    "    \n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "# Grabs entire model's response up until special xxbos token for a classification task,\n",
    "# i.e. once model begins a new sentence we consider the model finished with its answer.\n",
    "def get_clas_res(mdl, inpt, sp, task, n_toks = 10):\n",
    "    res = mdl.beam_search(inpt, n_words = n_toks, beam_sz = 1, top_k = 1).split(\" \")\n",
    "    res = sp.DecodePieces(res).split(\" \")[2:]\n",
    "    \n",
    "    try:\n",
    "        end_res = res.index(\"xxbos\")\n",
    "    except:\n",
    "        end_res = len(res) - 1\n",
    "    \n",
    "    res = \" \".join(res[:end_res])\n",
    "    res = res[res.find(task) + len(task):]\n",
    "    \n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hypothesis_1 = \"King Norodom Sihanouk has declined requests to chair a summit of Cambodia 's top political leaders , saying the meeting would not bring any progress in deadlocked negotiations to form a government .\\nGovernment and opposition parties have asked King Norodom Sihanouk to host a summit meeting after a series of post-election negotiations between the two opposition groups and Hun Sen 's party to form a new government failed .\\nHun Sen 's ruling party narrowly won a majority in elections in July , but the opposition _ claiming widespread intimidation and fraud _ has denied Hun Sen the two-thirds vote in parliament required to approve the next government .\\n\"\n",
    "references_1 = [\"Prospects were dim for resolution of the political crisis in Cambodia in October 1998.\\nPrime Minister Hun Sen insisted that talks take place in Cambodia while opposition leaders Ranariddh and Sam Rainsy, fearing arrest at home, wanted them abroad.\\nKing Sihanouk declined to chair talks in either place.\\nA U.S. House resolution criticized Hun Sen's regime while the opposition tried to cut off his access to loans.\\nBut in November the King announced a coalition government with Hun Sen heading the executive and Ranariddh leading the parliament.\\nLeft out, Sam Rainsy sought the King's assurance of Hun Sen's promise of safety and freedom for all politicians.\",\n",
    "                    \"Cambodian prime minister Hun Sen rejects demands of 2 opposition parties for talks in Beijing after failing to win a 2/3 majority in recent elections.\\nSihanouk refuses to host talks in Beijing.\\nOpposition parties ask the Asian Development Bank to stop loans to Hun Sen's government.\\nCCP defends Hun Sen to the US Senate.\\nFUNCINPEC refuses to share the presidency.\\nHun Sen and Ranariddh eventually form a coalition at summit convened by Sihanouk.\\nHun Sen remains prime minister, Ranariddh is president of the national assembly, and a new senate will be formed.\\nOpposition leader Rainsy left out.\\nHe seeks strong assurance of safety should he return to Cambodia.\\n\",\n",
    "                    ]\n",
    "\n",
    "hypothesis_2 = \"China 's government said Thursday that two prominent dissidents arrested this week are suspected of endangering national security _ the clearest sign yet Chinese leaders plan to quash a would-be opposition party .\\nOne leader of a suppressed new political party will be tried on Dec. 17 on a charge of colluding with foreign enemies of China '' to incite the subversion of state power , '' according to court documents given to his wife on Monday .\\nWith attorneys locked up , harassed or plain scared , two prominent dissidents will defend themselves against charges of subversion Thursday in China 's highest-profile dissident trials in two years .\\n\"\n",
    "references_2 = \"Hurricane Mitch, category 5 hurricane, brought widespread death and destruction to Central American.\\nEspecially hard hit was Honduras where an estimated 6,076 people lost their lives.\\nThe hurricane, which lingered off the coast of Honduras for 3 days before moving off, flooded large areas, destroying crops and property.\\nThe U.S. and European Union were joined by Pope John Paul II in a call for money and workers to help the stricken area.\\nPresident Clinton sent Tipper Gore, wife of Vice President Gore to the area to deliver much needed supplies to the area, demonstrating U.S. commitment to the recovery of the region.\\n\"\n",
    "\n",
    "all_hypothesis = [hypothesis_1, hypothesis_2]\n",
    "all_references = [references_1, references_2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = eval_rougeL(references, hypothesis1)\n",
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = eval_rougeL(comment_ref, comment_gen)\n",
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score.mean(axis=0), score.std(axis=0), score.median(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score.std(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "# priya\n",
    "\n",
    "from collections import Counter\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "\n",
    "from collections import Counter\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def _get_vectors(*strs):\n",
    "    text = [t for t in strs]\n",
    "    vectorizer = CountVectorizer(text)\n",
    "    vectorizer.fit(text)\n",
    "    return vectorizer.transform(text).toarray()\n",
    "\n",
    "def get_cosine_sim(reference_txt:str, generated_txt:str): \n",
    "    vectors = [t for t in _get_vectors(reference_txt, generated_txt)]\n",
    "    return round(cosine_similarity(vectors[0:1], vectors)[0][1],4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_cosine_sim(\"this is a cat\", \"this is a bat\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "# priya\n",
    "\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import wordnet\n",
    "lmtzr = WordNetLemmatizer()\n",
    "\n",
    "def _get_wordnet_pos(treebank_tag):\n",
    "\n",
    "    if treebank_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif treebank_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif treebank_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif treebank_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return wordnet.NOUN\n",
    "\n",
    "def _lemmatizeSentence(strr):\n",
    "    return ([lmtzr.lemmatize(word.lower(), _get_wordnet_pos(word)) for word in word_tokenize(strr)])\n",
    "\n",
    "def get_jaccard_sim(str1, str2):\n",
    "    a = set(_lemmatizeSentence(str1))\n",
    "    b = set(_lemmatizeSentence(str2))\n",
    "    c = a.intersection(b)\n",
    "    return round(float(len(c)) / (len(a) + len(b) - len(c)),4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3846"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_jaccard_sim('ai is our friend friend friend and it has been friendly cry', 'AI and humans have always been friendly crying')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "# # baaler metric.\n",
    "\n",
    "# !pip install editdistance\n",
    "import editdistance\n",
    "\n",
    "def levenshtein_distance_score(reference_txt: str, generated_txt: str):\n",
    "    return round(editdistance.eval(reference_txt.split(), generated_txt.split()), 4)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "levenshtein_distance_score(\"this is a cat\", \"this is a bat\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "levenshtein_distance_score(\"this is a cat\", \"is a cat this\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "levenshtein_distance_score(\"this is a cat\", \"this is a \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "levenshtein_distance_score(\"this is a cat\", \"this is a Tom\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
