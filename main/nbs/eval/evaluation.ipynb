{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Easily export jupyter cells to python module\n",
    "https://github.com/fastai/course-v3/blob/master/nbs/dl2/notebook2script.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted evaluation.ipynb to exp/nb_evaluation.py\r\n"
     ]
    }
   ],
   "source": [
    "! python /tf/main/src/scripts/notebook2script.py evaluation.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "from fastai.text import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append(\"../../\")\n",
    "from src.proc.exp.nb_proc import *\n",
    "from src.prep.exp.nb_prep import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "# Evaluation metrics for vulnerability detection - Accuracy, Precision, Recall\n",
    "def eval_vuln(mdl, tst):\n",
    "    tps, tns, fps, fns = 0, 0, 0, 0\n",
    "    print(\"Hello World\")\n",
    "    for inpt, lbl in zip(tst[\"query\"], tst[\"res\"]):\n",
    "        print(lbl)\n",
    "        pred = mdl.predict(inpt, 1, temperature=0.75)\n",
    "        if lbl == \"yes\":\n",
    "            if pred == lbl:\n",
    "                tps += 1\n",
    "            else: fns += 1\n",
    "        else:\n",
    "            if pred == lbl:\n",
    "                tns += 1\n",
    "            else: fps += 1\n",
    "            \n",
    "    acc   = (tps + tns) / len(tst)\n",
    "    prec  = tps / (tps + fps)\n",
    "    recal = tps / (tps + fns)\n",
    "    \n",
    "    return acc, prec, recal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Dependency downloads\n",
    "import nltk\n",
    "# required for meteor to perform similarity score, etc by looking for synonyms, antonyms...\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "# samples\n",
    "reference1 = 'It is a guide to action that ensures that the military will forever heed Party commands'\n",
    "reference2 = 'It is the guiding principle which guarantees the military forces always being under the command of the Party'\n",
    "reference3 = 'It is the practical guide for the army always to heed the directions of the party'\n",
    "exact_reference = 'It is a guide to action which ensures that the military always obeys the commands of the party'\n",
    "references = [reference1, reference2, reference3, exact_reference]\n",
    "hypothesis1 = 'It is a guide to action which ensures that the military always obeys the commands of the party'\n",
    "hypothesis2 = 'It is to insure the troops forever hearing the activity guidebook that party direct'\n",
    "hypotheses = [hypothesis1, hypothesis2]\n",
    "bad_reference = ['this is a cat']\n",
    "bad_sentence = 'non matching hypothesis'\n",
    "near_hypotheses = ['Here is cat','This is a dog', 'This is a dog.', 'this is a cat']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bleu Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install nltk\n",
    "from typing import List\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "\n",
    "def _eval_bleu(reference_texts: List[str], generated_text: str, weights: List[int]):\n",
    "    tokenized_references = [tokenizer.tokenize(reference) for reference in reference_texts]\n",
    "    tokenized_generated_text = tokenizer.tokenize(generated_text)\n",
    "    return round(sentence_bleu(\n",
    "        tokenized_references, \n",
    "        tokenized_generated_text, \n",
    "        weights=weights),\n",
    "        4)\n",
    "    \n",
    "def eval_bleu1(reference_texts: List[str], generated_text: str):\n",
    "    return _eval_bleu(reference_texts, \n",
    "                      generated_text, \n",
    "                      weights = (1,0,0,0))\n",
    "    \n",
    "def eval_bleu2(reference_texts: List[str], generated_text: str):\n",
    "    return _eval_bleu(reference_texts, \n",
    "                      generated_text, \n",
    "                      weights = (0.5,0.5,0,0))\n",
    "\n",
    "def eval_bleu3(reference_texts: List[str], generated_text: str):\n",
    "    return _eval_bleu(reference_texts, \n",
    "                      generated_text, \n",
    "                      weights = (0.33,0.33,0.33,0))\n",
    "\n",
    "def eval_bleu4(reference_texts: List[str], generated_text: str):\n",
    "    return _eval_bleu(reference_texts, \n",
    "                      generated_text, \n",
    "                      weights = (0.25,0.25,0.25,0.25))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.75\n",
      "0.5\n",
      "0.0\n",
      "0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/nltk/translate/bleu_score.py:523: UserWarning: \n",
      "The hypothesis contains 0 counts of 3-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "/usr/local/lib/python3.6/dist-packages/nltk/translate/bleu_score.py:523: UserWarning: \n",
      "The hypothesis contains 0 counts of 4-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n"
     ]
    }
   ],
   "source": [
    "# Sample Bleu Score\n",
    "bleu_ref = ['this is small test']\n",
    "bleu_cand = 'this is a test'\n",
    "print(eval_bleu1(bleu_ref, bleu_cand))\n",
    "print(eval_bleu2(bleu_ref, bleu_cand))\n",
    "print(eval_bleu3(bleu_ref, bleu_cand))\n",
    "print(eval_bleu4(bleu_ref, bleu_cand))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Meteor Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.translate.meteor_score import meteor_score\n",
    "def eval_meteor(reference_texts: List[str], generated_text: str):\n",
    "    return round(meteor_score(reference_texts, generated_text, preprocess=str.lower), 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.9999, 0.3696]"
      ]
     },
     "execution_count": 225,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#good hypotheses\n",
    "[eval_meteor(references, hypothesis) for hypothesis in hypotheses]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.0, 0.0)"
      ]
     },
     "execution_count": 228,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# bad hypothesis\n",
    "round(meteor_score(bad_reference, bad_sentence), 4),  eval_meteor(bad_reference, bad_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9922"
      ]
     },
     "execution_count": 230,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_meteor(bad_reference, 'this is a cat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.2564, 0.7361, 0.7361, 0.9922]"
      ]
     },
     "execution_count": 231,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# near hypotheses\n",
    "[eval_meteor(bad_reference, near_hypothesis) for near_hypothesis in near_hypotheses]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rouge-L Metric - Incomplete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting py-rouge\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9c/1d/0bdbaf559fb7afe32308ebc84a2028600988212d7eb7fb9f69c4e829e4a0/py_rouge-1.1-py3-none-any.whl (56kB)\n",
      "\u001b[K     |████████████████████████████████| 61kB 2.7MB/s eta 0:00:011\n",
      "\u001b[?25hInstalling collected packages: py-rouge\n",
      "Successfully installed py-rouge-1.1\n",
      "\u001b[33mWARNING: You are using pip version 19.2.3, however version 19.3.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "#setup \n",
    "!pip install py-rouge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [],
   "source": [
    "import rouge\n",
    "def _eval_rougeL_single_ref(reference_text: str, generated_text: str):\n",
    "    evaluator = rouge.Rouge(metrics=['rouge-l'],\n",
    "                           max_n=4,\n",
    "#                            limit_length=True,\n",
    "#                            length_limit=100,\n",
    "#                            length_limit_type='words',\n",
    "                           apply_avg=0,\n",
    "                           apply_best=0,\n",
    "                           alpha=0.5, # Default F1_score\n",
    "                           weight_factor=1.2,\n",
    "                           stemming=True)\n",
    "    # scores = evaluator.get_scores(all_hypothesis, all_references)\n",
    "    # watch out, it takes hypothesis first and then references.\n",
    "    score = evaluator.get_scores(generated_text, reference_text)['rouge-l'][0]\n",
    "    score_p = score['p'][0]\n",
    "    score_f = score['f'][0]\n",
    "    score_r = score['r'][0]\n",
    "    return [score_p, score_r, score_f]\n",
    "\n",
    "def eval_rougeL(reference_texts: List[str], generated_text: str):\n",
    "    scores = [\n",
    "        _eval_rougeL_single_ref(\n",
    "            reference, \n",
    "            generated_text) \n",
    "        for reference in reference_texts]\n",
    "#     return scores\n",
    "    result_df = pd.DataFrame(scores)\n",
    "    # be extra careful, mislabeling is going to be really damaging.\n",
    "    result_df.columns=['p', 'r', 'f']\n",
    "    return result_df\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "hypothesis_1 = \"King Norodom Sihanouk has declined requests to chair a summit of Cambodia 's top political leaders , saying the meeting would not bring any progress in deadlocked negotiations to form a government .\\nGovernment and opposition parties have asked King Norodom Sihanouk to host a summit meeting after a series of post-election negotiations between the two opposition groups and Hun Sen 's party to form a new government failed .\\nHun Sen 's ruling party narrowly won a majority in elections in July , but the opposition _ claiming widespread intimidation and fraud _ has denied Hun Sen the two-thirds vote in parliament required to approve the next government .\\n\"\n",
    "references_1 = [\"Prospects were dim for resolution of the political crisis in Cambodia in October 1998.\\nPrime Minister Hun Sen insisted that talks take place in Cambodia while opposition leaders Ranariddh and Sam Rainsy, fearing arrest at home, wanted them abroad.\\nKing Sihanouk declined to chair talks in either place.\\nA U.S. House resolution criticized Hun Sen's regime while the opposition tried to cut off his access to loans.\\nBut in November the King announced a coalition government with Hun Sen heading the executive and Ranariddh leading the parliament.\\nLeft out, Sam Rainsy sought the King's assurance of Hun Sen's promise of safety and freedom for all politicians.\",\n",
    "                    \"Cambodian prime minister Hun Sen rejects demands of 2 opposition parties for talks in Beijing after failing to win a 2/3 majority in recent elections.\\nSihanouk refuses to host talks in Beijing.\\nOpposition parties ask the Asian Development Bank to stop loans to Hun Sen's government.\\nCCP defends Hun Sen to the US Senate.\\nFUNCINPEC refuses to share the presidency.\\nHun Sen and Ranariddh eventually form a coalition at summit convened by Sihanouk.\\nHun Sen remains prime minister, Ranariddh is president of the national assembly, and a new senate will be formed.\\nOpposition leader Rainsy left out.\\nHe seeks strong assurance of safety should he return to Cambodia.\\n\",\n",
    "                    ]\n",
    "\n",
    "hypothesis_2 = \"China 's government said Thursday that two prominent dissidents arrested this week are suspected of endangering national security _ the clearest sign yet Chinese leaders plan to quash a would-be opposition party .\\nOne leader of a suppressed new political party will be tried on Dec. 17 on a charge of colluding with foreign enemies of China '' to incite the subversion of state power , '' according to court documents given to his wife on Monday .\\nWith attorneys locked up , harassed or plain scared , two prominent dissidents will defend themselves against charges of subversion Thursday in China 's highest-profile dissident trials in two years .\\n\"\n",
    "references_2 = \"Hurricane Mitch, category 5 hurricane, brought widespread death and destruction to Central American.\\nEspecially hard hit was Honduras where an estimated 6,076 people lost their lives.\\nThe hurricane, which lingered off the coast of Honduras for 3 days before moving off, flooded large areas, destroying crops and property.\\nThe U.S. and European Union were joined by Pope John Paul II in a call for money and workers to help the stricken area.\\nPresident Clinton sent Tipper Gore, wife of Vice President Gore to the area to deliver much needed supplies to the area, demonstrating U.S. commitment to the recovery of the region.\\n\"\n",
    "\n",
    "all_hypothesis = [hypothesis_1, hypothesis_2]\n",
    "all_references = [references_1, references_2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>p</th>\n",
       "      <th>r</th>\n",
       "      <th>f</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.663387</td>\n",
       "      <td>0.731803</td>\n",
       "      <td>0.695917</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.713275</td>\n",
       "      <td>0.713275</td>\n",
       "      <td>0.713275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.561231</td>\n",
       "      <td>0.619111</td>\n",
       "      <td>0.588752</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          p         r         f\n",
       "0  0.663387  0.731803  0.695917\n",
       "1  0.713275  0.713275  0.713275\n",
       "2  0.561231  0.619111  0.588752\n",
       "3  1.000000  1.000000  1.000000"
      ]
     },
     "execution_count": 237,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score = eval_rougeL(references, hypothesis1)\n",
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(p    0.734473\n",
       " r    0.766047\n",
       " f    0.749486\n",
       " dtype: float64, p    0.187989\n",
       " r    0.163586\n",
       " f    0.175854\n",
       " dtype: float64, p    0.688331\n",
       " r    0.722539\n",
       " f    0.704596\n",
       " dtype: float64)"
      ]
     },
     "execution_count": 238,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score.mean(axis=0), score.std(axis=0), score.median(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "p    0.187989\n",
       "r    0.163586\n",
       "f    0.175854\n",
       "dtype: float64"
      ]
     },
     "execution_count": 239,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score.std(axis=0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
