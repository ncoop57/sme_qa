{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai.text import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append(\"../../\")\n",
    "from eval.exp.nb_evaluation import *\n",
    "\n",
    "sys.path.append(\"../../../\")\n",
    "from src.proc.exp.nb_proc import *\n",
    "from src.prep.exp.nb_prep import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup paths and model type\n",
    "model_path = Path(\"/tf/data/models\")\n",
    "data_path  = Path(\"/tf/data/datasets\")\n",
    "\n",
    "task_type = \"merged\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_trn, df_val, df_tst = read_data(data_path/task_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bs = 8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Percentage of data to be used: sample "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data = gen_lm_data(df_trn, df_val, task_type, data_path, bs = bs)\n",
    "data.save(task_type + '/data_lm_100pct.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data = load_data(data_path/task_type, 'data_lm_100pct.pkl', bs=bs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(data.train_ds), len(data.valid_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# amit experiments\n",
    "learn = language_model_learner(data, AWD_LSTM,\n",
    "                               drop_mult = 0.3, pretrained = pretrained,\n",
    "                               metrics=[accuracy])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.lr_find()\n",
    "learn.recorder.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set hyperparameters\n",
    "max_lr = 1e-2\n",
    "moms = (0.5, .75)\n",
    "pct_strt = 0.02\n",
    "a_epochs = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks = [\n",
    "    callbacks.SaveModelCallback(\n",
    "        learn, every='improvement',\n",
    "        monitor='valid_loss', name='awd_lstm_save_model'\n",
    "    )\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#amit experiments\n",
    "learn.fit_one_cycle(a_epochs, max_lr, callbacks = callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.load(model_path/'awd_lstm_do_30pct')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn = load_model(df_trn, file = model_path/'awd_lstm_do_30pc.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "figure_plot = learn.recorder.plot_losses(return_fig=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "figure_plot.savefig(fname=\"awd_lstm_plot_losses.png\", format='png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "Image.open('/tf/main/nbs/mdling/awd_lstm/awd_lstm_plot_losses.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import\n",
    "sys.path.append(\"/tf/main/nbs/\")\n",
    "from eval.exp.nb_evaluation import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vulnerability Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "task_type = \"buggy\"\n",
    "vuln_trn, vuln_val, vuln_tst = read_data(data_path/task_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc, prec, recal = eval_vuln(learn, vuln_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc, prec, recal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sentencepiece as spm\n",
    "sp = spm.SentencePieceProcessor()\n",
    "sp.Load(str(data_path/\"merged/model.model\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEXT = df_val[\"query\"][500]\n",
    "N_WORDS = 200\n",
    "N_SENTENCES = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(TEXT)\n",
    "df_val[\"res\"][500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sp.DecodePieces(learn.predict(TEXT, 100, temperature=0.75).split(\" \")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.predict??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grabs entire model's response up until special xxbos token,\n",
    "# i.e. once model begins a new sentence we consider the model finished with its answer.\n",
    "def get_res(mdl, inpt, n_toks = 1_000):\n",
    "    res = mdl.predict(inpt, n_toks, temperature=0.75).split(\" \")\n",
    "    res = sp.DecodePieces(res).split(\" \")\n",
    "    try:\n",
    "        end_res = res.index(\"xxbos\")\n",
    "    except:\n",
    "        end_res = len(res) - 1\n",
    "    \n",
    "    res = \" \".join(res[:end_res])[len(inpt.replace(\" \", '')):]\n",
    "    \n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = get_res(learn, \"public static void main() {return;}<$bug$>\", n_toks = 10)\n",
    "res # [0:len(\"public static void main() {return;}<$bug$>\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def eval_txt(mdl, ds):\n",
    "    b1, b2, b3, b4 = [], [], [], []\n",
    "    meteor = []\n",
    "    rouge_l = []\n",
    "    for inpt, lbl in zip(ds[\"query\"], ds[\"res\"]):\n",
    "        tok_len = len(sp.EncodeAsPieces(inpt))\n",
    "        if tok_len > 1024:\n",
    "            continue\n",
    "            \n",
    "        pred = get_res(mdl, inpt, n_toks = 10)\n",
    "        # bleu 1-4\n",
    "        b1.append(eval_bleu1([lbl], pred))\n",
    "        b2.append(eval_bleu2([lbl], pred))\n",
    "        b3.append(eval_bleu3([lbl], pred))\n",
    "        b4.append(eval_bleu4([lbl], pred))\n",
    "        \n",
    "        # meteor\n",
    "        meteor.append(eval_meteor([lbl], pred))\n",
    "        \n",
    "        # rouge\n",
    "        rouge_l.append(eval_rougeL([lbl], pred))\n",
    "        \n",
    "    return b1, b2, b3, b4, meteor, rouge_l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation metrics for vulnerability detection - Accuracy, Precision, Recall\n",
    "def eval_vuln(mdl, tst):\n",
    "    tps, tns, fps, fns = 0, 0, 0, 0\n",
    "    tot = 0\n",
    "    for inpt, lbl in zip(tst[\"query\"], tst[\"res\"]):\n",
    "        tok_len = len(sp.EncodeAsPieces(inpt))\n",
    "        if tok_len > 1024:\n",
    "#             print(\"Skipping because size is too big\", tok_len)\n",
    "            continue\n",
    "        pred = get_res(mdl, inpt, n_toks = 10)\n",
    "        if lbl == \"yes\":\n",
    "            if pred == lbl:\n",
    "                tps += 1\n",
    "            else: fns += 1\n",
    "        else:\n",
    "            if pred == lbl:\n",
    "                tns += 1\n",
    "            else: fps += 1\n",
    "                \n",
    "        tot += 1\n",
    "        torch.cuda.empty_cache()\n",
    "            \n",
    "    acc   = (tps + tns) / tot\n",
    "    prec  = tps / (tps + fps) if (tps + fps) != 0 else 0.\n",
    "    recal = tps / (tps + fns) if (tps + fns) != 0 else 0.\n",
    "    \n",
    "    return acc, prec, recal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "acc, prec, recal = eval_vuln(learn, df_val[:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc, prec, recal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df_val[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
