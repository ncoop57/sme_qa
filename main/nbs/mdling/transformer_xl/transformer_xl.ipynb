{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install sklearn nltk rouge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai.text import *\n",
    "from statistics import mean, median, stdev\n",
    "\n",
    "import sentencepiece as spm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append(\"../../\")\n",
    "from eval.exp.nb_evaluation import *\n",
    "from eval.exp.nb_plot import *\n",
    "\n",
    "sys.path.append(\"../../../\")\n",
    "from src.prep.exp.nb_prep import *\n",
    "from src.proc.exp.nb_proc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup paths and model type\n",
    "model_path = Path(\"/tf/data/models\")\n",
    "data_path  = Path(\"/tf/data/datasets\")\n",
    "\n",
    "task_type = \"merged\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sp = spm.SentencePieceProcessor()\n",
    "sp.Load(str(data_path/\"merged/model.model\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_trn, df_val, df_tst = read_data(data_path/task_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df_trn), len(df_val), len(df_tst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "bs = 8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Percentage of data to be used: sample "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data = gen_lm_data(df_trn, df_val, task_type, data_path, bs = bs)\n",
    "data.save(task_type + '/data_lm_100pct.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data = load_data(data_path/task_type, 'data_lm_100pct.pkl', bs = bs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(492904, 105363)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data.train_ds), len(data.valid_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>idx</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>); ▁} ▁finally ▁{ ▁close ( is ); ▁} ▁}&lt;$ comment $&gt; co p ies ▁bytes ▁from ▁the ▁x x up ▁url ▁&lt; code &gt; source &lt; ▁/ ▁code &gt; ▁to ▁a ▁file ▁&lt; code &gt; destination &lt; ▁/ ▁code &gt; . ▁x x ma j ▁the ▁direct or ies ▁up ▁to ▁&lt; code &gt; destination &lt; ▁/ ▁code &gt; ▁will ▁be ▁created ▁if ▁they ▁don ' t ▁already ▁exist</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>; ▁}&lt;$ comment $&gt; use ▁this ▁x x up ▁api ▁to ▁fetch ▁ aaa user _ aaa group _ binding ▁resources ▁of ▁given ▁name ▁ . ▁x x bo s ▁public ▁synchronized ▁boolean ▁has new data () ▁{ ▁return ▁has new ; ▁}&lt;$ b ug $&gt; yes ▁x x bo s ▁x x ma j ▁my ▁main activity ▁class ▁has ▁an ▁if ▁that ▁check s ▁is ▁something ▁is ▁true ▁it</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>, ▁if match ) . to block ing (). single (). body (); ▁}&lt;$ comment $&gt; ab or ts ▁an ▁ unlock ed ▁ im mut ability ▁policy . ▁x x ma j ▁the ▁response ▁of ▁delete ▁has ▁ im mut ability period since c re ation in day s ▁set ▁to ▁0 . ▁e tag ▁in ▁if - match ▁is ▁required ▁for ▁this ▁operation . ▁x x ma j</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>▁int ▁i ▁= ▁0; ▁i ▁&lt; ▁split s . size -2 ; ▁i ++ ▁ ) ▁{ ▁if ( ▁select split be t we en ( split s . data [ i ], split s . data [ i + 2 ]) ▁&lt; ▁0 ▁ ) ▁{ ▁/ ▁/ ▁merge ▁the ▁two ▁lines ▁by ▁not ▁adding ▁it ▁change ▁= ▁true ; ▁} ▁else ▁{ ▁work . add ( split s</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>▁= ▁0; ▁i ▁&lt; ▁write list . size (); ▁i ++) ▁{ ▁if ( trace component . is any tra c ing enabled () ▁&amp;&amp; ▁tc . is debug enabled ()) ▁ sib tr . debug ( tc , ▁\" set ting ▁\" ▁+ ▁write list . get ( i ) ▁+ ▁\" ▁at ▁index ▁\" ▁+ ▁( i + ▁index ) ▁ ); ▁block vector . set ( i</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data.show_batch()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# amit experiments\n",
    "learn = language_model_learner(\n",
    "    data, TransformerXL, pretrained = pretrained, metrics=[accuracy]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.lr_find()\n",
    "learn.recorder.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_lr = 5e-4\n",
    "moms = (0.75, 0.825)\n",
    "pct_strt = 0.02\n",
    "a_epochs = 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "callback_fns = [\n",
    "    callbacks.SaveModelCallback(\n",
    "        learn, every='improvement',\n",
    "        monitor='valid_loss', name=f'transformerxl_{task_type}_save_model'\n",
    "    ),\n",
    "    callbacks.EarlyStoppingCallback(\n",
    "        learn, monitor='valid_loss', min_delta = 0.01,\n",
    "        patience = 3\n",
    "    )\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#amit experiments\n",
    "learn.fit_one_cycle(\n",
    "    a_epochs, max_lr, moms = moms,\n",
    "    pct_start = pct_strt, callbacks = callback_fns\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!curl -X POST -H 'Content-type: application/json' --data '{\"text\":\"from: semeru tower 1\\nstatus: model finished training\"}' https://hooks.slack.com/services/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"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LanguageLearner(data=TextLMDataBunch;\n",
       "\n",
       "Train: LabelList (492904 items)\n",
       "x: LMTextList\n",
       "▁x x bo s ▁public ▁static ▁void ▁copy url to file ( final ▁x x up ▁url ▁source , ▁final ▁x x ma j ▁file ▁destination ) ▁throws ▁ unchecked io exception ▁{ ▁input stream ▁is ▁= ▁null ; ▁try ▁{ ▁is ▁= ▁source . open stream (); ▁write ( destination , ▁is ); ▁} ▁catch ▁( io exception ▁e ) ▁{ ▁throw ▁new ▁ unchecked io exception ( e ); ▁} ▁finally ▁{ ▁close ( is ); ▁} ▁}<$ comment $> co p ies ▁bytes ▁from ▁the ▁x x up ▁url ▁< code > source < ▁/ ▁code > ▁to ▁a ▁file ▁< code > destination < ▁/ ▁code > . ▁x x ma j ▁the ▁direct or ies ▁up ▁to ▁< code > destination < ▁/ ▁code > ▁will ▁be ▁created ▁if ▁they ▁don ' t ▁already ▁exist . ▁< code > destination < ▁/ ▁code > ▁will ▁be ▁over written ▁if ▁it ▁already ▁exists . ▁< p > ▁x x ma j ▁warning : ▁this ▁method ▁does ▁not ▁set ▁a ▁connection ▁or ▁read ▁timeout ▁and ▁thus ▁might ▁block ▁for ever . ▁x x ma j ▁use ▁{@ link ▁# ▁copy url to file ( url , ▁x x ma j ▁file , ▁int , ▁int )} ▁with ▁reason able ▁timeout s ▁to ▁prevent ▁this . ▁@ param ▁source ▁the ▁< code > url < ▁/ ▁code > ▁to ▁copy ▁bytes ▁from , ▁must ▁not ▁be ▁{@ code ▁null } ▁@ param ▁destination ▁the ▁non - directory ▁< code > file < ▁/ ▁code > ▁to ▁write ▁bytes ▁to ▁( possibly ▁over writing ) , ▁must ▁not ▁be ▁{@ code ▁null } ▁@ throws ▁ unchecked io exception ▁if ▁< code > source < ▁/ ▁code > ▁x x up ▁url ▁cannot ▁be ▁open ed ▁@ throws ▁ unchecked io exception ▁if ▁< code > destination < ▁/ ▁code > ▁is ▁a ▁directory ▁@ throws ▁ unchecked io exception ▁if ▁< code > destination < ▁/ ▁code > ▁cannot ▁be ▁written ▁@ throws ▁ unchecked io exception ▁if ▁< code > destination < ▁/ ▁code > ▁needs ▁creating ▁but ▁can ' t ▁be ▁@ throws ▁ unchecked io exception ▁if ▁an ▁x x up ▁ io ▁error ▁occurs ▁during ▁copy ing,▁x x bo s ▁public ▁java . util . list < pl . ed u . ag h . analyze r . model . pre s s release > ▁get pre s s release s ( java . lang . string ▁month , ▁java . lang . string ▁year ) ▁{ ▁if ▁( ( pre s s release repository ) ▁== ▁null ) ▁{ ▁java . lang . system . out . println (\" repository ▁not ▁initial is ed \"); ▁return ▁new ▁java . util . array list <>(); ▁} ▁java . util . list < pl . ed u . ag h . analyze r . model . pre s s release > ▁result ▁= ▁pre s s release repository . find by month and year ( month , ▁year ); ▁if ▁( ( result ▁== ▁null ) ▁|| ▁( ( result . size ()) ▁< ▁1) ) ▁{ ▁java . lang . system . out . println (\" co ul d n ' t ▁find ▁current ▁date ▁- ▁all ▁entries ▁will ▁be ▁retrieved \"); ▁result ▁= ▁( ( java . util . list < pl . ed u . ag h . analyze r . model . pre s s release > ) ▁( pre s s release repository . find all ())); ▁} ▁return ▁result ; ▁}<$ b ug $> yes,▁x x bo s ▁i ▁have ▁a ▁linked list ▁and ▁its ▁elements ▁are ▁book s . ▁x x ma j ▁book s ▁have ▁their ▁price s ▁and ▁a ▁book ▁can ▁be ▁added ▁to ▁the ▁list ▁repeat ed ly ( un - order ed ) ▁and ▁each ▁time ▁when ▁they ▁are ▁added ▁their ▁price s ▁may ▁var y . ▁x x ma j ▁now ▁i ▁have ▁to ▁find ▁the ▁best ▁ s ell ing ▁book ▁in ▁the ▁list ▁by ▁adding ▁up ▁all ▁the ▁different ▁price s ▁of ▁the ▁same ▁book ▁and ▁ divide ▁it ▁by ▁the ▁number ▁of ▁occurrence ▁in ▁the ▁list . ▁i ▁am ▁having ▁trouble ▁with ▁the ▁find ing ▁all ▁the ▁occurrence ▁of ▁the ▁same ▁book ▁as ▁they ▁are ▁un - order ed . ▁can ▁anyone ▁give ▁some ▁idea s ▁about ▁this . ▁than k ▁you . ▁how ▁do ▁i ▁find ▁the ▁average ▁in ▁a ▁x x ma j ▁java ▁linked list ? <$ qa $> is ▁there ▁a any ▁specific ▁reason ▁you ▁are ▁using ▁a ▁linked list ? if ▁not ▁a ▁map ▁can ▁make ▁your ▁life ▁a ▁lot ▁easier : ▁map < string , ▁list < book >> ▁book sh el f ▁= ▁new ▁hash map < string , ▁list < book >> (); ▁void ▁add book ( book ▁book ) ▁{ ▁x x ma j ▁string ▁key ▁= ▁book . name ▁+ ▁book . author ; ▁/ ▁/ ▁x x ma j ▁for ▁ ill u str ation ▁list < book > ▁book list ▁= ▁null ; ▁if ▁(! book sh el f . contains key ( key )) ▁{ ▁book list ▁= ▁new ▁array list < book >(); ▁book sh el f . put ( key , ▁book list ); ▁} ▁else ▁{ ▁book list ▁= ▁book sh el f . get ( key ); ▁} ▁book list . add ( book ); ▁} ▁double ▁fetch av er age ( book ▁input ){ ▁x x ma j ▁string ▁key ▁= ▁\"\" ▁/ ▁* key ▁logic * ▁/ ▁ ; ▁list < book > ▁book list ▁= ▁book sh el f . get ( key ); ▁double ▁a v g ▁= ▁0.0 ; ▁for ( book ▁b : ▁book list ){ ▁a v g ▁+= ▁b . price ; ▁} ▁return ▁a v g ▁/ ▁book list . size (); ▁} ▁x x up ▁or ▁in ▁case ▁of ▁linked list : ▁linked list < book > ▁book list ▁= ▁new ▁linked list < book >(); ▁double ▁a v g ▁= ▁0.0 ; ▁int ▁counter ▁= ▁0; ▁for ▁( book ▁b ▁: ▁book list ) ▁{ ▁if ▁( b . equals ( input book )) ▁{ ▁/ ▁/ ▁must ▁override ▁hash code () ▁and ▁ equals ▁in ▁/ ▁/ ▁x x ma j ▁book ▁and ▁it ▁should ▁be ▁in dependent ▁of ▁/ ▁/ ▁price ▁a v g ▁+= ▁b . price ; ▁counter ++; ▁} ▁} ▁return ▁a v g ▁/ ▁counter ; ▁x x ma j ▁you ▁can ▁probably ▁ enhance ▁it ▁by ▁keep ing ▁the ▁x x ma j ▁list ▁sorted , ▁so ▁all ▁book s ▁with ▁same ▁name ▁and ▁ author ▁occur e ▁con se cut ive ly . ▁x x up ▁or ▁x x ma j ▁maintain ▁a ▁temporary list ▁in ▁case ▁you ▁don ' t ▁want ▁to ▁override ▁ equals : ▁linked list < book > ▁temporary book list ▁= ▁new ▁linked list < book >(); ▁for ▁( book ▁b ▁: ▁book list ) ▁{ ▁if ▁( b . name . equals ( input book . name ) ▁x x re p ▁4 ▁& ▁b . author . equals ( input book . author )) ▁{ ▁temporary book list . add ( b ); ▁} ▁} ▁double ▁a v g ▁= ▁0.0 ; ▁for ( book ▁b ▁: ▁temporary book list ){ ▁a v g ▁+= ▁b . price ; ▁} ▁return ▁a v g ▁/ ▁temporary book list . size (); ▁x x ma j ▁note : ▁price s ▁are ▁in ▁double ▁only ▁for ▁ ill u str ation . ▁x x ma j ▁use ▁of ▁big decimal ▁is ▁ enc our ag ed ▁for ▁price s ▁and ▁such .,▁x x bo s ▁public ▁final ▁void ▁delete application ( string ▁name ) ▁{ ▁delete application request ▁request ▁= ▁delete application request . new builder (). set name ( name ) . build (); ▁delete application ( request ); ▁}<$ comment $> delete s ▁specified ▁application . ▁< p > sample ▁code : ▁< pre >< code > ▁try ▁( application service client ▁application service client ▁= ▁application service client . create ()) ▁{ ▁application name ▁name ▁= ▁application name . of (\"[ project ]\", ▁x x up ▁\"[ tenant ]\", ▁x x up ▁\"[ profile ]\", ▁x x up ▁\"[ application ]\"); ▁application service client . delete application ( name . to string ()); ▁} ▁< ▁/ ▁code >< ▁/ ▁pre > ▁@ param ▁name ▁x x ma j ▁required . ▁< p > the ▁resource ▁name ▁of ▁the ▁application ▁to ▁be ▁deleted . ▁< p > the ▁format ▁is ▁\" project s ▁/ ▁{ project _ id } ▁/ ▁ tenant s ▁/ ▁{ tenant _ id } ▁/ ▁profile s ▁/ ▁{ profile _ id } ▁/ ▁application s ▁/ ▁{ application _ id } \", ▁for ▁example , ▁\" project s ▁/ ▁test - project ▁/ ▁ tenant s ▁/ ▁test - tenant ▁/ ▁profile s ▁/ ▁test - profile ▁/ ▁application s ▁/ ▁test - application \" . ▁@ throws ▁com . google . api . gax . rpc . api exception ▁if ▁the ▁remote ▁call ▁fails,▁x x bo s ▁public ▁void ▁run to ol ( final ▁java . lang . string ▁a to ol name , ▁final ▁java . awt . window ▁a parent ) ▁{ ▁if ▁( nl . l x t re me . ol s . client . client controller . log . is log g able ( java . util . logging . level . info )) ▁{ ▁ nl . l x t re me . ol s . client . client controller . log . log ( java . util . logging . level . info , ▁\" running ▁to ol : ▁\" {0} \" ▁... \", ▁a to ol name ); ▁} ▁final ▁ nl . l x t re me . ol s . client . to ol ▁to ol ▁= ▁get to ol ( a to ol name ); ▁if ▁( to ol ▁== ▁null ) ▁{ ▁javax . swing . j option pane . show message dialog ( a parent , ▁ (\" no ▁such ▁to ol ▁found : ▁\" ▁+ ▁a to ol name ) , ▁\" error ▁... \", ▁javax . swing . j option pane . error _ message ); ▁} else ▁{ ▁final ▁ nl . l x t re me . ol s . client . to ol context ▁context ▁= ▁create to ol context (); ▁to ol . process ( a parent , ▁this . data container , ▁context , ▁this ); ▁} ▁update action s (); ▁}<$ b ug $> yes\n",
       "y: LMLabelList\n",
       ",,,,\n",
       "Path: /tf/data/datasets;\n",
       "\n",
       "Valid: LabelList (105363 items)\n",
       "x: LMTextList\n",
       "▁x x bo s ▁public ▁void ▁set it type ( int ▁flag ) ▁{ ▁on ▁= ▁( flag & 0 x 01 ) ! =0; ▁loop ▁= ▁( flag & 0 x 02 ) ! =0; ▁ s us tain ▁= ▁( flag & 0 x 04 ) ! =0; ▁carry ▁= ▁( flag & 0 x 08 ) ! =0; ▁filter ▁= ▁( flag & 0 x 80 ) ! =0; ▁}<$ comment $> set s ▁the ▁boolean ▁values ▁corresponding ▁to ▁the ▁flag ▁value ▁it - version ▁( w hy ▁on ▁ ear th ▁needed ▁this ▁to ▁be ▁swap ed ? ! ) ▁@ since ▁12 . 11 . 200 6 ▁@ param ▁flag,▁x x bo s ▁public ▁org . data v y u . plugin s . plugin ▁get compatible plugin ( final ▁java . lang . string ▁class ifier , ▁final ▁java . io . file ▁file ) ▁{ ▁for ▁( org . data v y u . plugin s . plugin ▁candidate ▁: ▁plugin class ifier s . get ( class ifier )) ▁{ ▁for ▁( org . data v y u . plugin s . filter ▁filter ▁: ▁candidate . get filter s ()) ▁{ ▁if ▁( filter . get file filter (). accept ( file )) ▁{ ▁return ▁candidate ; ▁} ▁} ▁} ▁return ▁null ; ▁}<$ b ug $> yes,▁x x bo s ▁@ java . lang . override ▁@ java . lang . suppress warning s ( value ▁= ▁\" resource \") ▁protected ▁void ▁write ( p and a . log . log event ▁event ) ▁{ ▁java . io . print stream ▁out ; ▁if ▁( ( p and a . log . impl . console log . output ) ▁== ▁null ) ▁{ ▁out ▁= ▁( level . is g re ater or equal ( log level . warn )) ▁? ▁java . lang . system . err ▁: ▁java . lang . system . out ; ▁} else ▁{ ▁out ▁= ▁p and a . log . impl . console log . output ; ▁} ▁java . lang . string ▁msg ▁= ▁format . format ( event ); ▁out . print ( msg ); ▁if ▁( ( event . get error ()) ▁!= ▁null ) ▁{ ▁msg ▁= ▁p and a . lang . exception s . get stack trace ( event . get error ()); ▁out . print ( msg ); ▁} ▁}<$ b ug $> no,▁x x bo s ▁boolean ▁update callback ( task <?> ▁task , ▁x x ma j ▁fragment ▁callback , ▁x x ma j ▁string ▁annotation id ) ▁{ ▁if ▁( callback ▁== ▁null ) ▁{ ▁return ▁false ; ▁} ▁if ▁( update callback ( task , ▁callback . get activity (), ▁annotation id )) ▁{ ▁task . set fragment id ( fragment id help er . get fragment id ( callback )); ▁return ▁true ; ▁} ▁else ▁{ ▁return ▁false ; ▁} ▁}<$ comment $> ▁/ ▁* package,▁x x bo s ▁@ visible for test ing ▁byte buffer ▁ allocate memory () ▁{ ▁if ▁(! boolean . get boolean ( disable _ allocate _ direct _ property )) ▁{ ▁for ▁( int ▁ret rie s ▁= ▁0; ▁ret rie s ▁< ▁x x up ▁memory _ al location _ at temp ts ; ▁ret rie s ++) ▁{ ▁int ▁target capacity ▁= ▁get memory for sort ( ret rie s ); ▁try ▁{ ▁return ▁byte buffer . allocate direct ( target capacity ); ▁} ▁catch ▁( out of memory error ▁e ) ▁{ ▁log . info (\" failed ▁to ▁ allocate ▁direct ▁memory ▁for ▁sort : ▁\" ▁+ ▁target capacity ▁+ ▁\" ▁retry ing ▁with ▁a ▁smaller ▁buffer .\"); ▁} ▁} ▁} ▁x x ma j ▁runtime ▁runtime ▁= ▁runtime . get runtime (); ▁int ▁target capacity ▁= ▁get memory for sort ( memory _ al location _ at temp ts ); ▁try ▁{ ▁if ▁( target capacity ▁< ▁runtime . free memory () ▁+ ▁( runtime . max memory () ▁- ▁runtime . total memory ())) ▁{ ▁log . info (\" using ▁in direct ▁memory ▁allocation .\"); ▁return ▁byte buffer . allocate ( target capacity ); ▁} ▁else ▁{ ▁log . info (\" skip ping ▁in direct ▁memory ▁allocation .\"); ▁} ▁} ▁catch ▁( out of memory error ▁e ) ▁{ ▁log . info (\" failed ▁to ▁ allocate ▁non - direct ▁memory ▁for ▁sort : ▁\" ▁+ ▁target capacity ▁+ ▁\" ▁giving ▁up \"); ▁} ▁throw ▁new ▁ reject request exception (\" failed ▁to ▁ allocate ▁memory ▁for ▁sort ▁after ▁\" ▁+ ▁x x up ▁memory _ al location _ at temp ts ▁+ ▁\" ▁attempt s . ▁x x ma j ▁giving ▁up .\"); ▁}<$ comment $> this ▁attempt s ▁to ▁ allocate ▁as ▁much ▁memory ▁as ▁can ▁be ▁ claim ed ▁for ▁sort ing . ▁x x ma j ▁ ide ally ▁this ▁should ▁be ▁as ▁large ▁as ▁possible . ▁x x ma j ▁however ▁because ▁there ▁may ▁be ▁multiple ▁requests ▁occur ring ▁on ▁the ▁same ▁instance , ▁several ▁attempt s ▁may ▁be ▁made ▁to ▁ allocate ▁a ▁large ▁port ion . ▁@ throws ▁runtime exception ▁x x ma j ▁if ▁we ▁cannot ▁ allocate ▁after ▁several ▁attempt s .\n",
       "y: LMLabelList\n",
       ",,,,\n",
       "Path: /tf/data/datasets;\n",
       "\n",
       "Test: None, model=SequentialRNN(\n",
       "  (0): TransformerXL(\n",
       "    (encoder): Embedding(8000, 410)\n",
       "    (pos_enc): PositionalEncoding()\n",
       "    (drop_emb): Dropout(p=0.1, inplace=False)\n",
       "    (layers): ModuleList(\n",
       "      (0): DecoderLayer(\n",
       "        (mhra): MultiHeadRelativeAttention(\n",
       "          (attention): Linear(in_features=410, out_features=1230, bias=False)\n",
       "          (out): Linear(in_features=410, out_features=410, bias=False)\n",
       "          (drop_att): Dropout(p=0.1, inplace=False)\n",
       "          (drop_res): Dropout(p=0.1, inplace=False)\n",
       "          (ln): LayerNorm((410,), eps=1e-05, elementwise_affine=True)\n",
       "          (r_attn): Linear(in_features=410, out_features=410, bias=False)\n",
       "        )\n",
       "        (ff): SequentialEx(\n",
       "          (layers): ModuleList(\n",
       "            (0): Linear(in_features=410, out_features=2100, bias=True)\n",
       "            (1): ReLU(inplace=True)\n",
       "            (2): Dropout(p=0.1, inplace=False)\n",
       "            (3): Linear(in_features=2100, out_features=410, bias=True)\n",
       "            (4): Dropout(p=0.1, inplace=False)\n",
       "            (5): MergeLayer()\n",
       "            (6): LayerNorm((410,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1): DecoderLayer(\n",
       "        (mhra): MultiHeadRelativeAttention(\n",
       "          (attention): Linear(in_features=410, out_features=1230, bias=False)\n",
       "          (out): Linear(in_features=410, out_features=410, bias=False)\n",
       "          (drop_att): Dropout(p=0.1, inplace=False)\n",
       "          (drop_res): Dropout(p=0.1, inplace=False)\n",
       "          (ln): LayerNorm((410,), eps=1e-05, elementwise_affine=True)\n",
       "          (r_attn): Linear(in_features=410, out_features=410, bias=False)\n",
       "        )\n",
       "        (ff): SequentialEx(\n",
       "          (layers): ModuleList(\n",
       "            (0): Linear(in_features=410, out_features=2100, bias=True)\n",
       "            (1): ReLU(inplace=True)\n",
       "            (2): Dropout(p=0.1, inplace=False)\n",
       "            (3): Linear(in_features=2100, out_features=410, bias=True)\n",
       "            (4): Dropout(p=0.1, inplace=False)\n",
       "            (5): MergeLayer()\n",
       "            (6): LayerNorm((410,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (2): DecoderLayer(\n",
       "        (mhra): MultiHeadRelativeAttention(\n",
       "          (attention): Linear(in_features=410, out_features=1230, bias=False)\n",
       "          (out): Linear(in_features=410, out_features=410, bias=False)\n",
       "          (drop_att): Dropout(p=0.1, inplace=False)\n",
       "          (drop_res): Dropout(p=0.1, inplace=False)\n",
       "          (ln): LayerNorm((410,), eps=1e-05, elementwise_affine=True)\n",
       "          (r_attn): Linear(in_features=410, out_features=410, bias=False)\n",
       "        )\n",
       "        (ff): SequentialEx(\n",
       "          (layers): ModuleList(\n",
       "            (0): Linear(in_features=410, out_features=2100, bias=True)\n",
       "            (1): ReLU(inplace=True)\n",
       "            (2): Dropout(p=0.1, inplace=False)\n",
       "            (3): Linear(in_features=2100, out_features=410, bias=True)\n",
       "            (4): Dropout(p=0.1, inplace=False)\n",
       "            (5): MergeLayer()\n",
       "            (6): LayerNorm((410,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (3): DecoderLayer(\n",
       "        (mhra): MultiHeadRelativeAttention(\n",
       "          (attention): Linear(in_features=410, out_features=1230, bias=False)\n",
       "          (out): Linear(in_features=410, out_features=410, bias=False)\n",
       "          (drop_att): Dropout(p=0.1, inplace=False)\n",
       "          (drop_res): Dropout(p=0.1, inplace=False)\n",
       "          (ln): LayerNorm((410,), eps=1e-05, elementwise_affine=True)\n",
       "          (r_attn): Linear(in_features=410, out_features=410, bias=False)\n",
       "        )\n",
       "        (ff): SequentialEx(\n",
       "          (layers): ModuleList(\n",
       "            (0): Linear(in_features=410, out_features=2100, bias=True)\n",
       "            (1): ReLU(inplace=True)\n",
       "            (2): Dropout(p=0.1, inplace=False)\n",
       "            (3): Linear(in_features=2100, out_features=410, bias=True)\n",
       "            (4): Dropout(p=0.1, inplace=False)\n",
       "            (5): MergeLayer()\n",
       "            (6): LayerNorm((410,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (4): DecoderLayer(\n",
       "        (mhra): MultiHeadRelativeAttention(\n",
       "          (attention): Linear(in_features=410, out_features=1230, bias=False)\n",
       "          (out): Linear(in_features=410, out_features=410, bias=False)\n",
       "          (drop_att): Dropout(p=0.1, inplace=False)\n",
       "          (drop_res): Dropout(p=0.1, inplace=False)\n",
       "          (ln): LayerNorm((410,), eps=1e-05, elementwise_affine=True)\n",
       "          (r_attn): Linear(in_features=410, out_features=410, bias=False)\n",
       "        )\n",
       "        (ff): SequentialEx(\n",
       "          (layers): ModuleList(\n",
       "            (0): Linear(in_features=410, out_features=2100, bias=True)\n",
       "            (1): ReLU(inplace=True)\n",
       "            (2): Dropout(p=0.1, inplace=False)\n",
       "            (3): Linear(in_features=2100, out_features=410, bias=True)\n",
       "            (4): Dropout(p=0.1, inplace=False)\n",
       "            (5): MergeLayer()\n",
       "            (6): LayerNorm((410,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (5): DecoderLayer(\n",
       "        (mhra): MultiHeadRelativeAttention(\n",
       "          (attention): Linear(in_features=410, out_features=1230, bias=False)\n",
       "          (out): Linear(in_features=410, out_features=410, bias=False)\n",
       "          (drop_att): Dropout(p=0.1, inplace=False)\n",
       "          (drop_res): Dropout(p=0.1, inplace=False)\n",
       "          (ln): LayerNorm((410,), eps=1e-05, elementwise_affine=True)\n",
       "          (r_attn): Linear(in_features=410, out_features=410, bias=False)\n",
       "        )\n",
       "        (ff): SequentialEx(\n",
       "          (layers): ModuleList(\n",
       "            (0): Linear(in_features=410, out_features=2100, bias=True)\n",
       "            (1): ReLU(inplace=True)\n",
       "            (2): Dropout(p=0.1, inplace=False)\n",
       "            (3): Linear(in_features=2100, out_features=410, bias=True)\n",
       "            (4): Dropout(p=0.1, inplace=False)\n",
       "            (5): MergeLayer()\n",
       "            (6): LayerNorm((410,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (6): DecoderLayer(\n",
       "        (mhra): MultiHeadRelativeAttention(\n",
       "          (attention): Linear(in_features=410, out_features=1230, bias=False)\n",
       "          (out): Linear(in_features=410, out_features=410, bias=False)\n",
       "          (drop_att): Dropout(p=0.1, inplace=False)\n",
       "          (drop_res): Dropout(p=0.1, inplace=False)\n",
       "          (ln): LayerNorm((410,), eps=1e-05, elementwise_affine=True)\n",
       "          (r_attn): Linear(in_features=410, out_features=410, bias=False)\n",
       "        )\n",
       "        (ff): SequentialEx(\n",
       "          (layers): ModuleList(\n",
       "            (0): Linear(in_features=410, out_features=2100, bias=True)\n",
       "            (1): ReLU(inplace=True)\n",
       "            (2): Dropout(p=0.1, inplace=False)\n",
       "            (3): Linear(in_features=2100, out_features=410, bias=True)\n",
       "            (4): Dropout(p=0.1, inplace=False)\n",
       "            (5): MergeLayer()\n",
       "            (6): LayerNorm((410,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (7): DecoderLayer(\n",
       "        (mhra): MultiHeadRelativeAttention(\n",
       "          (attention): Linear(in_features=410, out_features=1230, bias=False)\n",
       "          (out): Linear(in_features=410, out_features=410, bias=False)\n",
       "          (drop_att): Dropout(p=0.1, inplace=False)\n",
       "          (drop_res): Dropout(p=0.1, inplace=False)\n",
       "          (ln): LayerNorm((410,), eps=1e-05, elementwise_affine=True)\n",
       "          (r_attn): Linear(in_features=410, out_features=410, bias=False)\n",
       "        )\n",
       "        (ff): SequentialEx(\n",
       "          (layers): ModuleList(\n",
       "            (0): Linear(in_features=410, out_features=2100, bias=True)\n",
       "            (1): ReLU(inplace=True)\n",
       "            (2): Dropout(p=0.1, inplace=False)\n",
       "            (3): Linear(in_features=2100, out_features=410, bias=True)\n",
       "            (4): Dropout(p=0.1, inplace=False)\n",
       "            (5): MergeLayer()\n",
       "            (6): LayerNorm((410,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (8): DecoderLayer(\n",
       "        (mhra): MultiHeadRelativeAttention(\n",
       "          (attention): Linear(in_features=410, out_features=1230, bias=False)\n",
       "          (out): Linear(in_features=410, out_features=410, bias=False)\n",
       "          (drop_att): Dropout(p=0.1, inplace=False)\n",
       "          (drop_res): Dropout(p=0.1, inplace=False)\n",
       "          (ln): LayerNorm((410,), eps=1e-05, elementwise_affine=True)\n",
       "          (r_attn): Linear(in_features=410, out_features=410, bias=False)\n",
       "        )\n",
       "        (ff): SequentialEx(\n",
       "          (layers): ModuleList(\n",
       "            (0): Linear(in_features=410, out_features=2100, bias=True)\n",
       "            (1): ReLU(inplace=True)\n",
       "            (2): Dropout(p=0.1, inplace=False)\n",
       "            (3): Linear(in_features=2100, out_features=410, bias=True)\n",
       "            (4): Dropout(p=0.1, inplace=False)\n",
       "            (5): MergeLayer()\n",
       "            (6): LayerNorm((410,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (9): DecoderLayer(\n",
       "        (mhra): MultiHeadRelativeAttention(\n",
       "          (attention): Linear(in_features=410, out_features=1230, bias=False)\n",
       "          (out): Linear(in_features=410, out_features=410, bias=False)\n",
       "          (drop_att): Dropout(p=0.1, inplace=False)\n",
       "          (drop_res): Dropout(p=0.1, inplace=False)\n",
       "          (ln): LayerNorm((410,), eps=1e-05, elementwise_affine=True)\n",
       "          (r_attn): Linear(in_features=410, out_features=410, bias=False)\n",
       "        )\n",
       "        (ff): SequentialEx(\n",
       "          (layers): ModuleList(\n",
       "            (0): Linear(in_features=410, out_features=2100, bias=True)\n",
       "            (1): ReLU(inplace=True)\n",
       "            (2): Dropout(p=0.1, inplace=False)\n",
       "            (3): Linear(in_features=2100, out_features=410, bias=True)\n",
       "            (4): Dropout(p=0.1, inplace=False)\n",
       "            (5): MergeLayer()\n",
       "            (6): LayerNorm((410,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (10): DecoderLayer(\n",
       "        (mhra): MultiHeadRelativeAttention(\n",
       "          (attention): Linear(in_features=410, out_features=1230, bias=False)\n",
       "          (out): Linear(in_features=410, out_features=410, bias=False)\n",
       "          (drop_att): Dropout(p=0.1, inplace=False)\n",
       "          (drop_res): Dropout(p=0.1, inplace=False)\n",
       "          (ln): LayerNorm((410,), eps=1e-05, elementwise_affine=True)\n",
       "          (r_attn): Linear(in_features=410, out_features=410, bias=False)\n",
       "        )\n",
       "        (ff): SequentialEx(\n",
       "          (layers): ModuleList(\n",
       "            (0): Linear(in_features=410, out_features=2100, bias=True)\n",
       "            (1): ReLU(inplace=True)\n",
       "            (2): Dropout(p=0.1, inplace=False)\n",
       "            (3): Linear(in_features=2100, out_features=410, bias=True)\n",
       "            (4): Dropout(p=0.1, inplace=False)\n",
       "            (5): MergeLayer()\n",
       "            (6): LayerNorm((410,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (11): DecoderLayer(\n",
       "        (mhra): MultiHeadRelativeAttention(\n",
       "          (attention): Linear(in_features=410, out_features=1230, bias=False)\n",
       "          (out): Linear(in_features=410, out_features=410, bias=False)\n",
       "          (drop_att): Dropout(p=0.1, inplace=False)\n",
       "          (drop_res): Dropout(p=0.1, inplace=False)\n",
       "          (ln): LayerNorm((410,), eps=1e-05, elementwise_affine=True)\n",
       "          (r_attn): Linear(in_features=410, out_features=410, bias=False)\n",
       "        )\n",
       "        (ff): SequentialEx(\n",
       "          (layers): ModuleList(\n",
       "            (0): Linear(in_features=410, out_features=2100, bias=True)\n",
       "            (1): ReLU(inplace=True)\n",
       "            (2): Dropout(p=0.1, inplace=False)\n",
       "            (3): Linear(in_features=2100, out_features=410, bias=True)\n",
       "            (4): Dropout(p=0.1, inplace=False)\n",
       "            (5): MergeLayer()\n",
       "            (6): LayerNorm((410,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (1): LinearDecoder(\n",
       "    (decoder): Linear(in_features=410, out_features=8000, bias=True)\n",
       "    (output_dp): RNNDropout()\n",
       "  )\n",
       "), opt_func=functools.partial(<class 'torch.optim.adam.Adam'>, betas=(0.9, 0.99)), loss_func=FlattenedLoss of CrossEntropyLoss(), metrics=[<function accuracy at 0x7fe68df6e1e0>], true_wd=True, bn_wd=True, wd=0.01, train_bn=True, path=PosixPath('/tf/data/datasets/merged'), model_dir='models', callback_fns=[functools.partial(<class 'fastai.basic_train.Recorder'>, add_time=True, silent=False)], callbacks=[RNNTrainer\n",
       "learn: LanguageLearner(data=TextLMDataBunch;\n",
       "\n",
       "Train: LabelList (492904 items)\n",
       "x: LMTextList\n",
       "▁x x bo s ▁public ▁static ▁void ▁copy url to file ( final ▁x x up ▁url ▁source , ▁final ▁x x ma j ▁file ▁destination ) ▁throws ▁ unchecked io exception ▁{ ▁input stream ▁is ▁= ▁null ; ▁try ▁{ ▁is ▁= ▁source . open stream (); ▁write ( destination , ▁is ); ▁} ▁catch ▁( io exception ▁e ) ▁{ ▁throw ▁new ▁ unchecked io exception ( e ); ▁} ▁finally ▁{ ▁close ( is ); ▁} ▁}<$ comment $> co p ies ▁bytes ▁from ▁the ▁x x up ▁url ▁< code > source < ▁/ ▁code > ▁to ▁a ▁file ▁< code > destination < ▁/ ▁code > . ▁x x ma j ▁the ▁direct or ies ▁up ▁to ▁< code > destination < ▁/ ▁code > ▁will ▁be ▁created ▁if ▁they ▁don ' t ▁already ▁exist . ▁< code > destination < ▁/ ▁code > ▁will ▁be ▁over written ▁if ▁it ▁already ▁exists . ▁< p > ▁x x ma j ▁warning : ▁this ▁method ▁does ▁not ▁set ▁a ▁connection ▁or ▁read ▁timeout ▁and ▁thus ▁might ▁block ▁for ever . ▁x x ma j ▁use ▁{@ link ▁# ▁copy url to file ( url , ▁x x ma j ▁file , ▁int , ▁int )} ▁with ▁reason able ▁timeout s ▁to ▁prevent ▁this . ▁@ param ▁source ▁the ▁< code > url < ▁/ ▁code > ▁to ▁copy ▁bytes ▁from , ▁must ▁not ▁be ▁{@ code ▁null } ▁@ param ▁destination ▁the ▁non - directory ▁< code > file < ▁/ ▁code > ▁to ▁write ▁bytes ▁to ▁( possibly ▁over writing ) , ▁must ▁not ▁be ▁{@ code ▁null } ▁@ throws ▁ unchecked io exception ▁if ▁< code > source < ▁/ ▁code > ▁x x up ▁url ▁cannot ▁be ▁open ed ▁@ throws ▁ unchecked io exception ▁if ▁< code > destination < ▁/ ▁code > ▁is ▁a ▁directory ▁@ throws ▁ unchecked io exception ▁if ▁< code > destination < ▁/ ▁code > ▁cannot ▁be ▁written ▁@ throws ▁ unchecked io exception ▁if ▁< code > destination < ▁/ ▁code > ▁needs ▁creating ▁but ▁can ' t ▁be ▁@ throws ▁ unchecked io exception ▁if ▁an ▁x x up ▁ io ▁error ▁occurs ▁during ▁copy ing,▁x x bo s ▁public ▁java . util . list < pl . ed u . ag h . analyze r . model . pre s s release > ▁get pre s s release s ( java . lang . string ▁month , ▁java . lang . string ▁year ) ▁{ ▁if ▁( ( pre s s release repository ) ▁== ▁null ) ▁{ ▁java . lang . system . out . println (\" repository ▁not ▁initial is ed \"); ▁return ▁new ▁java . util . array list <>(); ▁} ▁java . util . list < pl . ed u . ag h . analyze r . model . pre s s release > ▁result ▁= ▁pre s s release repository . find by month and year ( month , ▁year ); ▁if ▁( ( result ▁== ▁null ) ▁|| ▁( ( result . size ()) ▁< ▁1) ) ▁{ ▁java . lang . system . out . println (\" co ul d n ' t ▁find ▁current ▁date ▁- ▁all ▁entries ▁will ▁be ▁retrieved \"); ▁result ▁= ▁( ( java . util . list < pl . ed u . ag h . analyze r . model . pre s s release > ) ▁( pre s s release repository . find all ())); ▁} ▁return ▁result ; ▁}<$ b ug $> yes,▁x x bo s ▁i ▁have ▁a ▁linked list ▁and ▁its ▁elements ▁are ▁book s . ▁x x ma j ▁book s ▁have ▁their ▁price s ▁and ▁a ▁book ▁can ▁be ▁added ▁to ▁the ▁list ▁repeat ed ly ( un - order ed ) ▁and ▁each ▁time ▁when ▁they ▁are ▁added ▁their ▁price s ▁may ▁var y . ▁x x ma j ▁now ▁i ▁have ▁to ▁find ▁the ▁best ▁ s ell ing ▁book ▁in ▁the ▁list ▁by ▁adding ▁up ▁all ▁the ▁different ▁price s ▁of ▁the ▁same ▁book ▁and ▁ divide ▁it ▁by ▁the ▁number ▁of ▁occurrence ▁in ▁the ▁list . ▁i ▁am ▁having ▁trouble ▁with ▁the ▁find ing ▁all ▁the ▁occurrence ▁of ▁the ▁same ▁book ▁as ▁they ▁are ▁un - order ed . ▁can ▁anyone ▁give ▁some ▁idea s ▁about ▁this . ▁than k ▁you . ▁how ▁do ▁i ▁find ▁the ▁average ▁in ▁a ▁x x ma j ▁java ▁linked list ? <$ qa $> is ▁there ▁a any ▁specific ▁reason ▁you ▁are ▁using ▁a ▁linked list ? if ▁not ▁a ▁map ▁can ▁make ▁your ▁life ▁a ▁lot ▁easier : ▁map < string , ▁list < book >> ▁book sh el f ▁= ▁new ▁hash map < string , ▁list < book >> (); ▁void ▁add book ( book ▁book ) ▁{ ▁x x ma j ▁string ▁key ▁= ▁book . name ▁+ ▁book . author ; ▁/ ▁/ ▁x x ma j ▁for ▁ ill u str ation ▁list < book > ▁book list ▁= ▁null ; ▁if ▁(! book sh el f . contains key ( key )) ▁{ ▁book list ▁= ▁new ▁array list < book >(); ▁book sh el f . put ( key , ▁book list ); ▁} ▁else ▁{ ▁book list ▁= ▁book sh el f . get ( key ); ▁} ▁book list . add ( book ); ▁} ▁double ▁fetch av er age ( book ▁input ){ ▁x x ma j ▁string ▁key ▁= ▁\"\" ▁/ ▁* key ▁logic * ▁/ ▁ ; ▁list < book > ▁book list ▁= ▁book sh el f . get ( key ); ▁double ▁a v g ▁= ▁0.0 ; ▁for ( book ▁b : ▁book list ){ ▁a v g ▁+= ▁b . price ; ▁} ▁return ▁a v g ▁/ ▁book list . size (); ▁} ▁x x up ▁or ▁in ▁case ▁of ▁linked list : ▁linked list < book > ▁book list ▁= ▁new ▁linked list < book >(); ▁double ▁a v g ▁= ▁0.0 ; ▁int ▁counter ▁= ▁0; ▁for ▁( book ▁b ▁: ▁book list ) ▁{ ▁if ▁( b . equals ( input book )) ▁{ ▁/ ▁/ ▁must ▁override ▁hash code () ▁and ▁ equals ▁in ▁/ ▁/ ▁x x ma j ▁book ▁and ▁it ▁should ▁be ▁in dependent ▁of ▁/ ▁/ ▁price ▁a v g ▁+= ▁b . price ; ▁counter ++; ▁} ▁} ▁return ▁a v g ▁/ ▁counter ; ▁x x ma j ▁you ▁can ▁probably ▁ enhance ▁it ▁by ▁keep ing ▁the ▁x x ma j ▁list ▁sorted , ▁so ▁all ▁book s ▁with ▁same ▁name ▁and ▁ author ▁occur e ▁con se cut ive ly . ▁x x up ▁or ▁x x ma j ▁maintain ▁a ▁temporary list ▁in ▁case ▁you ▁don ' t ▁want ▁to ▁override ▁ equals : ▁linked list < book > ▁temporary book list ▁= ▁new ▁linked list < book >(); ▁for ▁( book ▁b ▁: ▁book list ) ▁{ ▁if ▁( b . name . equals ( input book . name ) ▁x x re p ▁4 ▁& ▁b . author . equals ( input book . author )) ▁{ ▁temporary book list . add ( b ); ▁} ▁} ▁double ▁a v g ▁= ▁0.0 ; ▁for ( book ▁b ▁: ▁temporary book list ){ ▁a v g ▁+= ▁b . price ; ▁} ▁return ▁a v g ▁/ ▁temporary book list . size (); ▁x x ma j ▁note : ▁price s ▁are ▁in ▁double ▁only ▁for ▁ ill u str ation . ▁x x ma j ▁use ▁of ▁big decimal ▁is ▁ enc our ag ed ▁for ▁price s ▁and ▁such .,▁x x bo s ▁public ▁final ▁void ▁delete application ( string ▁name ) ▁{ ▁delete application request ▁request ▁= ▁delete application request . new builder (). set name ( name ) . build (); ▁delete application ( request ); ▁}<$ comment $> delete s ▁specified ▁application . ▁< p > sample ▁code : ▁< pre >< code > ▁try ▁( application service client ▁application service client ▁= ▁application service client . create ()) ▁{ ▁application name ▁name ▁= ▁application name . of (\"[ project ]\", ▁x x up ▁\"[ tenant ]\", ▁x x up ▁\"[ profile ]\", ▁x x up ▁\"[ application ]\"); ▁application service client . delete application ( name . to string ()); ▁} ▁< ▁/ ▁code >< ▁/ ▁pre > ▁@ param ▁name ▁x x ma j ▁required . ▁< p > the ▁resource ▁name ▁of ▁the ▁application ▁to ▁be ▁deleted . ▁< p > the ▁format ▁is ▁\" project s ▁/ ▁{ project _ id } ▁/ ▁ tenant s ▁/ ▁{ tenant _ id } ▁/ ▁profile s ▁/ ▁{ profile _ id } ▁/ ▁application s ▁/ ▁{ application _ id } \", ▁for ▁example , ▁\" project s ▁/ ▁test - project ▁/ ▁ tenant s ▁/ ▁test - tenant ▁/ ▁profile s ▁/ ▁test - profile ▁/ ▁application s ▁/ ▁test - application \" . ▁@ throws ▁com . google . api . gax . rpc . api exception ▁if ▁the ▁remote ▁call ▁fails,▁x x bo s ▁public ▁void ▁run to ol ( final ▁java . lang . string ▁a to ol name , ▁final ▁java . awt . window ▁a parent ) ▁{ ▁if ▁( nl . l x t re me . ol s . client . client controller . log . is log g able ( java . util . logging . level . info )) ▁{ ▁ nl . l x t re me . ol s . client . client controller . log . log ( java . util . logging . level . info , ▁\" running ▁to ol : ▁\" {0} \" ▁... \", ▁a to ol name ); ▁} ▁final ▁ nl . l x t re me . ol s . client . to ol ▁to ol ▁= ▁get to ol ( a to ol name ); ▁if ▁( to ol ▁== ▁null ) ▁{ ▁javax . swing . j option pane . show message dialog ( a parent , ▁ (\" no ▁such ▁to ol ▁found : ▁\" ▁+ ▁a to ol name ) , ▁\" error ▁... \", ▁javax . swing . j option pane . error _ message ); ▁} else ▁{ ▁final ▁ nl . l x t re me . ol s . client . to ol context ▁context ▁= ▁create to ol context (); ▁to ol . process ( a parent , ▁this . data container , ▁context , ▁this ); ▁} ▁update action s (); ▁}<$ b ug $> yes\n",
       "y: LMLabelList\n",
       ",,,,\n",
       "Path: /tf/data/datasets;\n",
       "\n",
       "Valid: LabelList (105363 items)\n",
       "x: LMTextList\n",
       "▁x x bo s ▁public ▁void ▁set it type ( int ▁flag ) ▁{ ▁on ▁= ▁( flag & 0 x 01 ) ! =0; ▁loop ▁= ▁( flag & 0 x 02 ) ! =0; ▁ s us tain ▁= ▁( flag & 0 x 04 ) ! =0; ▁carry ▁= ▁( flag & 0 x 08 ) ! =0; ▁filter ▁= ▁( flag & 0 x 80 ) ! =0; ▁}<$ comment $> set s ▁the ▁boolean ▁values ▁corresponding ▁to ▁the ▁flag ▁value ▁it - version ▁( w hy ▁on ▁ ear th ▁needed ▁this ▁to ▁be ▁swap ed ? ! ) ▁@ since ▁12 . 11 . 200 6 ▁@ param ▁flag,▁x x bo s ▁public ▁org . data v y u . plugin s . plugin ▁get compatible plugin ( final ▁java . lang . string ▁class ifier , ▁final ▁java . io . file ▁file ) ▁{ ▁for ▁( org . data v y u . plugin s . plugin ▁candidate ▁: ▁plugin class ifier s . get ( class ifier )) ▁{ ▁for ▁( org . data v y u . plugin s . filter ▁filter ▁: ▁candidate . get filter s ()) ▁{ ▁if ▁( filter . get file filter (). accept ( file )) ▁{ ▁return ▁candidate ; ▁} ▁} ▁} ▁return ▁null ; ▁}<$ b ug $> yes,▁x x bo s ▁@ java . lang . override ▁@ java . lang . suppress warning s ( value ▁= ▁\" resource \") ▁protected ▁void ▁write ( p and a . log . log event ▁event ) ▁{ ▁java . io . print stream ▁out ; ▁if ▁( ( p and a . log . impl . console log . output ) ▁== ▁null ) ▁{ ▁out ▁= ▁( level . is g re ater or equal ( log level . warn )) ▁? ▁java . lang . system . err ▁: ▁java . lang . system . out ; ▁} else ▁{ ▁out ▁= ▁p and a . log . impl . console log . output ; ▁} ▁java . lang . string ▁msg ▁= ▁format . format ( event ); ▁out . print ( msg ); ▁if ▁( ( event . get error ()) ▁!= ▁null ) ▁{ ▁msg ▁= ▁p and a . lang . exception s . get stack trace ( event . get error ()); ▁out . print ( msg ); ▁} ▁}<$ b ug $> no,▁x x bo s ▁boolean ▁update callback ( task <?> ▁task , ▁x x ma j ▁fragment ▁callback , ▁x x ma j ▁string ▁annotation id ) ▁{ ▁if ▁( callback ▁== ▁null ) ▁{ ▁return ▁false ; ▁} ▁if ▁( update callback ( task , ▁callback . get activity (), ▁annotation id )) ▁{ ▁task . set fragment id ( fragment id help er . get fragment id ( callback )); ▁return ▁true ; ▁} ▁else ▁{ ▁return ▁false ; ▁} ▁}<$ comment $> ▁/ ▁* package,▁x x bo s ▁@ visible for test ing ▁byte buffer ▁ allocate memory () ▁{ ▁if ▁(! boolean . get boolean ( disable _ allocate _ direct _ property )) ▁{ ▁for ▁( int ▁ret rie s ▁= ▁0; ▁ret rie s ▁< ▁x x up ▁memory _ al location _ at temp ts ; ▁ret rie s ++) ▁{ ▁int ▁target capacity ▁= ▁get memory for sort ( ret rie s ); ▁try ▁{ ▁return ▁byte buffer . allocate direct ( target capacity ); ▁} ▁catch ▁( out of memory error ▁e ) ▁{ ▁log . info (\" failed ▁to ▁ allocate ▁direct ▁memory ▁for ▁sort : ▁\" ▁+ ▁target capacity ▁+ ▁\" ▁retry ing ▁with ▁a ▁smaller ▁buffer .\"); ▁} ▁} ▁} ▁x x ma j ▁runtime ▁runtime ▁= ▁runtime . get runtime (); ▁int ▁target capacity ▁= ▁get memory for sort ( memory _ al location _ at temp ts ); ▁try ▁{ ▁if ▁( target capacity ▁< ▁runtime . free memory () ▁+ ▁( runtime . max memory () ▁- ▁runtime . total memory ())) ▁{ ▁log . info (\" using ▁in direct ▁memory ▁allocation .\"); ▁return ▁byte buffer . allocate ( target capacity ); ▁} ▁else ▁{ ▁log . info (\" skip ping ▁in direct ▁memory ▁allocation .\"); ▁} ▁} ▁catch ▁( out of memory error ▁e ) ▁{ ▁log . info (\" failed ▁to ▁ allocate ▁non - direct ▁memory ▁for ▁sort : ▁\" ▁+ ▁target capacity ▁+ ▁\" ▁giving ▁up \"); ▁} ▁throw ▁new ▁ reject request exception (\" failed ▁to ▁ allocate ▁memory ▁for ▁sort ▁after ▁\" ▁+ ▁x x up ▁memory _ al location _ at temp ts ▁+ ▁\" ▁attempt s . ▁x x ma j ▁giving ▁up .\"); ▁}<$ comment $> this ▁attempt s ▁to ▁ allocate ▁as ▁much ▁memory ▁as ▁can ▁be ▁ claim ed ▁for ▁sort ing . ▁x x ma j ▁ ide ally ▁this ▁should ▁be ▁as ▁large ▁as ▁possible . ▁x x ma j ▁however ▁because ▁there ▁may ▁be ▁multiple ▁requests ▁occur ring ▁on ▁the ▁same ▁instance , ▁several ▁attempt s ▁may ▁be ▁made ▁to ▁ allocate ▁a ▁large ▁port ion . ▁@ throws ▁runtime exception ▁x x ma j ▁if ▁we ▁cannot ▁ allocate ▁after ▁several ▁attempt s .\n",
       "y: LMLabelList\n",
       ",,,,\n",
       "Path: /tf/data/datasets;\n",
       "\n",
       "Test: None, model=SequentialRNN(\n",
       "  (0): TransformerXL(\n",
       "    (encoder): Embedding(8000, 410)\n",
       "    (pos_enc): PositionalEncoding()\n",
       "    (drop_emb): Dropout(p=0.1, inplace=False)\n",
       "    (layers): ModuleList(\n",
       "      (0): DecoderLayer(\n",
       "        (mhra): MultiHeadRelativeAttention(\n",
       "          (attention): Linear(in_features=410, out_features=1230, bias=False)\n",
       "          (out): Linear(in_features=410, out_features=410, bias=False)\n",
       "          (drop_att): Dropout(p=0.1, inplace=False)\n",
       "          (drop_res): Dropout(p=0.1, inplace=False)\n",
       "          (ln): LayerNorm((410,), eps=1e-05, elementwise_affine=True)\n",
       "          (r_attn): Linear(in_features=410, out_features=410, bias=False)\n",
       "        )\n",
       "        (ff): SequentialEx(\n",
       "          (layers): ModuleList(\n",
       "            (0): Linear(in_features=410, out_features=2100, bias=True)\n",
       "            (1): ReLU(inplace=True)\n",
       "            (2): Dropout(p=0.1, inplace=False)\n",
       "            (3): Linear(in_features=2100, out_features=410, bias=True)\n",
       "            (4): Dropout(p=0.1, inplace=False)\n",
       "            (5): MergeLayer()\n",
       "            (6): LayerNorm((410,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1): DecoderLayer(\n",
       "        (mhra): MultiHeadRelativeAttention(\n",
       "          (attention): Linear(in_features=410, out_features=1230, bias=False)\n",
       "          (out): Linear(in_features=410, out_features=410, bias=False)\n",
       "          (drop_att): Dropout(p=0.1, inplace=False)\n",
       "          (drop_res): Dropout(p=0.1, inplace=False)\n",
       "          (ln): LayerNorm((410,), eps=1e-05, elementwise_affine=True)\n",
       "          (r_attn): Linear(in_features=410, out_features=410, bias=False)\n",
       "        )\n",
       "        (ff): SequentialEx(\n",
       "          (layers): ModuleList(\n",
       "            (0): Linear(in_features=410, out_features=2100, bias=True)\n",
       "            (1): ReLU(inplace=True)\n",
       "            (2): Dropout(p=0.1, inplace=False)\n",
       "            (3): Linear(in_features=2100, out_features=410, bias=True)\n",
       "            (4): Dropout(p=0.1, inplace=False)\n",
       "            (5): MergeLayer()\n",
       "            (6): LayerNorm((410,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (2): DecoderLayer(\n",
       "        (mhra): MultiHeadRelativeAttention(\n",
       "          (attention): Linear(in_features=410, out_features=1230, bias=False)\n",
       "          (out): Linear(in_features=410, out_features=410, bias=False)\n",
       "          (drop_att): Dropout(p=0.1, inplace=False)\n",
       "          (drop_res): Dropout(p=0.1, inplace=False)\n",
       "          (ln): LayerNorm((410,), eps=1e-05, elementwise_affine=True)\n",
       "          (r_attn): Linear(in_features=410, out_features=410, bias=False)\n",
       "        )\n",
       "        (ff): SequentialEx(\n",
       "          (layers): ModuleList(\n",
       "            (0): Linear(in_features=410, out_features=2100, bias=True)\n",
       "            (1): ReLU(inplace=True)\n",
       "            (2): Dropout(p=0.1, inplace=False)\n",
       "            (3): Linear(in_features=2100, out_features=410, bias=True)\n",
       "            (4): Dropout(p=0.1, inplace=False)\n",
       "            (5): MergeLayer()\n",
       "            (6): LayerNorm((410,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (3): DecoderLayer(\n",
       "        (mhra): MultiHeadRelativeAttention(\n",
       "          (attention): Linear(in_features=410, out_features=1230, bias=False)\n",
       "          (out): Linear(in_features=410, out_features=410, bias=False)\n",
       "          (drop_att): Dropout(p=0.1, inplace=False)\n",
       "          (drop_res): Dropout(p=0.1, inplace=False)\n",
       "          (ln): LayerNorm((410,), eps=1e-05, elementwise_affine=True)\n",
       "          (r_attn): Linear(in_features=410, out_features=410, bias=False)\n",
       "        )\n",
       "        (ff): SequentialEx(\n",
       "          (layers): ModuleList(\n",
       "            (0): Linear(in_features=410, out_features=2100, bias=True)\n",
       "            (1): ReLU(inplace=True)\n",
       "            (2): Dropout(p=0.1, inplace=False)\n",
       "            (3): Linear(in_features=2100, out_features=410, bias=True)\n",
       "            (4): Dropout(p=0.1, inplace=False)\n",
       "            (5): MergeLayer()\n",
       "            (6): LayerNorm((410,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (4): DecoderLayer(\n",
       "        (mhra): MultiHeadRelativeAttention(\n",
       "          (attention): Linear(in_features=410, out_features=1230, bias=False)\n",
       "          (out): Linear(in_features=410, out_features=410, bias=False)\n",
       "          (drop_att): Dropout(p=0.1, inplace=False)\n",
       "          (drop_res): Dropout(p=0.1, inplace=False)\n",
       "          (ln): LayerNorm((410,), eps=1e-05, elementwise_affine=True)\n",
       "          (r_attn): Linear(in_features=410, out_features=410, bias=False)\n",
       "        )\n",
       "        (ff): SequentialEx(\n",
       "          (layers): ModuleList(\n",
       "            (0): Linear(in_features=410, out_features=2100, bias=True)\n",
       "            (1): ReLU(inplace=True)\n",
       "            (2): Dropout(p=0.1, inplace=False)\n",
       "            (3): Linear(in_features=2100, out_features=410, bias=True)\n",
       "            (4): Dropout(p=0.1, inplace=False)\n",
       "            (5): MergeLayer()\n",
       "            (6): LayerNorm((410,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (5): DecoderLayer(\n",
       "        (mhra): MultiHeadRelativeAttention(\n",
       "          (attention): Linear(in_features=410, out_features=1230, bias=False)\n",
       "          (out): Linear(in_features=410, out_features=410, bias=False)\n",
       "          (drop_att): Dropout(p=0.1, inplace=False)\n",
       "          (drop_res): Dropout(p=0.1, inplace=False)\n",
       "          (ln): LayerNorm((410,), eps=1e-05, elementwise_affine=True)\n",
       "          (r_attn): Linear(in_features=410, out_features=410, bias=False)\n",
       "        )\n",
       "        (ff): SequentialEx(\n",
       "          (layers): ModuleList(\n",
       "            (0): Linear(in_features=410, out_features=2100, bias=True)\n",
       "            (1): ReLU(inplace=True)\n",
       "            (2): Dropout(p=0.1, inplace=False)\n",
       "            (3): Linear(in_features=2100, out_features=410, bias=True)\n",
       "            (4): Dropout(p=0.1, inplace=False)\n",
       "            (5): MergeLayer()\n",
       "            (6): LayerNorm((410,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (6): DecoderLayer(\n",
       "        (mhra): MultiHeadRelativeAttention(\n",
       "          (attention): Linear(in_features=410, out_features=1230, bias=False)\n",
       "          (out): Linear(in_features=410, out_features=410, bias=False)\n",
       "          (drop_att): Dropout(p=0.1, inplace=False)\n",
       "          (drop_res): Dropout(p=0.1, inplace=False)\n",
       "          (ln): LayerNorm((410,), eps=1e-05, elementwise_affine=True)\n",
       "          (r_attn): Linear(in_features=410, out_features=410, bias=False)\n",
       "        )\n",
       "        (ff): SequentialEx(\n",
       "          (layers): ModuleList(\n",
       "            (0): Linear(in_features=410, out_features=2100, bias=True)\n",
       "            (1): ReLU(inplace=True)\n",
       "            (2): Dropout(p=0.1, inplace=False)\n",
       "            (3): Linear(in_features=2100, out_features=410, bias=True)\n",
       "            (4): Dropout(p=0.1, inplace=False)\n",
       "            (5): MergeLayer()\n",
       "            (6): LayerNorm((410,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (7): DecoderLayer(\n",
       "        (mhra): MultiHeadRelativeAttention(\n",
       "          (attention): Linear(in_features=410, out_features=1230, bias=False)\n",
       "          (out): Linear(in_features=410, out_features=410, bias=False)\n",
       "          (drop_att): Dropout(p=0.1, inplace=False)\n",
       "          (drop_res): Dropout(p=0.1, inplace=False)\n",
       "          (ln): LayerNorm((410,), eps=1e-05, elementwise_affine=True)\n",
       "          (r_attn): Linear(in_features=410, out_features=410, bias=False)\n",
       "        )\n",
       "        (ff): SequentialEx(\n",
       "          (layers): ModuleList(\n",
       "            (0): Linear(in_features=410, out_features=2100, bias=True)\n",
       "            (1): ReLU(inplace=True)\n",
       "            (2): Dropout(p=0.1, inplace=False)\n",
       "            (3): Linear(in_features=2100, out_features=410, bias=True)\n",
       "            (4): Dropout(p=0.1, inplace=False)\n",
       "            (5): MergeLayer()\n",
       "            (6): LayerNorm((410,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (8): DecoderLayer(\n",
       "        (mhra): MultiHeadRelativeAttention(\n",
       "          (attention): Linear(in_features=410, out_features=1230, bias=False)\n",
       "          (out): Linear(in_features=410, out_features=410, bias=False)\n",
       "          (drop_att): Dropout(p=0.1, inplace=False)\n",
       "          (drop_res): Dropout(p=0.1, inplace=False)\n",
       "          (ln): LayerNorm((410,), eps=1e-05, elementwise_affine=True)\n",
       "          (r_attn): Linear(in_features=410, out_features=410, bias=False)\n",
       "        )\n",
       "        (ff): SequentialEx(\n",
       "          (layers): ModuleList(\n",
       "            (0): Linear(in_features=410, out_features=2100, bias=True)\n",
       "            (1): ReLU(inplace=True)\n",
       "            (2): Dropout(p=0.1, inplace=False)\n",
       "            (3): Linear(in_features=2100, out_features=410, bias=True)\n",
       "            (4): Dropout(p=0.1, inplace=False)\n",
       "            (5): MergeLayer()\n",
       "            (6): LayerNorm((410,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (9): DecoderLayer(\n",
       "        (mhra): MultiHeadRelativeAttention(\n",
       "          (attention): Linear(in_features=410, out_features=1230, bias=False)\n",
       "          (out): Linear(in_features=410, out_features=410, bias=False)\n",
       "          (drop_att): Dropout(p=0.1, inplace=False)\n",
       "          (drop_res): Dropout(p=0.1, inplace=False)\n",
       "          (ln): LayerNorm((410,), eps=1e-05, elementwise_affine=True)\n",
       "          (r_attn): Linear(in_features=410, out_features=410, bias=False)\n",
       "        )\n",
       "        (ff): SequentialEx(\n",
       "          (layers): ModuleList(\n",
       "            (0): Linear(in_features=410, out_features=2100, bias=True)\n",
       "            (1): ReLU(inplace=True)\n",
       "            (2): Dropout(p=0.1, inplace=False)\n",
       "            (3): Linear(in_features=2100, out_features=410, bias=True)\n",
       "            (4): Dropout(p=0.1, inplace=False)\n",
       "            (5): MergeLayer()\n",
       "            (6): LayerNorm((410,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (10): DecoderLayer(\n",
       "        (mhra): MultiHeadRelativeAttention(\n",
       "          (attention): Linear(in_features=410, out_features=1230, bias=False)\n",
       "          (out): Linear(in_features=410, out_features=410, bias=False)\n",
       "          (drop_att): Dropout(p=0.1, inplace=False)\n",
       "          (drop_res): Dropout(p=0.1, inplace=False)\n",
       "          (ln): LayerNorm((410,), eps=1e-05, elementwise_affine=True)\n",
       "          (r_attn): Linear(in_features=410, out_features=410, bias=False)\n",
       "        )\n",
       "        (ff): SequentialEx(\n",
       "          (layers): ModuleList(\n",
       "            (0): Linear(in_features=410, out_features=2100, bias=True)\n",
       "            (1): ReLU(inplace=True)\n",
       "            (2): Dropout(p=0.1, inplace=False)\n",
       "            (3): Linear(in_features=2100, out_features=410, bias=True)\n",
       "            (4): Dropout(p=0.1, inplace=False)\n",
       "            (5): MergeLayer()\n",
       "            (6): LayerNorm((410,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (11): DecoderLayer(\n",
       "        (mhra): MultiHeadRelativeAttention(\n",
       "          (attention): Linear(in_features=410, out_features=1230, bias=False)\n",
       "          (out): Linear(in_features=410, out_features=410, bias=False)\n",
       "          (drop_att): Dropout(p=0.1, inplace=False)\n",
       "          (drop_res): Dropout(p=0.1, inplace=False)\n",
       "          (ln): LayerNorm((410,), eps=1e-05, elementwise_affine=True)\n",
       "          (r_attn): Linear(in_features=410, out_features=410, bias=False)\n",
       "        )\n",
       "        (ff): SequentialEx(\n",
       "          (layers): ModuleList(\n",
       "            (0): Linear(in_features=410, out_features=2100, bias=True)\n",
       "            (1): ReLU(inplace=True)\n",
       "            (2): Dropout(p=0.1, inplace=False)\n",
       "            (3): Linear(in_features=2100, out_features=410, bias=True)\n",
       "            (4): Dropout(p=0.1, inplace=False)\n",
       "            (5): MergeLayer()\n",
       "            (6): LayerNorm((410,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (1): LinearDecoder(\n",
       "    (decoder): Linear(in_features=410, out_features=8000, bias=True)\n",
       "    (output_dp): RNNDropout()\n",
       "  )\n",
       "), opt_func=functools.partial(<class 'torch.optim.adam.Adam'>, betas=(0.9, 0.99)), loss_func=FlattenedLoss of CrossEntropyLoss(), metrics=[<function accuracy at 0x7fe68df6e1e0>], true_wd=True, bn_wd=True, wd=0.01, train_bn=True, path=PosixPath('/tf/data/datasets/merged'), model_dir='models', callback_fns=[functools.partial(<class 'fastai.basic_train.Recorder'>, add_time=True, silent=False)], callbacks=[...], layer_groups=[Sequential(\n",
       "  (0): DecoderLayer(\n",
       "    (mhra): MultiHeadRelativeAttention(\n",
       "      (attention): Linear(in_features=410, out_features=1230, bias=False)\n",
       "      (out): Linear(in_features=410, out_features=410, bias=False)\n",
       "      (drop_att): Dropout(p=0.1, inplace=False)\n",
       "      (drop_res): Dropout(p=0.1, inplace=False)\n",
       "      (ln): LayerNorm((410,), eps=1e-05, elementwise_affine=True)\n",
       "      (r_attn): Linear(in_features=410, out_features=410, bias=False)\n",
       "    )\n",
       "    (ff): SequentialEx(\n",
       "      (layers): ModuleList(\n",
       "        (0): Linear(in_features=410, out_features=2100, bias=True)\n",
       "        (1): ReLU(inplace=True)\n",
       "        (2): Dropout(p=0.1, inplace=False)\n",
       "        (3): Linear(in_features=2100, out_features=410, bias=True)\n",
       "        (4): Dropout(p=0.1, inplace=False)\n",
       "        (5): MergeLayer()\n",
       "        (6): LayerNorm((410,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (1): DecoderLayer(\n",
       "    (mhra): MultiHeadRelativeAttention(\n",
       "      (attention): Linear(in_features=410, out_features=1230, bias=False)\n",
       "      (out): Linear(in_features=410, out_features=410, bias=False)\n",
       "      (drop_att): Dropout(p=0.1, inplace=False)\n",
       "      (drop_res): Dropout(p=0.1, inplace=False)\n",
       "      (ln): LayerNorm((410,), eps=1e-05, elementwise_affine=True)\n",
       "      (r_attn): Linear(in_features=410, out_features=410, bias=False)\n",
       "    )\n",
       "    (ff): SequentialEx(\n",
       "      (layers): ModuleList(\n",
       "        (0): Linear(in_features=410, out_features=2100, bias=True)\n",
       "        (1): ReLU(inplace=True)\n",
       "        (2): Dropout(p=0.1, inplace=False)\n",
       "        (3): Linear(in_features=2100, out_features=410, bias=True)\n",
       "        (4): Dropout(p=0.1, inplace=False)\n",
       "        (5): MergeLayer()\n",
       "        (6): LayerNorm((410,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (2): DecoderLayer(\n",
       "    (mhra): MultiHeadRelativeAttention(\n",
       "      (attention): Linear(in_features=410, out_features=1230, bias=False)\n",
       "      (out): Linear(in_features=410, out_features=410, bias=False)\n",
       "      (drop_att): Dropout(p=0.1, inplace=False)\n",
       "      (drop_res): Dropout(p=0.1, inplace=False)\n",
       "      (ln): LayerNorm((410,), eps=1e-05, elementwise_affine=True)\n",
       "      (r_attn): Linear(in_features=410, out_features=410, bias=False)\n",
       "    )\n",
       "    (ff): SequentialEx(\n",
       "      (layers): ModuleList(\n",
       "        (0): Linear(in_features=410, out_features=2100, bias=True)\n",
       "        (1): ReLU(inplace=True)\n",
       "        (2): Dropout(p=0.1, inplace=False)\n",
       "        (3): Linear(in_features=2100, out_features=410, bias=True)\n",
       "        (4): Dropout(p=0.1, inplace=False)\n",
       "        (5): MergeLayer()\n",
       "        (6): LayerNorm((410,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (3): DecoderLayer(\n",
       "    (mhra): MultiHeadRelativeAttention(\n",
       "      (attention): Linear(in_features=410, out_features=1230, bias=False)\n",
       "      (out): Linear(in_features=410, out_features=410, bias=False)\n",
       "      (drop_att): Dropout(p=0.1, inplace=False)\n",
       "      (drop_res): Dropout(p=0.1, inplace=False)\n",
       "      (ln): LayerNorm((410,), eps=1e-05, elementwise_affine=True)\n",
       "      (r_attn): Linear(in_features=410, out_features=410, bias=False)\n",
       "    )\n",
       "    (ff): SequentialEx(\n",
       "      (layers): ModuleList(\n",
       "        (0): Linear(in_features=410, out_features=2100, bias=True)\n",
       "        (1): ReLU(inplace=True)\n",
       "        (2): Dropout(p=0.1, inplace=False)\n",
       "        (3): Linear(in_features=2100, out_features=410, bias=True)\n",
       "        (4): Dropout(p=0.1, inplace=False)\n",
       "        (5): MergeLayer()\n",
       "        (6): LayerNorm((410,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (4): ParameterModule()\n",
       "  (5): ParameterModule()\n",
       "), Sequential(\n",
       "  (0): DecoderLayer(\n",
       "    (mhra): MultiHeadRelativeAttention(\n",
       "      (attention): Linear(in_features=410, out_features=1230, bias=False)\n",
       "      (out): Linear(in_features=410, out_features=410, bias=False)\n",
       "      (drop_att): Dropout(p=0.1, inplace=False)\n",
       "      (drop_res): Dropout(p=0.1, inplace=False)\n",
       "      (ln): LayerNorm((410,), eps=1e-05, elementwise_affine=True)\n",
       "      (r_attn): Linear(in_features=410, out_features=410, bias=False)\n",
       "    )\n",
       "    (ff): SequentialEx(\n",
       "      (layers): ModuleList(\n",
       "        (0): Linear(in_features=410, out_features=2100, bias=True)\n",
       "        (1): ReLU(inplace=True)\n",
       "        (2): Dropout(p=0.1, inplace=False)\n",
       "        (3): Linear(in_features=2100, out_features=410, bias=True)\n",
       "        (4): Dropout(p=0.1, inplace=False)\n",
       "        (5): MergeLayer()\n",
       "        (6): LayerNorm((410,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (1): DecoderLayer(\n",
       "    (mhra): MultiHeadRelativeAttention(\n",
       "      (attention): Linear(in_features=410, out_features=1230, bias=False)\n",
       "      (out): Linear(in_features=410, out_features=410, bias=False)\n",
       "      (drop_att): Dropout(p=0.1, inplace=False)\n",
       "      (drop_res): Dropout(p=0.1, inplace=False)\n",
       "      (ln): LayerNorm((410,), eps=1e-05, elementwise_affine=True)\n",
       "      (r_attn): Linear(in_features=410, out_features=410, bias=False)\n",
       "    )\n",
       "    (ff): SequentialEx(\n",
       "      (layers): ModuleList(\n",
       "        (0): Linear(in_features=410, out_features=2100, bias=True)\n",
       "        (1): ReLU(inplace=True)\n",
       "        (2): Dropout(p=0.1, inplace=False)\n",
       "        (3): Linear(in_features=2100, out_features=410, bias=True)\n",
       "        (4): Dropout(p=0.1, inplace=False)\n",
       "        (5): MergeLayer()\n",
       "        (6): LayerNorm((410,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (2): DecoderLayer(\n",
       "    (mhra): MultiHeadRelativeAttention(\n",
       "      (attention): Linear(in_features=410, out_features=1230, bias=False)\n",
       "      (out): Linear(in_features=410, out_features=410, bias=False)\n",
       "      (drop_att): Dropout(p=0.1, inplace=False)\n",
       "      (drop_res): Dropout(p=0.1, inplace=False)\n",
       "      (ln): LayerNorm((410,), eps=1e-05, elementwise_affine=True)\n",
       "      (r_attn): Linear(in_features=410, out_features=410, bias=False)\n",
       "    )\n",
       "    (ff): SequentialEx(\n",
       "      (layers): ModuleList(\n",
       "        (0): Linear(in_features=410, out_features=2100, bias=True)\n",
       "        (1): ReLU(inplace=True)\n",
       "        (2): Dropout(p=0.1, inplace=False)\n",
       "        (3): Linear(in_features=2100, out_features=410, bias=True)\n",
       "        (4): Dropout(p=0.1, inplace=False)\n",
       "        (5): MergeLayer()\n",
       "        (6): LayerNorm((410,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (3): DecoderLayer(\n",
       "    (mhra): MultiHeadRelativeAttention(\n",
       "      (attention): Linear(in_features=410, out_features=1230, bias=False)\n",
       "      (out): Linear(in_features=410, out_features=410, bias=False)\n",
       "      (drop_att): Dropout(p=0.1, inplace=False)\n",
       "      (drop_res): Dropout(p=0.1, inplace=False)\n",
       "      (ln): LayerNorm((410,), eps=1e-05, elementwise_affine=True)\n",
       "      (r_attn): Linear(in_features=410, out_features=410, bias=False)\n",
       "    )\n",
       "    (ff): SequentialEx(\n",
       "      (layers): ModuleList(\n",
       "        (0): Linear(in_features=410, out_features=2100, bias=True)\n",
       "        (1): ReLU(inplace=True)\n",
       "        (2): Dropout(p=0.1, inplace=False)\n",
       "        (3): Linear(in_features=2100, out_features=410, bias=True)\n",
       "        (4): Dropout(p=0.1, inplace=False)\n",
       "        (5): MergeLayer()\n",
       "        (6): LayerNorm((410,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "), Sequential(\n",
       "  (0): DecoderLayer(\n",
       "    (mhra): MultiHeadRelativeAttention(\n",
       "      (attention): Linear(in_features=410, out_features=1230, bias=False)\n",
       "      (out): Linear(in_features=410, out_features=410, bias=False)\n",
       "      (drop_att): Dropout(p=0.1, inplace=False)\n",
       "      (drop_res): Dropout(p=0.1, inplace=False)\n",
       "      (ln): LayerNorm((410,), eps=1e-05, elementwise_affine=True)\n",
       "      (r_attn): Linear(in_features=410, out_features=410, bias=False)\n",
       "    )\n",
       "    (ff): SequentialEx(\n",
       "      (layers): ModuleList(\n",
       "        (0): Linear(in_features=410, out_features=2100, bias=True)\n",
       "        (1): ReLU(inplace=True)\n",
       "        (2): Dropout(p=0.1, inplace=False)\n",
       "        (3): Linear(in_features=2100, out_features=410, bias=True)\n",
       "        (4): Dropout(p=0.1, inplace=False)\n",
       "        (5): MergeLayer()\n",
       "        (6): LayerNorm((410,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (1): DecoderLayer(\n",
       "    (mhra): MultiHeadRelativeAttention(\n",
       "      (attention): Linear(in_features=410, out_features=1230, bias=False)\n",
       "      (out): Linear(in_features=410, out_features=410, bias=False)\n",
       "      (drop_att): Dropout(p=0.1, inplace=False)\n",
       "      (drop_res): Dropout(p=0.1, inplace=False)\n",
       "      (ln): LayerNorm((410,), eps=1e-05, elementwise_affine=True)\n",
       "      (r_attn): Linear(in_features=410, out_features=410, bias=False)\n",
       "    )\n",
       "    (ff): SequentialEx(\n",
       "      (layers): ModuleList(\n",
       "        (0): Linear(in_features=410, out_features=2100, bias=True)\n",
       "        (1): ReLU(inplace=True)\n",
       "        (2): Dropout(p=0.1, inplace=False)\n",
       "        (3): Linear(in_features=2100, out_features=410, bias=True)\n",
       "        (4): Dropout(p=0.1, inplace=False)\n",
       "        (5): MergeLayer()\n",
       "        (6): LayerNorm((410,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (2): DecoderLayer(\n",
       "    (mhra): MultiHeadRelativeAttention(\n",
       "      (attention): Linear(in_features=410, out_features=1230, bias=False)\n",
       "      (out): Linear(in_features=410, out_features=410, bias=False)\n",
       "      (drop_att): Dropout(p=0.1, inplace=False)\n",
       "      (drop_res): Dropout(p=0.1, inplace=False)\n",
       "      (ln): LayerNorm((410,), eps=1e-05, elementwise_affine=True)\n",
       "      (r_attn): Linear(in_features=410, out_features=410, bias=False)\n",
       "    )\n",
       "    (ff): SequentialEx(\n",
       "      (layers): ModuleList(\n",
       "        (0): Linear(in_features=410, out_features=2100, bias=True)\n",
       "        (1): ReLU(inplace=True)\n",
       "        (2): Dropout(p=0.1, inplace=False)\n",
       "        (3): Linear(in_features=2100, out_features=410, bias=True)\n",
       "        (4): Dropout(p=0.1, inplace=False)\n",
       "        (5): MergeLayer()\n",
       "        (6): LayerNorm((410,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (3): DecoderLayer(\n",
       "    (mhra): MultiHeadRelativeAttention(\n",
       "      (attention): Linear(in_features=410, out_features=1230, bias=False)\n",
       "      (out): Linear(in_features=410, out_features=410, bias=False)\n",
       "      (drop_att): Dropout(p=0.1, inplace=False)\n",
       "      (drop_res): Dropout(p=0.1, inplace=False)\n",
       "      (ln): LayerNorm((410,), eps=1e-05, elementwise_affine=True)\n",
       "      (r_attn): Linear(in_features=410, out_features=410, bias=False)\n",
       "    )\n",
       "    (ff): SequentialEx(\n",
       "      (layers): ModuleList(\n",
       "        (0): Linear(in_features=410, out_features=2100, bias=True)\n",
       "        (1): ReLU(inplace=True)\n",
       "        (2): Dropout(p=0.1, inplace=False)\n",
       "        (3): Linear(in_features=2100, out_features=410, bias=True)\n",
       "        (4): Dropout(p=0.1, inplace=False)\n",
       "        (5): MergeLayer()\n",
       "        (6): LayerNorm((410,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "), Sequential(\n",
       "  (0): Embedding(8000, 410)\n",
       "  (1): LinearDecoder(\n",
       "    (decoder): Linear(in_features=410, out_features=8000, bias=True)\n",
       "    (output_dp): RNNDropout()\n",
       "  )\n",
       ")], add_time=True, silent=False, cb_fns_registered=False)\n",
       "alpha: 2.0\n",
       "beta: 1.0], layer_groups=[Sequential(\n",
       "  (0): DecoderLayer(\n",
       "    (mhra): MultiHeadRelativeAttention(\n",
       "      (attention): Linear(in_features=410, out_features=1230, bias=False)\n",
       "      (out): Linear(in_features=410, out_features=410, bias=False)\n",
       "      (drop_att): Dropout(p=0.1, inplace=False)\n",
       "      (drop_res): Dropout(p=0.1, inplace=False)\n",
       "      (ln): LayerNorm((410,), eps=1e-05, elementwise_affine=True)\n",
       "      (r_attn): Linear(in_features=410, out_features=410, bias=False)\n",
       "    )\n",
       "    (ff): SequentialEx(\n",
       "      (layers): ModuleList(\n",
       "        (0): Linear(in_features=410, out_features=2100, bias=True)\n",
       "        (1): ReLU(inplace=True)\n",
       "        (2): Dropout(p=0.1, inplace=False)\n",
       "        (3): Linear(in_features=2100, out_features=410, bias=True)\n",
       "        (4): Dropout(p=0.1, inplace=False)\n",
       "        (5): MergeLayer()\n",
       "        (6): LayerNorm((410,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (1): DecoderLayer(\n",
       "    (mhra): MultiHeadRelativeAttention(\n",
       "      (attention): Linear(in_features=410, out_features=1230, bias=False)\n",
       "      (out): Linear(in_features=410, out_features=410, bias=False)\n",
       "      (drop_att): Dropout(p=0.1, inplace=False)\n",
       "      (drop_res): Dropout(p=0.1, inplace=False)\n",
       "      (ln): LayerNorm((410,), eps=1e-05, elementwise_affine=True)\n",
       "      (r_attn): Linear(in_features=410, out_features=410, bias=False)\n",
       "    )\n",
       "    (ff): SequentialEx(\n",
       "      (layers): ModuleList(\n",
       "        (0): Linear(in_features=410, out_features=2100, bias=True)\n",
       "        (1): ReLU(inplace=True)\n",
       "        (2): Dropout(p=0.1, inplace=False)\n",
       "        (3): Linear(in_features=2100, out_features=410, bias=True)\n",
       "        (4): Dropout(p=0.1, inplace=False)\n",
       "        (5): MergeLayer()\n",
       "        (6): LayerNorm((410,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (2): DecoderLayer(\n",
       "    (mhra): MultiHeadRelativeAttention(\n",
       "      (attention): Linear(in_features=410, out_features=1230, bias=False)\n",
       "      (out): Linear(in_features=410, out_features=410, bias=False)\n",
       "      (drop_att): Dropout(p=0.1, inplace=False)\n",
       "      (drop_res): Dropout(p=0.1, inplace=False)\n",
       "      (ln): LayerNorm((410,), eps=1e-05, elementwise_affine=True)\n",
       "      (r_attn): Linear(in_features=410, out_features=410, bias=False)\n",
       "    )\n",
       "    (ff): SequentialEx(\n",
       "      (layers): ModuleList(\n",
       "        (0): Linear(in_features=410, out_features=2100, bias=True)\n",
       "        (1): ReLU(inplace=True)\n",
       "        (2): Dropout(p=0.1, inplace=False)\n",
       "        (3): Linear(in_features=2100, out_features=410, bias=True)\n",
       "        (4): Dropout(p=0.1, inplace=False)\n",
       "        (5): MergeLayer()\n",
       "        (6): LayerNorm((410,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (3): DecoderLayer(\n",
       "    (mhra): MultiHeadRelativeAttention(\n",
       "      (attention): Linear(in_features=410, out_features=1230, bias=False)\n",
       "      (out): Linear(in_features=410, out_features=410, bias=False)\n",
       "      (drop_att): Dropout(p=0.1, inplace=False)\n",
       "      (drop_res): Dropout(p=0.1, inplace=False)\n",
       "      (ln): LayerNorm((410,), eps=1e-05, elementwise_affine=True)\n",
       "      (r_attn): Linear(in_features=410, out_features=410, bias=False)\n",
       "    )\n",
       "    (ff): SequentialEx(\n",
       "      (layers): ModuleList(\n",
       "        (0): Linear(in_features=410, out_features=2100, bias=True)\n",
       "        (1): ReLU(inplace=True)\n",
       "        (2): Dropout(p=0.1, inplace=False)\n",
       "        (3): Linear(in_features=2100, out_features=410, bias=True)\n",
       "        (4): Dropout(p=0.1, inplace=False)\n",
       "        (5): MergeLayer()\n",
       "        (6): LayerNorm((410,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (4): ParameterModule()\n",
       "  (5): ParameterModule()\n",
       "), Sequential(\n",
       "  (0): DecoderLayer(\n",
       "    (mhra): MultiHeadRelativeAttention(\n",
       "      (attention): Linear(in_features=410, out_features=1230, bias=False)\n",
       "      (out): Linear(in_features=410, out_features=410, bias=False)\n",
       "      (drop_att): Dropout(p=0.1, inplace=False)\n",
       "      (drop_res): Dropout(p=0.1, inplace=False)\n",
       "      (ln): LayerNorm((410,), eps=1e-05, elementwise_affine=True)\n",
       "      (r_attn): Linear(in_features=410, out_features=410, bias=False)\n",
       "    )\n",
       "    (ff): SequentialEx(\n",
       "      (layers): ModuleList(\n",
       "        (0): Linear(in_features=410, out_features=2100, bias=True)\n",
       "        (1): ReLU(inplace=True)\n",
       "        (2): Dropout(p=0.1, inplace=False)\n",
       "        (3): Linear(in_features=2100, out_features=410, bias=True)\n",
       "        (4): Dropout(p=0.1, inplace=False)\n",
       "        (5): MergeLayer()\n",
       "        (6): LayerNorm((410,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (1): DecoderLayer(\n",
       "    (mhra): MultiHeadRelativeAttention(\n",
       "      (attention): Linear(in_features=410, out_features=1230, bias=False)\n",
       "      (out): Linear(in_features=410, out_features=410, bias=False)\n",
       "      (drop_att): Dropout(p=0.1, inplace=False)\n",
       "      (drop_res): Dropout(p=0.1, inplace=False)\n",
       "      (ln): LayerNorm((410,), eps=1e-05, elementwise_affine=True)\n",
       "      (r_attn): Linear(in_features=410, out_features=410, bias=False)\n",
       "    )\n",
       "    (ff): SequentialEx(\n",
       "      (layers): ModuleList(\n",
       "        (0): Linear(in_features=410, out_features=2100, bias=True)\n",
       "        (1): ReLU(inplace=True)\n",
       "        (2): Dropout(p=0.1, inplace=False)\n",
       "        (3): Linear(in_features=2100, out_features=410, bias=True)\n",
       "        (4): Dropout(p=0.1, inplace=False)\n",
       "        (5): MergeLayer()\n",
       "        (6): LayerNorm((410,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (2): DecoderLayer(\n",
       "    (mhra): MultiHeadRelativeAttention(\n",
       "      (attention): Linear(in_features=410, out_features=1230, bias=False)\n",
       "      (out): Linear(in_features=410, out_features=410, bias=False)\n",
       "      (drop_att): Dropout(p=0.1, inplace=False)\n",
       "      (drop_res): Dropout(p=0.1, inplace=False)\n",
       "      (ln): LayerNorm((410,), eps=1e-05, elementwise_affine=True)\n",
       "      (r_attn): Linear(in_features=410, out_features=410, bias=False)\n",
       "    )\n",
       "    (ff): SequentialEx(\n",
       "      (layers): ModuleList(\n",
       "        (0): Linear(in_features=410, out_features=2100, bias=True)\n",
       "        (1): ReLU(inplace=True)\n",
       "        (2): Dropout(p=0.1, inplace=False)\n",
       "        (3): Linear(in_features=2100, out_features=410, bias=True)\n",
       "        (4): Dropout(p=0.1, inplace=False)\n",
       "        (5): MergeLayer()\n",
       "        (6): LayerNorm((410,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (3): DecoderLayer(\n",
       "    (mhra): MultiHeadRelativeAttention(\n",
       "      (attention): Linear(in_features=410, out_features=1230, bias=False)\n",
       "      (out): Linear(in_features=410, out_features=410, bias=False)\n",
       "      (drop_att): Dropout(p=0.1, inplace=False)\n",
       "      (drop_res): Dropout(p=0.1, inplace=False)\n",
       "      (ln): LayerNorm((410,), eps=1e-05, elementwise_affine=True)\n",
       "      (r_attn): Linear(in_features=410, out_features=410, bias=False)\n",
       "    )\n",
       "    (ff): SequentialEx(\n",
       "      (layers): ModuleList(\n",
       "        (0): Linear(in_features=410, out_features=2100, bias=True)\n",
       "        (1): ReLU(inplace=True)\n",
       "        (2): Dropout(p=0.1, inplace=False)\n",
       "        (3): Linear(in_features=2100, out_features=410, bias=True)\n",
       "        (4): Dropout(p=0.1, inplace=False)\n",
       "        (5): MergeLayer()\n",
       "        (6): LayerNorm((410,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "), Sequential(\n",
       "  (0): DecoderLayer(\n",
       "    (mhra): MultiHeadRelativeAttention(\n",
       "      (attention): Linear(in_features=410, out_features=1230, bias=False)\n",
       "      (out): Linear(in_features=410, out_features=410, bias=False)\n",
       "      (drop_att): Dropout(p=0.1, inplace=False)\n",
       "      (drop_res): Dropout(p=0.1, inplace=False)\n",
       "      (ln): LayerNorm((410,), eps=1e-05, elementwise_affine=True)\n",
       "      (r_attn): Linear(in_features=410, out_features=410, bias=False)\n",
       "    )\n",
       "    (ff): SequentialEx(\n",
       "      (layers): ModuleList(\n",
       "        (0): Linear(in_features=410, out_features=2100, bias=True)\n",
       "        (1): ReLU(inplace=True)\n",
       "        (2): Dropout(p=0.1, inplace=False)\n",
       "        (3): Linear(in_features=2100, out_features=410, bias=True)\n",
       "        (4): Dropout(p=0.1, inplace=False)\n",
       "        (5): MergeLayer()\n",
       "        (6): LayerNorm((410,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (1): DecoderLayer(\n",
       "    (mhra): MultiHeadRelativeAttention(\n",
       "      (attention): Linear(in_features=410, out_features=1230, bias=False)\n",
       "      (out): Linear(in_features=410, out_features=410, bias=False)\n",
       "      (drop_att): Dropout(p=0.1, inplace=False)\n",
       "      (drop_res): Dropout(p=0.1, inplace=False)\n",
       "      (ln): LayerNorm((410,), eps=1e-05, elementwise_affine=True)\n",
       "      (r_attn): Linear(in_features=410, out_features=410, bias=False)\n",
       "    )\n",
       "    (ff): SequentialEx(\n",
       "      (layers): ModuleList(\n",
       "        (0): Linear(in_features=410, out_features=2100, bias=True)\n",
       "        (1): ReLU(inplace=True)\n",
       "        (2): Dropout(p=0.1, inplace=False)\n",
       "        (3): Linear(in_features=2100, out_features=410, bias=True)\n",
       "        (4): Dropout(p=0.1, inplace=False)\n",
       "        (5): MergeLayer()\n",
       "        (6): LayerNorm((410,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (2): DecoderLayer(\n",
       "    (mhra): MultiHeadRelativeAttention(\n",
       "      (attention): Linear(in_features=410, out_features=1230, bias=False)\n",
       "      (out): Linear(in_features=410, out_features=410, bias=False)\n",
       "      (drop_att): Dropout(p=0.1, inplace=False)\n",
       "      (drop_res): Dropout(p=0.1, inplace=False)\n",
       "      (ln): LayerNorm((410,), eps=1e-05, elementwise_affine=True)\n",
       "      (r_attn): Linear(in_features=410, out_features=410, bias=False)\n",
       "    )\n",
       "    (ff): SequentialEx(\n",
       "      (layers): ModuleList(\n",
       "        (0): Linear(in_features=410, out_features=2100, bias=True)\n",
       "        (1): ReLU(inplace=True)\n",
       "        (2): Dropout(p=0.1, inplace=False)\n",
       "        (3): Linear(in_features=2100, out_features=410, bias=True)\n",
       "        (4): Dropout(p=0.1, inplace=False)\n",
       "        (5): MergeLayer()\n",
       "        (6): LayerNorm((410,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (3): DecoderLayer(\n",
       "    (mhra): MultiHeadRelativeAttention(\n",
       "      (attention): Linear(in_features=410, out_features=1230, bias=False)\n",
       "      (out): Linear(in_features=410, out_features=410, bias=False)\n",
       "      (drop_att): Dropout(p=0.1, inplace=False)\n",
       "      (drop_res): Dropout(p=0.1, inplace=False)\n",
       "      (ln): LayerNorm((410,), eps=1e-05, elementwise_affine=True)\n",
       "      (r_attn): Linear(in_features=410, out_features=410, bias=False)\n",
       "    )\n",
       "    (ff): SequentialEx(\n",
       "      (layers): ModuleList(\n",
       "        (0): Linear(in_features=410, out_features=2100, bias=True)\n",
       "        (1): ReLU(inplace=True)\n",
       "        (2): Dropout(p=0.1, inplace=False)\n",
       "        (3): Linear(in_features=2100, out_features=410, bias=True)\n",
       "        (4): Dropout(p=0.1, inplace=False)\n",
       "        (5): MergeLayer()\n",
       "        (6): LayerNorm((410,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "), Sequential(\n",
       "  (0): Embedding(8000, 410)\n",
       "  (1): LinearDecoder(\n",
       "    (decoder): Linear(in_features=410, out_features=8000, bias=True)\n",
       "    (output_dp): RNNDropout()\n",
       "  )\n",
       ")], add_time=True, silent=False, cb_fns_registered=False)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learn.load(f'transformerxl_{task_type}_save_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.recorder.plot_metrics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# learn.recorder.plot_losses()\n",
    "figure_plot = learn.recorder.plot_losses(return_fig=True)\n",
    "figure_plot.savefig(fname=f\"transformerxl_{task_type}_plot_losses.png\", format='png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "Image.open(f'/tf/main/nbs/mdling/transformer_xl/transformerxl_{task_type}_plot_losses.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def max_len(max_query, max_res, df):\n",
    "    return len(sp.EncodeAsPieces(df[0])) <= max_query and \\\n",
    "           len(sp.EncodeAsPieces(df[1])) <= max_res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vulnerability Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_clas_res(learn, \"xxbos \" + vuln_val[\"query\"][0] + tags[\"buggy\"],\n",
    "        sp, tags[\"buggy\"], n_toks = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_query = 1024\n",
    "max_res   = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "task_type = \"buggy\"\n",
    "vuln_trn, vuln_val, vuln_tst = read_data(data_path/task_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vuln_tst = tag_task(vuln_tst, task_type)\n",
    "vuln_tst = list(filter(partial(max_len, max_query, max_res), zip(vuln_tst[\"query\"], vuln_tst[\"res\"])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vuln_tst = pd.DataFrame({\"query\": [row[0] for row in vuln_tst],\n",
    "                         \"res\": [row[1] for row in vuln_tst]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vuln_tst.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(vuln_tst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc, prec, recal = eval_vuln(learn, vuln_tst, sp, tags[task_type], max_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc, prec, recal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!curl -X POST -H 'Content-type: application/json' --data '{\"text\":\"from: semeru tower 2\\nstatus: finished buggy eval\"}' https://hooks.slack.com/services/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"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comment Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comment = '''I wanted to play around with bitwise operators and specifically wanted to make a circular bitwise shift. So say that I have the number 101. Shifting it left 1 step should result in 011. Now when I try this example in Java, it just shows a bunch of zeros, like this:\n",
    "\n",
    "//Circular right shift\n",
    "private static void testCircular() {\n",
    "    int x = 37;\n",
    "    System.out.println(x + \" Is \" + Integer.toBinaryString(x));\n",
    "\n",
    "    x = (x &gt;&gt;&gt; 8) | (x &lt;&lt; (Integer.SIZE - 8));\n",
    "\n",
    "    System.out.println(x + \" Is \" + Integer.toBinaryString(x));\n",
    "}\n",
    "\n",
    "This gives me the following result:\n",
    "37 Is 100101\n",
    "620756992 Is 100101000000000000000000000000\n",
    "\n",
    "As you can see, it merly added trailing zeroes, not shifting anything. I also tried the state = Integer.rotateRight(state,8); method, it does the same thing. What am I missing here?\n",
    "\n",
    "Bit circular shift just shows a bunch of zeros\n",
    "\n",
    "Answer A:\n",
    "I think it works as expected, what You are missing is full representation of number in bits - when You print it's skipping zeros at the beginning. Integer is stored in 32 bits, so full representation looks like this:\n",
    "\n",
    "int x = 37;\n",
    "00000000000000000000000000100101\n",
    "\n",
    "x = (x &gt;&gt;&gt; 8) | (x &lt;&lt; (Integer.SIZE - 8));\n",
    "00100101000000000000000000000000\n",
    "\n",
    "EDIT\n",
    "\n",
    "Here is a method to get a full string representation of an Integer:\n",
    "\n",
    "public static String toBinaryStringWithLeadingZeros(int x) {\n",
    "    StringBuffer buf = new StringBuffer(32);\n",
    "    char[] arr = new char[Integer.numberOfLeadingZeros(x)];\n",
    "    Arrays.fill(arr, '0');\n",
    "    buf.append(arr);\n",
    "    buf.append(Integer.toBinaryString(x));\n",
    "    return buf.toString();\n",
    "}'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_seq_res(learn, \"xxbos \" + cmt_tst[\"res\"][2018] + tags[\"mthds_cmts\"],\n",
    "        sp, tags[\"mthds_cmts\"], n_toks = 128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_query = 512\n",
    "max_res   = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "task_type = \"mthds_cmts\"\n",
    "cmt_trn, cmt_val, cmt_tst = read_data(data_path/task_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "cmt_tst = tag_task(cmt_tst, task_type)\n",
    "cmt_tst = list(filter(partial(max_len, max_query, max_res), zip(cmt_tst[\"query\"], cmt_tst[\"res\"])))\n",
    "cmt_tst = cmt_tst[:int(len(cmt_tst) * 0.1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "cmt_tst = pd.DataFrame({\"query\": [row[0] for row in cmt_tst],\n",
    "                        \"res\": [row[1] for row in cmt_tst]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>query</th>\n",
       "      <th>res</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>public void removeAttributeValueFromEntity(int...</td>\n",
       "      <td>Removes the attribute value with the given ind...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>public static INDArray add(INDArray x, INDArra...</td>\n",
       "      <td>Broadcast add op. See: {@link BroadcastAddOp}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>public int deleteCascade(\\n\\t\\t\\tCollection&lt;Da...</td>\n",
       "      <td>Delete the collection of Data Column Constrain...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>public static ImmutableList&lt;JClassType&gt; filter...</td>\n",
       "      <td>&lt;p&gt;filterSubtypesForDeserialization&lt;/p&gt;\\n\\n@pa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>@XmlElementDecl(namespace = PROV_NS, name = \"h...</td>\n",
       "      <td>Create an instance of {@link JAXBElement }{@co...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               query  \\\n",
       "0  public void removeAttributeValueFromEntity(int...   \n",
       "1  public static INDArray add(INDArray x, INDArra...   \n",
       "2  public int deleteCascade(\\n\\t\\t\\tCollection<Da...   \n",
       "3  public static ImmutableList<JClassType> filter...   \n",
       "4  @XmlElementDecl(namespace = PROV_NS, name = \"h...   \n",
       "\n",
       "                                                 res  \n",
       "0  Removes the attribute value with the given ind...  \n",
       "1      Broadcast add op. See: {@link BroadcastAddOp}  \n",
       "2  Delete the collection of Data Column Constrain...  \n",
       "3  <p>filterSubtypesForDeserialization</p>\\n\\n@pa...  \n",
       "4  Create an instance of {@link JAXBElement }{@co...  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cmt_tst.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4355"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(cmt_tst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▍         | 211/4355 [07:19<2:24:22,  2.09s/it]"
     ]
    }
   ],
   "source": [
    "b1, b2, b3, b4, meteor, rouge_l, levenshtein, jaccard, preds = eval_txt(\n",
    "        learn, cmt_tst, sp, tags[task_type], max_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.25425230769230767,\n",
       " 0.16997400688863376,\n",
       " 0.12094082663605052,\n",
       " 0.09496436280137772)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean(b1), mean(b2), mean(b3), mean(b4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "rouge_l = np.array(rouge_l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.24651986222732492, array([0.386974, 0.383501, 0.35174 ]))"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean(meteor), np.mean(rouge_l, axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "27.78346727898967"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean(levenshtein)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 648x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig = box_whisker_plot(levenshtein, \"Levenshtein Distance\", \"Distance\", \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig.savefig(fname=f\"transformerxl_{task_type}_levenshtein.png\", format='png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.27870569460390354"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean(jaccard)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!curl -X POST -H 'Content-type: application/json' --data '{\"text\":\"from: semeru tower 2\\nstatus: finished comment eval\"}' https://hooks.slack.com/services/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"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### StackOverflow QA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = '''I use pdfbox to read a PDF file and I want to delete the line breaks in a string. I have used\n",
    "\n",
    " string = string.replaceAll(\"/n|/r|/t\", \"\");\n",
    "\n",
    "\n",
    "but it doesn't work. Please help me.\n",
    "\n",
    "this is the string from pdf\n",
    "\n",
    "\n",
    "how to delete line breaks in String read from a pdf file in java'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "get_seq_res(learn, \"xxbos \" + question + tags[\"so_posts\"],\n",
    "        sp, tags[\"so_posts\"], n_toks = 512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_query = 1024\n",
    "max_res   = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "task_type = \"so_posts\"\n",
    "so_trn, so_val, so_tst = read_data(data_path/task_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "so_tst = tag_task(so_tst, task_type)\n",
    "so_tst = list(filter(partial(max_len, max_query, max_res), zip(so_tst[\"query\"], so_tst[\"res\"])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "so_tst = pd.DataFrame({\"query\": [row[0] for row in so_tst],\n",
    "                        \"res\": [row[1] for row in so_tst]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "so_tst.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(so_tst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "b1, b2, b3, b4, meteor, rouge_l, levenshtein, jaccard, preds = eval_txt(\n",
    "    learn, so_tst, sp, tags[task_type], max_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "mean(b1), mean(b2), mean(b3), mean(b4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rouge_l = np.array(rouge_l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean(meteor), np.mean(rouge_l, axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean(levenshtein)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = box_whisker_plot(levenshtein, \"Levenshtein Distance\", \"Distance\", \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig.savefig(fname=f\"transformerxl_{task_type}_levenshtein.png\", format='png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean(cosine), mean(jaccard)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!curl -X POST -H 'Content-type: application/json' --data '{\"text\":\"from: semeru tower 2\\nstatus: finished so_qa eval\"}' https://hooks.slack.com/services/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"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
