{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Source Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai.text import *\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing fastai storage locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = Path(\"/tf/data/models/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Language model Pretraining"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that language models can use a lot of GPU, so you may need to decrease batchsize here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bs=128"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's grab the full dataset for what follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = Path(\"/tf/data/datasets/raw/raw_java/data00m_god-r\")\n",
    "path.ls()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data_lm = (TextList.from_folder(path, extensions={\".java\"},\n",
    "                                processor = [OpenFileProcessor(),\n",
    "                                             SPProcessor(lang=\"en\")])\n",
    "           #Inputs: all the text files in path\n",
    "            .filter_by_folder(include=['sm_train', 'sm_valid', 'sm_test']) \n",
    "#            #We may have other temp folders that contain text files so we only keep what's in train and test\n",
    "            .split_by_folder(valid='sm_valid', train='sm_train')\n",
    "           #We randomly split and keep 10% (10,000 reviews) for validation\n",
    "            .label_for_lm()           \n",
    "           #We want to do a language model so we label accordingly\n",
    "            .databunch(bs=bs))\n",
    "data_lm.save('data_lm.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have to use a special kind of `TextDataBunch` for the language model, that ignores the labels (that's why we put 0 everywhere), will shuffle the texts at each epoch before concatenating them all together (only for training, we don't shuffle for the validation set) and will send batches that read that text in order with targets that are the next word in the sentence.\n",
    "\n",
    "The line before being a bit long, we want to load quickly the final ids by using the following cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_lm = load_data(path, 'data_lm.pkl', bs=bs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_lm.show_batch(), len(data_lm.train_ds), len(data_lm.valid_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then put this in a learner object very easily with a model loaded with the pretrained weights. They'll be downloaded the first time you'll execute the following line and stored in `~/.fastai/models/` (or elsewhere if you specified different paths in your config file)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn = language_model_learner(data_lm, TransformerXL, drop_mult=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "learn.lr_find()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.recorder.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set hyperparameters\n",
    "max_lr = 5e-2\n",
    "moms = (0.5, 0.75)\n",
    "pct_strt = 0.02"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "learn.fit_one_cycle(1, max_lr, moms=moms, pct_start = pct_strt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.fit_one_cycle(5, max_lr, moms=moms, pct_start = pct_strt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.recorder.plot_losses()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.save(model_path, 'fit_head')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.load(model_path, 'fit_head');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To complete the fine-tuning, we can then unfeeze and launch a new training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.unfreeze(-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# learn.fit_one_cycle(10, 5e-4, moms=(0.8,0.7), pct_start = 0.02)\n",
    "learn.fit_one_cycle(1, slice(1e-2/(2.6**4),1e-2), moms=(0.8,0.7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.save(model_path, 'first')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.freeze_to(-3)\n",
    "learn.fit_one_cycle(1, slice(5e-3/(2.6**4),5e-3), moms=(0.8,0.7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.save('third')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.unfreeze()\n",
    "learn.fit_one_cycle(2, slice(1e-3/(2.6**4),1e-3), moms=(0.8,0.7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.save(model_path, 'fine_tuned')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! curl -X POST -H 'Content-type: application/json' --data '{\"text\":\"from: semeru tower 2\\nstatus: finished training TransformerXL\"}' https://hooks.slack.com/services/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"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.recorder.plot_metrics()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How good is our model? Well let's try to see what it predicts after a few given words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.load('fine_tuned');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEXT = \"public String get\"\n",
    "N_WORDS = 40\n",
    "N_SENTENCES = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\".join(learn.predict(TEXT, N_WORDS, temperature=0.75) for _ in range(N_SENTENCES)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have to save not only the model, but also its encoder, the part that's responsible for creating and updating the hidden state. For the next part, we don't care about the part that tries to guess the next word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.save_encoder('fine_tuned_enc')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we'll create a new data object that only grabs the labelled data and keeps those labels. Again, this line takes a bit of time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = untar_data(URLs.IMDB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_clas = (TextList.from_folder(path, vocab=data_lm.vocab, extensions={\".java\"},\n",
    "                                processor = [OpenFileProcessor(),\n",
    "                                             SPProcessor(lang=\"en\")])\n",
    "             .filter_by_folder(include=['sm_train', 'sm_valid'])\n",
    "             #grab all the text files in path\n",
    "             .split_by_folder(train='sm_train', valid='sm_valid')\n",
    "             #split by train and valid folder (that only keeps 'train' and 'test' so no need to filter)\n",
    "             .label_from_folder(classes=['before', 'after'])\n",
    "             #label them all with their folders\n",
    "             .databunch(bs=bs))\n",
    "\n",
    "data_clas.save('data_clas.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PosixPath('/tf/data/datasets/raw/raw_java/data00m_god-r/data_clas.pkl'),\n",
       " PosixPath('/tf/data/datasets/raw/raw_java/data00m_god-r/data_lm.pkl'),\n",
       " PosixPath('/tf/data/datasets/raw/raw_java/data00m_god-r/tmp'),\n",
       " PosixPath('/tf/data/datasets/raw/raw_java/data00m_god-r/sm_test'),\n",
       " PosixPath('/tf/data/datasets/raw/raw_java/data00m_god-r/sm_train'),\n",
       " PosixPath('/tf/data/datasets/raw/raw_java/data00m_god-r/train'),\n",
       " PosixPath('/tf/data/datasets/raw/raw_java/data00m_god-r/models'),\n",
       " PosixPath('/tf/data/datasets/raw/raw_java/data00m_god-r/tmp.sh'),\n",
       " PosixPath('/tf/data/datasets/raw/raw_java/data00m_god-r/valid'),\n",
       " PosixPath('/tf/data/datasets/raw/raw_java/data00m_god-r/sm_valid'),\n",
       " PosixPath('/tf/data/datasets/raw/raw_java/data00m_god-r/rename.sh'),\n",
       " PosixPath('/tf/data/datasets/raw/raw_java/data00m_god-r/test')]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = Path(\"/tf/data/datasets/raw/raw_java/data00m_god-r\")\n",
    "path.ls()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "bs = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ItemList??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_clas = TextClasDataBunch.from_csv(path, 'security-training.csv',\n",
    "                                       text_cols = 'code', label_cols = 'label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_clas.save('data_clas.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_clas = load_data(path, 'data_clas.pkl', bs=bs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>▁xxbos ▁@ java . lang . suppress warnings ( value ▁= ▁\" unchecked \") ▁private ▁void ▁initcomponents () ▁{ ▁ jframe help dialog ▁= ▁new ▁javax . swing . jframe (); ▁jpanel help dialog ▁= ▁new ▁javax . swing . jpanel (); ▁jscrollpane 8 ▁= ▁new ▁javax . swing . jscrollpane (); ▁ jtextarea help file ▁= ▁new ▁javax . swing . jtextarea (); ▁jpanel 1 ▁= ▁new ▁javax .</td>\n",
       "      <td>before</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>▁xxbos ▁private ▁static ▁void ▁init () ▁{ ▁ timezone mapper . poly [ 18 00 ] ▁= ▁new ▁ timezone mapper . tz polygon ( 35 . 499 57 3 f , ▁xxup ▁61 . 2 590 2 f , ▁xxup ▁35 . 52 172 f , ▁xxup ▁61 . 27 1 64 f , ▁xxup ▁35 . 61 3 167 f , ▁xxup ▁61 . 276 638 f ,</td>\n",
       "      <td>after</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>▁xxbos ▁@ java . lang . override ▁public ▁boolean ▁oncommand ( final ▁org . bukkit . command . commandsender ▁sender , ▁final ▁org . bukkit . command . command ▁command , ▁final ▁java . lang . string ▁label , ▁final ▁java . lang . string [] ▁split ) ▁{ ▁if ▁(!( sender ▁instanceof ▁org . bukkit . entity . player )) ▁{ ▁return ▁false ; ▁} ▁final ▁org . bukkit</td>\n",
       "      <td>after</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>▁xxbos ▁@ java . lang . override ▁public ▁java . lang . string ▁ getdestination ( java . lang . string ▁function , ▁com . strate lia . webactiv . kmelia . control . kmelia session controller ▁ kmelia , ▁org . silverpeas . servlet . httprequest ▁request ) ▁{ ▁com . strate lia . silverpeas . silver trace . silver trace . info (\" kmelia \", ▁\" kmelia request</td>\n",
       "      <td>before</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>▁xxbos ▁@ java . lang . override ▁protected ▁void ▁build ns d _ r ( lu . fisch . structorizer . parsers . reduction ▁_ reduction , ▁ lu . fisch . structorizer . elements . sub queue ▁_ parentnode ) ▁{ ▁if ▁(( _ reduction . size ()) ▁&gt; ▁0) ▁{ ▁java . lang . string ▁rule ▁= ▁_ reduction . getparent (). tostring (); ▁java . lang .</td>\n",
       "      <td>before</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data_clas.show_batch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50000"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data_clas.train_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then create a model to classify those reviews and load the encoder we saved before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RNNLearner(data=TextClasDataBunch;\n",
       "\n",
       "Train: LabelList (50000 items)\n",
       "x: TextList\n",
       "▁xxbos ▁public ▁void ▁return results ( java . util . arraylist < com . example . pi tur . new track om m ender . trackinfo > ▁query results ) ▁{ ▁if ▁( query results ▁!= ▁null ) ▁{ ▁this . query results ▁= ▁query results ; ▁adapter ▁= ▁new ▁com . example . pi tur . new track om m ender . trackinfo adapter ( this , ▁xxmaj ▁r . layout . list _ item , ▁query results ); ▁set listadapter ( adapter ); ▁set item clicklistener s ( this . get listview ()); ▁} else ▁{ ▁android . widget . toast . maketext ( this , ▁\" your ▁query ▁g ave ▁no ▁results . \", ▁toast . length _ long ) . show (); ▁} ▁},▁xxbos ▁public ▁static ▁synchronized ▁jsonobject ▁ handling exchange ( java . util . arraylist < java . lang . string > ▁server list , ▁java . util . arraylist < java . lang . string > ▁server list _ exchange , ▁java . util . arraylist < java . lang . string > ▁hostname list _ exchange , ▁java . util . arraylist < java . lang . string > ▁port list _ exchange ) ▁{ ▁jsonobject ▁server response ▁= ▁new ▁jsonobject (); ▁java . lang . string ▁response ; ▁java . lang . string ▁errormessage ; ▁java . lang . string ▁hostname pattern ▁= ▁\"^ ( ( [ a - za - z 0-9 ] |[ a - za - z 0-9 ][ a - za - z 0-9 ▁\\ ▁\\ ▁- ]* [ a - za - z 0-9 ]) ▁\\ ▁\\ ▁ . ) * ( [ a - za - z 0-9 ] |[ a - za - z 0-9 ][ a - za - z 0-9 ▁\\ ▁\\ ▁- ]* [ a - za - z 0-9 ]) $\" ; ▁java . lang . string ▁port pattern ▁= ▁\"^ ( [ 0 -5 ] ? ▁\\ ▁\\ ▁d ? ▁\\ ▁\\ ▁d ? ▁\\ ▁\\ ▁d ? ▁\\ ▁\\ ▁d | 6 [0-4] ▁\\ ▁\\ ▁d ▁\\ ▁\\ ▁d ▁\\ ▁\\ ▁d | 65 [0-4] ▁\\ ▁\\ ▁d ▁\\ ▁\\ ▁d | 655 [ 0 - 2 ] ▁\\ ▁\\ ▁d | 655 3 [ 0 -5 ]) $\" ; ▁for ▁( int ▁i ▁= ▁0; ▁i ▁< ▁( server list _ exchange . size ()); ▁i ++) ▁{ ▁if ▁(!( ( java . util . regex . pattern . matches ( hostname pattern , ▁hostname list _ exchange . get ( i ))) ▁&& ▁( java . util . regex . pattern . matches ( port pattern , ▁port list _ exchange . get ( i ▁xxrep ▁5 ▁) ▁{ ▁response ▁= ▁\" error \"; ▁errormessage ▁= ▁\" missing ▁resource template \"; ▁server response . put ( constant enum . commandtype . response . name (), ▁response ); ▁server response . put ( constant enum . command argument . errormessage . name (), ▁errormessage ); ▁return ▁server response ; ▁} else ▁if ▁(!( server list . contains ( server list _ exchange . get ( i ▁xxrep ▁4 ▁) ▁{ ▁server list . add ( server list _ exchange . get ( i )); ▁} ▁} ▁response ▁= ▁\" success \"; ▁server response . put ( constant enum . commandtype . response . name (), ▁response ); ▁return ▁server response ; ▁},▁xxbos ▁public ▁static ▁int ▁ converttime ( int ▁time , ▁int ▁duration ) ▁{ ▁int ▁temp ▁= ▁0; ▁if ▁((( time ▁% ▁100) ▁+ ▁duration ) ▁> ▁60) ▁{ ▁temp ▁= ▁duration ▁- ▁60 ; ▁time ▁+= ▁100 ▁+ ▁temp ; ▁} else ▁if ▁((( time ▁% ▁100) ▁+ ▁duration ) ▁== ▁60) ▁time ▁= ▁(( time ▁+ ▁100) ▁/ ▁100) ▁* ▁100 ; ▁else ▁time ▁+= ▁duration ; ▁return ▁time ; ▁},▁xxbos ▁public ▁static ▁float ▁convertto dp (@ android . support . annotation . nonnull ▁final ▁android . content . context ▁c , ▁final ▁float ▁px ) ▁{ ▁final ▁android . util . displaymetrics ▁m ▁= ▁c . getresources (). get displaymetrics (); ▁final ▁float ▁dp ▁= ▁android . util . typedvalue . apply dimension ( typedvalue . complex _ unit _ dip , ▁px , ▁m ); ▁t imber . log . t imber . d (\" convert ▁% f ▁px ▁to ▁% f ▁dp \", ▁px , ▁dp ); ▁return ▁dp ; ▁},▁xxbos ▁@ org . junit . test ▁public ▁void ▁test workflow pause resume () ▁throws ▁java . lang . exception ▁{ ▁java . lang . string ▁pause resume workflow app ▁= ▁\" pause resume workflow app \"; ▁java . lang . string ▁pause resume workflow ▁= ▁\" pause resume workflow \"; ▁java . io . file ▁first simple action file ▁= ▁new ▁java . io . file ((( tmp folder . new folder ()) ▁+ ▁\" ▁/ ▁first simple action . file \")); ▁java . io . file ▁first simple action done file ▁= ▁new ▁java . io . file ((( tmp folder . new folder ()) ▁+ ▁\" ▁/ ▁first simple action . file . done \")); ▁java . io . file ▁for kedsimpleaction file ▁= ▁new ▁java . io . file ((( tmp folder . new folder ()) ▁+ ▁\" ▁/ ▁for kedsimpleaction . file \")); ▁java . io . file ▁for kedsimpleaction done file ▁= ▁new ▁java . io . file ((( tmp folder . new folder ()) ▁+ ▁\" ▁/ ▁for kedsimpleaction . file . done \")); ▁java . io . file ▁another for kedsimpleaction file ▁= ▁new ▁java . io . file ((( tmp folder . new folder ()) ▁+ ▁\" ▁/ ▁another for kedsimpleaction . file \")); ▁java . io . file ▁another for kedsimpleaction done file ▁= ▁new ▁java . io . file ((( tmp folder . new folder ()) ▁+ ▁\" ▁/ ▁another for kedsimpleaction . file . done \")); ▁java . io . file ▁last simple action file ▁= ▁new ▁java . io . file ((( tmp folder . new folder ()) ▁+ ▁\" ▁/ ▁last simple action . file \")); ▁java . io . file ▁last simple action done file ▁= ▁new ▁java . io . file ((( tmp folder . new folder ()) ▁+ ▁\" ▁/ ▁last simple action . file . done \")); ▁org . apache . http . httpresponse ▁response ▁= ▁deploy ( co . c ask . cdap . pause resume work low app . class , ▁constants . gateway . api _ version _3_ token , ▁co . c ask . cdap . internal . app . services . http . handlers . test _ namespace 2); ▁org . junit . assert . assertequals (200 , ▁response . get statusline (). get statuscode ()); ▁co . c ask . cdap . proto . id . program ▁program id ▁= ▁id . program . from ( co . c ask . cdap . internal . app . services . http . handlers . test _ namespace 2, ▁pause resume workflow app , ▁program type . workflow , ▁pause resume workflow ); ▁java . util . map < java . lang . string , ▁java . lang . string > ▁runtime arguments ▁= ▁com . google . common . collect . maps . newhashmap (); ▁runtime arguments . put (\" first . simple . action . file \", ▁first simple action file . getabsolutepath ()); ▁runtime arguments . put (\" first . simple . action . done file \", ▁first simple action done file . getabsolutepath ()); ▁runtime arguments . put (\" fork ed . simple . action . file \", ▁for kedsimpleaction file . getabsolutepath ()); ▁runtime arguments . put (\" fork ed . simple . action . done file \", ▁for kedsimpleaction done file . getabsolutepath ()); ▁runtime arguments . put (\" another fork ed . simple . action . file \", ▁another for kedsimpleaction file . getabsolutepath ()); ▁runtime arguments . put (\" another fork ed . simple . action . done file \", ▁another for kedsimpleaction done file . getabsolutepath ()); ▁runtime arguments . put (\" last . simple . action . file \", ▁last simple action file . getabsolutepath ()); ▁runtime arguments . put (\" last . simple . action . done file \", ▁last simple action done file . getabsolutepath ()); ▁set and test runtime args ( programid , ▁runtime arguments ); ▁start program ( programid , ▁200); ▁wait state ( programid , ▁xxup ▁\" running \"); ▁java . util . list < co . c ask . cdap . proto . run record > ▁history runs ▁= ▁ getprogram runs ( programid , ▁\" running \"); ▁org . junit . assert . asserttrue ((( history runs . size ()) ▁== ▁1)); ▁java . lang . string ▁run id ▁= ▁history runs . get (0). getpid (); ▁while ▁(!( first simple action file . exists ())) ▁{ ▁java . util . concurrent . timeunit . milliseconds . sleep (50 ); ▁} ▁verify running program count ( programid , ▁run id , ▁1); ▁ suspend workflow ( programid , ▁run id , ▁200); ▁wait state ( programid , ▁xxup ▁\" suspended \"); ▁verify program runs ( programid , ▁\" suspended \"); ▁ suspend workflow ( programid , ▁run id , ▁4 09 ); ▁first simple action done file . createnewfile (); ▁verify running program count ( programid , ▁run id , ▁0); ▁verify program runs ( programid , ▁\" suspended \"); ▁resume workflow ( programid , ▁run id , ▁200); ▁wait state ( programid , ▁xxup ▁\" running \"); ▁verify program runs ( programid , ▁\" running \"); ▁resume workflow ( programid , ▁run id , ▁4 09 ); ▁while ▁(!( ( for kedsimpleaction file . exists ()) ▁&& ▁( another for kedsimpleaction file . exists ( ▁xxrep ▁4 ▁) ▁{ ▁java . util . concurrent . timeunit . milliseconds . sleep (50 ); ▁} ▁verify running program count ( programid , ▁run id , ▁2 ); ▁ suspend workflow ( programid , ▁run id , ▁200); ▁wait state ( programid , ▁xxup ▁\" suspended \"); ▁verify program runs ( programid , ▁\" suspended \"); ▁for kedsimpleaction done file . createnewfile (); ▁another for kedsimpleaction done file . createnewfile (); ▁verify running program count ( programid , ▁run id , ▁0); ▁verify program runs ( programid , ▁\" suspended \"); ▁org . junit . assert . asserttrue ((!( last simple action file . exists ( ▁xxrep ▁4 ▁) ▁; ▁resume workflow ( programid , ▁run id , ▁200); ▁wait state ( programid , ▁xxup ▁\" running \"); ▁while ▁(!( last simple action file . exists ())) ▁{ ▁java . util . concurrent . timeunit . seconds . sleep (1); ▁} ▁verify running program count ( programid , ▁run id , ▁1); ▁last simple action done file . createnewfile (); ▁verify program runs ( programid , ▁\" completed \"); ▁wait state ( programid , ▁xxup ▁\" stopped \"); ▁ suspend workflow ( programid , ▁run id , ▁404 ); ▁resume workflow ( programid , ▁run id , ▁404 ); ▁}\n",
       "y: CategoryList\n",
       "after,after,after,after,after\n",
       "Path: /tf/data/datasets/raw/raw_java/data00m_god-r;\n",
       "\n",
       "Valid: LabelList (50000 items)\n",
       "x: TextList\n",
       "▁xxbos ▁@ java . lang . override ▁public ▁int ▁onstartcommand ( android . content . intent ▁intent , ▁int ▁flags , ▁int ▁startid ) ▁{ ▁java . util . arraylist < java . lang . string > ▁missing ▁= ▁new ▁java . util . arraylist <>(); ▁for ▁( java . lang . string ▁p ▁: ▁xxup ▁required _ permissions ) ▁{ ▁int ▁permission _ access ▁= ▁android . support . v 4. content . context compat . checkself permission ( getapplication context (), ▁p ); ▁if ▁( permission _ access ▁!= ▁( android . content . pm . packagemanager . permission _ granted )) ▁{ ▁missing . add ( p ); ▁} ▁} ▁if ▁(( missing . size ()) ▁> ▁0) ▁{ ▁android . content . intent ▁permission request ▁= ▁new ▁android . content . intent ( this , ▁com . aware . ui . permissions handler . class ); ▁permission request . putextra ( permissions handler . extra _ required _ permissions , ▁missing . toarray ( new ▁java . lang . string [ missing . size ()]) ); ▁permission request . setflags ( intent . flag _ activity _ new _ task ); ▁startactivity ( permission request ); ▁} ▁com . aware . utils . aware _ sensor . tag ▁= ▁(( com . aware . aware . get setting ( getapplication context (), ▁ aware _ preferences . debug _ tag ) . length ()) ▁> ▁0) ▁? ▁com . aware . aware . get setting ( getapplication context (), ▁ aware _ preferences . debug _ tag ) ▁: ▁com . aware . utils . aware _ sensor . tag ; ▁com . aware . utils . aware _ sensor . debug ▁= ▁com . aware . aware . get setting ( getapplication context (), ▁ aware _ preferences . debug _ flag ) . equals (\" true \"); ▁if ▁( com . aware . utils . aware _ sensor . debug ) ▁android . util . log . d ( com . aware . utils . aware _ sensor . tag , ▁(( com . aware . utils . aware _ sensor . tag ) ▁+ ▁\" ▁sensor ▁active . . . \")); ▁return ▁xxup ▁start _ sticky ; ▁},▁xxbos ▁@ java . lang . override ▁protected ▁java . lang . string ▁get exclude suite filter ( java . lang . class <?> ▁ klass ) ▁throws ▁java . lang . exception ▁{ ▁nl . hsa c . fitness e . junit . hsa c fitness e runner . exclude suite filter ▁exclude suite filter annotation ▁= ▁(( nl . hsa c . fitness e . junit . hsa c fitness e runner . exclude suite filter ) ▁( klass . getannotation ( nl . hsa c . fitness e . junit . hsa c fitness e runner . exclude suite filter . class ))); ▁if ▁( exclude suite filter annotation ▁== ▁null ) ▁{ ▁return ▁null ; ▁} else ▁if ▁(!( \"\". equals ( exclude suite filter annotation . value ( ▁xxrep ▁4 ▁) ▁{ ▁return ▁exclude suite filter annotation . value (); ▁} else ▁if ▁(!( \"\". equals ( exclude suite filter annotation . system property ( ▁xxrep ▁4 ▁) ▁{ ▁return ▁java . lang . system . getproperty ( exclude suite filter annotation . system property ()); ▁} else ▁{ ▁throw ▁new ▁org . junit . runner s . model . initialization error (\" in ▁annotation ▁@ exclude suite filter ▁you ▁have ▁to ▁specify ▁either ▁\\ ▁' value ▁\\ ▁' ▁or ▁\\ ▁' system property ▁\\ ▁' \"); ▁} ▁},▁xxbos ▁private ▁java . lang . stringbuffer ▁dec line friend ( int ▁user 1 id , ▁int ▁user 2 id ) ▁{ ▁java . sql . statement ▁con ▁= ▁null ; ▁try ▁{ ▁con ▁= ▁db . createstatement (); ▁} ▁catch ▁( java . sql . sqlexception ▁e ) ▁{ ▁e . printstacktrace (); ▁} ▁java . sql . resultset ▁result ; ▁try ▁{ ▁result ▁= ▁con . executequery ▁xxrep ▁4 ▁( ▁xxup ▁\" update ▁friends ▁xxup ▁set ▁accepted ▁= ▁0 ▁xxup ▁where ▁user _1 ▁= \" ▁+ ▁user 1 id ) ▁+ ▁xxup ▁\" and ▁user _ 2 ▁= ▁\") ▁+ ▁user 2 id )); ▁} ▁catch ▁( java . sql . sqlexception ▁e ) ▁{ ▁e . printstacktrace (); ▁} ▁services . response ▁response ▁= ▁new ▁services . response ( utility . responsecode s . status _ ok ); ▁return ▁response . tojson (); ▁},▁xxbos ▁public ▁void ▁set point index ( int ▁point index ) ▁{ ▁if ▁(( point index ▁>= ▁0) ▁&& ▁( point index ▁< ▁( _ buffer . length ( ▁xxrep ▁4 ▁) ▁{ ▁_ point index ▁= ▁point index ; ▁} ▁},▁xxbos ▁public ▁void ▁change buffer data ( org . rajawali 3 d . buffer info ▁buffer info , ▁java . nio . buffer ▁new data , ▁int ▁index ) ▁{ ▁change buffer data ( buffer info , ▁new data , ▁index , ▁new data . capacity ()); ▁}\n",
       "y: CategoryList\n",
       "before,before,before,before,before\n",
       "Path: /tf/data/datasets/raw/raw_java/data00m_god-r;\n",
       "\n",
       "Test: None, model=SequentialRNN(\n",
       "  (0): MultiBatchEncoder(\n",
       "    (module): TransformerXL(\n",
       "      (encoder): Embedding(30000, 410)\n",
       "      (pos_enc): PositionalEncoding()\n",
       "      (drop_emb): Dropout(p=0.05, inplace=False)\n",
       "      (layers): ModuleList(\n",
       "        (0): DecoderLayer(\n",
       "          (mhra): MultiHeadRelativeAttention(\n",
       "            (attention): Linear(in_features=410, out_features=1230, bias=False)\n",
       "            (out): Linear(in_features=410, out_features=410, bias=False)\n",
       "            (drop_att): Dropout(p=0.05, inplace=False)\n",
       "            (drop_res): Dropout(p=0.05, inplace=False)\n",
       "            (ln): LayerNorm((410,), eps=1e-05, elementwise_affine=True)\n",
       "            (r_attn): Linear(in_features=410, out_features=410, bias=False)\n",
       "          )\n",
       "          (ff): SequentialEx(\n",
       "            (layers): ModuleList(\n",
       "              (0): Linear(in_features=410, out_features=2100, bias=True)\n",
       "              (1): ReLU(inplace=True)\n",
       "              (2): Dropout(p=0.05, inplace=False)\n",
       "              (3): Linear(in_features=2100, out_features=410, bias=True)\n",
       "              (4): Dropout(p=0.05, inplace=False)\n",
       "              (5): MergeLayer()\n",
       "              (6): LayerNorm((410,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (1): DecoderLayer(\n",
       "          (mhra): MultiHeadRelativeAttention(\n",
       "            (attention): Linear(in_features=410, out_features=1230, bias=False)\n",
       "            (out): Linear(in_features=410, out_features=410, bias=False)\n",
       "            (drop_att): Dropout(p=0.05, inplace=False)\n",
       "            (drop_res): Dropout(p=0.05, inplace=False)\n",
       "            (ln): LayerNorm((410,), eps=1e-05, elementwise_affine=True)\n",
       "            (r_attn): Linear(in_features=410, out_features=410, bias=False)\n",
       "          )\n",
       "          (ff): SequentialEx(\n",
       "            (layers): ModuleList(\n",
       "              (0): Linear(in_features=410, out_features=2100, bias=True)\n",
       "              (1): ReLU(inplace=True)\n",
       "              (2): Dropout(p=0.05, inplace=False)\n",
       "              (3): Linear(in_features=2100, out_features=410, bias=True)\n",
       "              (4): Dropout(p=0.05, inplace=False)\n",
       "              (5): MergeLayer()\n",
       "              (6): LayerNorm((410,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (2): DecoderLayer(\n",
       "          (mhra): MultiHeadRelativeAttention(\n",
       "            (attention): Linear(in_features=410, out_features=1230, bias=False)\n",
       "            (out): Linear(in_features=410, out_features=410, bias=False)\n",
       "            (drop_att): Dropout(p=0.05, inplace=False)\n",
       "            (drop_res): Dropout(p=0.05, inplace=False)\n",
       "            (ln): LayerNorm((410,), eps=1e-05, elementwise_affine=True)\n",
       "            (r_attn): Linear(in_features=410, out_features=410, bias=False)\n",
       "          )\n",
       "          (ff): SequentialEx(\n",
       "            (layers): ModuleList(\n",
       "              (0): Linear(in_features=410, out_features=2100, bias=True)\n",
       "              (1): ReLU(inplace=True)\n",
       "              (2): Dropout(p=0.05, inplace=False)\n",
       "              (3): Linear(in_features=2100, out_features=410, bias=True)\n",
       "              (4): Dropout(p=0.05, inplace=False)\n",
       "              (5): MergeLayer()\n",
       "              (6): LayerNorm((410,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (3): DecoderLayer(\n",
       "          (mhra): MultiHeadRelativeAttention(\n",
       "            (attention): Linear(in_features=410, out_features=1230, bias=False)\n",
       "            (out): Linear(in_features=410, out_features=410, bias=False)\n",
       "            (drop_att): Dropout(p=0.05, inplace=False)\n",
       "            (drop_res): Dropout(p=0.05, inplace=False)\n",
       "            (ln): LayerNorm((410,), eps=1e-05, elementwise_affine=True)\n",
       "            (r_attn): Linear(in_features=410, out_features=410, bias=False)\n",
       "          )\n",
       "          (ff): SequentialEx(\n",
       "            (layers): ModuleList(\n",
       "              (0): Linear(in_features=410, out_features=2100, bias=True)\n",
       "              (1): ReLU(inplace=True)\n",
       "              (2): Dropout(p=0.05, inplace=False)\n",
       "              (3): Linear(in_features=2100, out_features=410, bias=True)\n",
       "              (4): Dropout(p=0.05, inplace=False)\n",
       "              (5): MergeLayer()\n",
       "              (6): LayerNorm((410,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (4): DecoderLayer(\n",
       "          (mhra): MultiHeadRelativeAttention(\n",
       "            (attention): Linear(in_features=410, out_features=1230, bias=False)\n",
       "            (out): Linear(in_features=410, out_features=410, bias=False)\n",
       "            (drop_att): Dropout(p=0.05, inplace=False)\n",
       "            (drop_res): Dropout(p=0.05, inplace=False)\n",
       "            (ln): LayerNorm((410,), eps=1e-05, elementwise_affine=True)\n",
       "            (r_attn): Linear(in_features=410, out_features=410, bias=False)\n",
       "          )\n",
       "          (ff): SequentialEx(\n",
       "            (layers): ModuleList(\n",
       "              (0): Linear(in_features=410, out_features=2100, bias=True)\n",
       "              (1): ReLU(inplace=True)\n",
       "              (2): Dropout(p=0.05, inplace=False)\n",
       "              (3): Linear(in_features=2100, out_features=410, bias=True)\n",
       "              (4): Dropout(p=0.05, inplace=False)\n",
       "              (5): MergeLayer()\n",
       "              (6): LayerNorm((410,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (5): DecoderLayer(\n",
       "          (mhra): MultiHeadRelativeAttention(\n",
       "            (attention): Linear(in_features=410, out_features=1230, bias=False)\n",
       "            (out): Linear(in_features=410, out_features=410, bias=False)\n",
       "            (drop_att): Dropout(p=0.05, inplace=False)\n",
       "            (drop_res): Dropout(p=0.05, inplace=False)\n",
       "            (ln): LayerNorm((410,), eps=1e-05, elementwise_affine=True)\n",
       "            (r_attn): Linear(in_features=410, out_features=410, bias=False)\n",
       "          )\n",
       "          (ff): SequentialEx(\n",
       "            (layers): ModuleList(\n",
       "              (0): Linear(in_features=410, out_features=2100, bias=True)\n",
       "              (1): ReLU(inplace=True)\n",
       "              (2): Dropout(p=0.05, inplace=False)\n",
       "              (3): Linear(in_features=2100, out_features=410, bias=True)\n",
       "              (4): Dropout(p=0.05, inplace=False)\n",
       "              (5): MergeLayer()\n",
       "              (6): LayerNorm((410,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (6): DecoderLayer(\n",
       "          (mhra): MultiHeadRelativeAttention(\n",
       "            (attention): Linear(in_features=410, out_features=1230, bias=False)\n",
       "            (out): Linear(in_features=410, out_features=410, bias=False)\n",
       "            (drop_att): Dropout(p=0.05, inplace=False)\n",
       "            (drop_res): Dropout(p=0.05, inplace=False)\n",
       "            (ln): LayerNorm((410,), eps=1e-05, elementwise_affine=True)\n",
       "            (r_attn): Linear(in_features=410, out_features=410, bias=False)\n",
       "          )\n",
       "          (ff): SequentialEx(\n",
       "            (layers): ModuleList(\n",
       "              (0): Linear(in_features=410, out_features=2100, bias=True)\n",
       "              (1): ReLU(inplace=True)\n",
       "              (2): Dropout(p=0.05, inplace=False)\n",
       "              (3): Linear(in_features=2100, out_features=410, bias=True)\n",
       "              (4): Dropout(p=0.05, inplace=False)\n",
       "              (5): MergeLayer()\n",
       "              (6): LayerNorm((410,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (7): DecoderLayer(\n",
       "          (mhra): MultiHeadRelativeAttention(\n",
       "            (attention): Linear(in_features=410, out_features=1230, bias=False)\n",
       "            (out): Linear(in_features=410, out_features=410, bias=False)\n",
       "            (drop_att): Dropout(p=0.05, inplace=False)\n",
       "            (drop_res): Dropout(p=0.05, inplace=False)\n",
       "            (ln): LayerNorm((410,), eps=1e-05, elementwise_affine=True)\n",
       "            (r_attn): Linear(in_features=410, out_features=410, bias=False)\n",
       "          )\n",
       "          (ff): SequentialEx(\n",
       "            (layers): ModuleList(\n",
       "              (0): Linear(in_features=410, out_features=2100, bias=True)\n",
       "              (1): ReLU(inplace=True)\n",
       "              (2): Dropout(p=0.05, inplace=False)\n",
       "              (3): Linear(in_features=2100, out_features=410, bias=True)\n",
       "              (4): Dropout(p=0.05, inplace=False)\n",
       "              (5): MergeLayer()\n",
       "              (6): LayerNorm((410,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (8): DecoderLayer(\n",
       "          (mhra): MultiHeadRelativeAttention(\n",
       "            (attention): Linear(in_features=410, out_features=1230, bias=False)\n",
       "            (out): Linear(in_features=410, out_features=410, bias=False)\n",
       "            (drop_att): Dropout(p=0.05, inplace=False)\n",
       "            (drop_res): Dropout(p=0.05, inplace=False)\n",
       "            (ln): LayerNorm((410,), eps=1e-05, elementwise_affine=True)\n",
       "            (r_attn): Linear(in_features=410, out_features=410, bias=False)\n",
       "          )\n",
       "          (ff): SequentialEx(\n",
       "            (layers): ModuleList(\n",
       "              (0): Linear(in_features=410, out_features=2100, bias=True)\n",
       "              (1): ReLU(inplace=True)\n",
       "              (2): Dropout(p=0.05, inplace=False)\n",
       "              (3): Linear(in_features=2100, out_features=410, bias=True)\n",
       "              (4): Dropout(p=0.05, inplace=False)\n",
       "              (5): MergeLayer()\n",
       "              (6): LayerNorm((410,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (9): DecoderLayer(\n",
       "          (mhra): MultiHeadRelativeAttention(\n",
       "            (attention): Linear(in_features=410, out_features=1230, bias=False)\n",
       "            (out): Linear(in_features=410, out_features=410, bias=False)\n",
       "            (drop_att): Dropout(p=0.05, inplace=False)\n",
       "            (drop_res): Dropout(p=0.05, inplace=False)\n",
       "            (ln): LayerNorm((410,), eps=1e-05, elementwise_affine=True)\n",
       "            (r_attn): Linear(in_features=410, out_features=410, bias=False)\n",
       "          )\n",
       "          (ff): SequentialEx(\n",
       "            (layers): ModuleList(\n",
       "              (0): Linear(in_features=410, out_features=2100, bias=True)\n",
       "              (1): ReLU(inplace=True)\n",
       "              (2): Dropout(p=0.05, inplace=False)\n",
       "              (3): Linear(in_features=2100, out_features=410, bias=True)\n",
       "              (4): Dropout(p=0.05, inplace=False)\n",
       "              (5): MergeLayer()\n",
       "              (6): LayerNorm((410,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (10): DecoderLayer(\n",
       "          (mhra): MultiHeadRelativeAttention(\n",
       "            (attention): Linear(in_features=410, out_features=1230, bias=False)\n",
       "            (out): Linear(in_features=410, out_features=410, bias=False)\n",
       "            (drop_att): Dropout(p=0.05, inplace=False)\n",
       "            (drop_res): Dropout(p=0.05, inplace=False)\n",
       "            (ln): LayerNorm((410,), eps=1e-05, elementwise_affine=True)\n",
       "            (r_attn): Linear(in_features=410, out_features=410, bias=False)\n",
       "          )\n",
       "          (ff): SequentialEx(\n",
       "            (layers): ModuleList(\n",
       "              (0): Linear(in_features=410, out_features=2100, bias=True)\n",
       "              (1): ReLU(inplace=True)\n",
       "              (2): Dropout(p=0.05, inplace=False)\n",
       "              (3): Linear(in_features=2100, out_features=410, bias=True)\n",
       "              (4): Dropout(p=0.05, inplace=False)\n",
       "              (5): MergeLayer()\n",
       "              (6): LayerNorm((410,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (11): DecoderLayer(\n",
       "          (mhra): MultiHeadRelativeAttention(\n",
       "            (attention): Linear(in_features=410, out_features=1230, bias=False)\n",
       "            (out): Linear(in_features=410, out_features=410, bias=False)\n",
       "            (drop_att): Dropout(p=0.05, inplace=False)\n",
       "            (drop_res): Dropout(p=0.05, inplace=False)\n",
       "            (ln): LayerNorm((410,), eps=1e-05, elementwise_affine=True)\n",
       "            (r_attn): Linear(in_features=410, out_features=410, bias=False)\n",
       "          )\n",
       "          (ff): SequentialEx(\n",
       "            (layers): ModuleList(\n",
       "              (0): Linear(in_features=410, out_features=2100, bias=True)\n",
       "              (1): ReLU(inplace=True)\n",
       "              (2): Dropout(p=0.05, inplace=False)\n",
       "              (3): Linear(in_features=2100, out_features=410, bias=True)\n",
       "              (4): Dropout(p=0.05, inplace=False)\n",
       "              (5): MergeLayer()\n",
       "              (6): LayerNorm((410,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (1): PoolingLinearClassifier(\n",
       "    (layers): Sequential(\n",
       "      (0): BatchNorm1d(1230, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (1): Dropout(p=0.05, inplace=False)\n",
       "      (2): Linear(in_features=1230, out_features=50, bias=True)\n",
       "      (3): ReLU(inplace=True)\n",
       "      (4): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (5): Dropout(p=0.1, inplace=False)\n",
       "      (6): Linear(in_features=50, out_features=2, bias=True)\n",
       "    )\n",
       "  )\n",
       "), opt_func=functools.partial(<class 'torch.optim.adam.Adam'>, betas=(0.9, 0.99)), loss_func=FlattenedLoss of CrossEntropyLoss(), metrics=[<function accuracy at 0x7f152760dbf8>], true_wd=True, bn_wd=True, wd=0.01, train_bn=True, path=PosixPath('/tf/data/datasets/raw/raw_java/data00m_god-r'), model_dir='models', callback_fns=[functools.partial(<class 'fastai.basic_train.Recorder'>, add_time=True, silent=False)], callbacks=[RNNTrainer\n",
       "learn: RNNLearner(data=TextClasDataBunch;\n",
       "\n",
       "Train: LabelList (50000 items)\n",
       "x: TextList\n",
       "▁xxbos ▁public ▁void ▁return results ( java . util . arraylist < com . example . pi tur . new track om m ender . trackinfo > ▁query results ) ▁{ ▁if ▁( query results ▁!= ▁null ) ▁{ ▁this . query results ▁= ▁query results ; ▁adapter ▁= ▁new ▁com . example . pi tur . new track om m ender . trackinfo adapter ( this , ▁xxmaj ▁r . layout . list _ item , ▁query results ); ▁set listadapter ( adapter ); ▁set item clicklistener s ( this . get listview ()); ▁} else ▁{ ▁android . widget . toast . maketext ( this , ▁\" your ▁query ▁g ave ▁no ▁results . \", ▁toast . length _ long ) . show (); ▁} ▁},▁xxbos ▁public ▁static ▁synchronized ▁jsonobject ▁ handling exchange ( java . util . arraylist < java . lang . string > ▁server list , ▁java . util . arraylist < java . lang . string > ▁server list _ exchange , ▁java . util . arraylist < java . lang . string > ▁hostname list _ exchange , ▁java . util . arraylist < java . lang . string > ▁port list _ exchange ) ▁{ ▁jsonobject ▁server response ▁= ▁new ▁jsonobject (); ▁java . lang . string ▁response ; ▁java . lang . string ▁errormessage ; ▁java . lang . string ▁hostname pattern ▁= ▁\"^ ( ( [ a - za - z 0-9 ] |[ a - za - z 0-9 ][ a - za - z 0-9 ▁\\ ▁\\ ▁- ]* [ a - za - z 0-9 ]) ▁\\ ▁\\ ▁ . ) * ( [ a - za - z 0-9 ] |[ a - za - z 0-9 ][ a - za - z 0-9 ▁\\ ▁\\ ▁- ]* [ a - za - z 0-9 ]) $\" ; ▁java . lang . string ▁port pattern ▁= ▁\"^ ( [ 0 -5 ] ? ▁\\ ▁\\ ▁d ? ▁\\ ▁\\ ▁d ? ▁\\ ▁\\ ▁d ? ▁\\ ▁\\ ▁d | 6 [0-4] ▁\\ ▁\\ ▁d ▁\\ ▁\\ ▁d ▁\\ ▁\\ ▁d | 65 [0-4] ▁\\ ▁\\ ▁d ▁\\ ▁\\ ▁d | 655 [ 0 - 2 ] ▁\\ ▁\\ ▁d | 655 3 [ 0 -5 ]) $\" ; ▁for ▁( int ▁i ▁= ▁0; ▁i ▁< ▁( server list _ exchange . size ()); ▁i ++) ▁{ ▁if ▁(!( ( java . util . regex . pattern . matches ( hostname pattern , ▁hostname list _ exchange . get ( i ))) ▁&& ▁( java . util . regex . pattern . matches ( port pattern , ▁port list _ exchange . get ( i ▁xxrep ▁5 ▁) ▁{ ▁response ▁= ▁\" error \"; ▁errormessage ▁= ▁\" missing ▁resource template \"; ▁server response . put ( constant enum . commandtype . response . name (), ▁response ); ▁server response . put ( constant enum . command argument . errormessage . name (), ▁errormessage ); ▁return ▁server response ; ▁} else ▁if ▁(!( server list . contains ( server list _ exchange . get ( i ▁xxrep ▁4 ▁) ▁{ ▁server list . add ( server list _ exchange . get ( i )); ▁} ▁} ▁response ▁= ▁\" success \"; ▁server response . put ( constant enum . commandtype . response . name (), ▁response ); ▁return ▁server response ; ▁},▁xxbos ▁public ▁static ▁int ▁ converttime ( int ▁time , ▁int ▁duration ) ▁{ ▁int ▁temp ▁= ▁0; ▁if ▁((( time ▁% ▁100) ▁+ ▁duration ) ▁> ▁60) ▁{ ▁temp ▁= ▁duration ▁- ▁60 ; ▁time ▁+= ▁100 ▁+ ▁temp ; ▁} else ▁if ▁((( time ▁% ▁100) ▁+ ▁duration ) ▁== ▁60) ▁time ▁= ▁(( time ▁+ ▁100) ▁/ ▁100) ▁* ▁100 ; ▁else ▁time ▁+= ▁duration ; ▁return ▁time ; ▁},▁xxbos ▁public ▁static ▁float ▁convertto dp (@ android . support . annotation . nonnull ▁final ▁android . content . context ▁c , ▁final ▁float ▁px ) ▁{ ▁final ▁android . util . displaymetrics ▁m ▁= ▁c . getresources (). get displaymetrics (); ▁final ▁float ▁dp ▁= ▁android . util . typedvalue . apply dimension ( typedvalue . complex _ unit _ dip , ▁px , ▁m ); ▁t imber . log . t imber . d (\" convert ▁% f ▁px ▁to ▁% f ▁dp \", ▁px , ▁dp ); ▁return ▁dp ; ▁},▁xxbos ▁@ org . junit . test ▁public ▁void ▁test workflow pause resume () ▁throws ▁java . lang . exception ▁{ ▁java . lang . string ▁pause resume workflow app ▁= ▁\" pause resume workflow app \"; ▁java . lang . string ▁pause resume workflow ▁= ▁\" pause resume workflow \"; ▁java . io . file ▁first simple action file ▁= ▁new ▁java . io . file ((( tmp folder . new folder ()) ▁+ ▁\" ▁/ ▁first simple action . file \")); ▁java . io . file ▁first simple action done file ▁= ▁new ▁java . io . file ((( tmp folder . new folder ()) ▁+ ▁\" ▁/ ▁first simple action . file . done \")); ▁java . io . file ▁for kedsimpleaction file ▁= ▁new ▁java . io . file ((( tmp folder . new folder ()) ▁+ ▁\" ▁/ ▁for kedsimpleaction . file \")); ▁java . io . file ▁for kedsimpleaction done file ▁= ▁new ▁java . io . file ((( tmp folder . new folder ()) ▁+ ▁\" ▁/ ▁for kedsimpleaction . file . done \")); ▁java . io . file ▁another for kedsimpleaction file ▁= ▁new ▁java . io . file ((( tmp folder . new folder ()) ▁+ ▁\" ▁/ ▁another for kedsimpleaction . file \")); ▁java . io . file ▁another for kedsimpleaction done file ▁= ▁new ▁java . io . file ((( tmp folder . new folder ()) ▁+ ▁\" ▁/ ▁another for kedsimpleaction . file . done \")); ▁java . io . file ▁last simple action file ▁= ▁new ▁java . io . file ((( tmp folder . new folder ()) ▁+ ▁\" ▁/ ▁last simple action . file \")); ▁java . io . file ▁last simple action done file ▁= ▁new ▁java . io . file ((( tmp folder . new folder ()) ▁+ ▁\" ▁/ ▁last simple action . file . done \")); ▁org . apache . http . httpresponse ▁response ▁= ▁deploy ( co . c ask . cdap . pause resume work low app . class , ▁constants . gateway . api _ version _3_ token , ▁co . c ask . cdap . internal . app . services . http . handlers . test _ namespace 2); ▁org . junit . assert . assertequals (200 , ▁response . get statusline (). get statuscode ()); ▁co . c ask . cdap . proto . id . program ▁program id ▁= ▁id . program . from ( co . c ask . cdap . internal . app . services . http . handlers . test _ namespace 2, ▁pause resume workflow app , ▁program type . workflow , ▁pause resume workflow ); ▁java . util . map < java . lang . string , ▁java . lang . string > ▁runtime arguments ▁= ▁com . google . common . collect . maps . newhashmap (); ▁runtime arguments . put (\" first . simple . action . file \", ▁first simple action file . getabsolutepath ()); ▁runtime arguments . put (\" first . simple . action . done file \", ▁first simple action done file . getabsolutepath ()); ▁runtime arguments . put (\" fork ed . simple . action . file \", ▁for kedsimpleaction file . getabsolutepath ()); ▁runtime arguments . put (\" fork ed . simple . action . done file \", ▁for kedsimpleaction done file . getabsolutepath ()); ▁runtime arguments . put (\" another fork ed . simple . action . file \", ▁another for kedsimpleaction file . getabsolutepath ()); ▁runtime arguments . put (\" another fork ed . simple . action . done file \", ▁another for kedsimpleaction done file . getabsolutepath ()); ▁runtime arguments . put (\" last . simple . action . file \", ▁last simple action file . getabsolutepath ()); ▁runtime arguments . put (\" last . simple . action . done file \", ▁last simple action done file . getabsolutepath ()); ▁set and test runtime args ( programid , ▁runtime arguments ); ▁start program ( programid , ▁200); ▁wait state ( programid , ▁xxup ▁\" running \"); ▁java . util . list < co . c ask . cdap . proto . run record > ▁history runs ▁= ▁ getprogram runs ( programid , ▁\" running \"); ▁org . junit . assert . asserttrue ((( history runs . size ()) ▁== ▁1)); ▁java . lang . string ▁run id ▁= ▁history runs . get (0). getpid (); ▁while ▁(!( first simple action file . exists ())) ▁{ ▁java . util . concurrent . timeunit . milliseconds . sleep (50 ); ▁} ▁verify running program count ( programid , ▁run id , ▁1); ▁ suspend workflow ( programid , ▁run id , ▁200); ▁wait state ( programid , ▁xxup ▁\" suspended \"); ▁verify program runs ( programid , ▁\" suspended \"); ▁ suspend workflow ( programid , ▁run id , ▁4 09 ); ▁first simple action done file . createnewfile (); ▁verify running program count ( programid , ▁run id , ▁0); ▁verify program runs ( programid , ▁\" suspended \"); ▁resume workflow ( programid , ▁run id , ▁200); ▁wait state ( programid , ▁xxup ▁\" running \"); ▁verify program runs ( programid , ▁\" running \"); ▁resume workflow ( programid , ▁run id , ▁4 09 ); ▁while ▁(!( ( for kedsimpleaction file . exists ()) ▁&& ▁( another for kedsimpleaction file . exists ( ▁xxrep ▁4 ▁) ▁{ ▁java . util . concurrent . timeunit . milliseconds . sleep (50 ); ▁} ▁verify running program count ( programid , ▁run id , ▁2 ); ▁ suspend workflow ( programid , ▁run id , ▁200); ▁wait state ( programid , ▁xxup ▁\" suspended \"); ▁verify program runs ( programid , ▁\" suspended \"); ▁for kedsimpleaction done file . createnewfile (); ▁another for kedsimpleaction done file . createnewfile (); ▁verify running program count ( programid , ▁run id , ▁0); ▁verify program runs ( programid , ▁\" suspended \"); ▁org . junit . assert . asserttrue ((!( last simple action file . exists ( ▁xxrep ▁4 ▁) ▁; ▁resume workflow ( programid , ▁run id , ▁200); ▁wait state ( programid , ▁xxup ▁\" running \"); ▁while ▁(!( last simple action file . exists ())) ▁{ ▁java . util . concurrent . timeunit . seconds . sleep (1); ▁} ▁verify running program count ( programid , ▁run id , ▁1); ▁last simple action done file . createnewfile (); ▁verify program runs ( programid , ▁\" completed \"); ▁wait state ( programid , ▁xxup ▁\" stopped \"); ▁ suspend workflow ( programid , ▁run id , ▁404 ); ▁resume workflow ( programid , ▁run id , ▁404 ); ▁}\n",
       "y: CategoryList\n",
       "after,after,after,after,after\n",
       "Path: /tf/data/datasets/raw/raw_java/data00m_god-r;\n",
       "\n",
       "Valid: LabelList (50000 items)\n",
       "x: TextList\n",
       "▁xxbos ▁@ java . lang . override ▁public ▁int ▁onstartcommand ( android . content . intent ▁intent , ▁int ▁flags , ▁int ▁startid ) ▁{ ▁java . util . arraylist < java . lang . string > ▁missing ▁= ▁new ▁java . util . arraylist <>(); ▁for ▁( java . lang . string ▁p ▁: ▁xxup ▁required _ permissions ) ▁{ ▁int ▁permission _ access ▁= ▁android . support . v 4. content . context compat . checkself permission ( getapplication context (), ▁p ); ▁if ▁( permission _ access ▁!= ▁( android . content . pm . packagemanager . permission _ granted )) ▁{ ▁missing . add ( p ); ▁} ▁} ▁if ▁(( missing . size ()) ▁> ▁0) ▁{ ▁android . content . intent ▁permission request ▁= ▁new ▁android . content . intent ( this , ▁com . aware . ui . permissions handler . class ); ▁permission request . putextra ( permissions handler . extra _ required _ permissions , ▁missing . toarray ( new ▁java . lang . string [ missing . size ()]) ); ▁permission request . setflags ( intent . flag _ activity _ new _ task ); ▁startactivity ( permission request ); ▁} ▁com . aware . utils . aware _ sensor . tag ▁= ▁(( com . aware . aware . get setting ( getapplication context (), ▁ aware _ preferences . debug _ tag ) . length ()) ▁> ▁0) ▁? ▁com . aware . aware . get setting ( getapplication context (), ▁ aware _ preferences . debug _ tag ) ▁: ▁com . aware . utils . aware _ sensor . tag ; ▁com . aware . utils . aware _ sensor . debug ▁= ▁com . aware . aware . get setting ( getapplication context (), ▁ aware _ preferences . debug _ flag ) . equals (\" true \"); ▁if ▁( com . aware . utils . aware _ sensor . debug ) ▁android . util . log . d ( com . aware . utils . aware _ sensor . tag , ▁(( com . aware . utils . aware _ sensor . tag ) ▁+ ▁\" ▁sensor ▁active . . . \")); ▁return ▁xxup ▁start _ sticky ; ▁},▁xxbos ▁@ java . lang . override ▁protected ▁java . lang . string ▁get exclude suite filter ( java . lang . class <?> ▁ klass ) ▁throws ▁java . lang . exception ▁{ ▁nl . hsa c . fitness e . junit . hsa c fitness e runner . exclude suite filter ▁exclude suite filter annotation ▁= ▁(( nl . hsa c . fitness e . junit . hsa c fitness e runner . exclude suite filter ) ▁( klass . getannotation ( nl . hsa c . fitness e . junit . hsa c fitness e runner . exclude suite filter . class ))); ▁if ▁( exclude suite filter annotation ▁== ▁null ) ▁{ ▁return ▁null ; ▁} else ▁if ▁(!( \"\". equals ( exclude suite filter annotation . value ( ▁xxrep ▁4 ▁) ▁{ ▁return ▁exclude suite filter annotation . value (); ▁} else ▁if ▁(!( \"\". equals ( exclude suite filter annotation . system property ( ▁xxrep ▁4 ▁) ▁{ ▁return ▁java . lang . system . getproperty ( exclude suite filter annotation . system property ()); ▁} else ▁{ ▁throw ▁new ▁org . junit . runner s . model . initialization error (\" in ▁annotation ▁@ exclude suite filter ▁you ▁have ▁to ▁specify ▁either ▁\\ ▁' value ▁\\ ▁' ▁or ▁\\ ▁' system property ▁\\ ▁' \"); ▁} ▁},▁xxbos ▁private ▁java . lang . stringbuffer ▁dec line friend ( int ▁user 1 id , ▁int ▁user 2 id ) ▁{ ▁java . sql . statement ▁con ▁= ▁null ; ▁try ▁{ ▁con ▁= ▁db . createstatement (); ▁} ▁catch ▁( java . sql . sqlexception ▁e ) ▁{ ▁e . printstacktrace (); ▁} ▁java . sql . resultset ▁result ; ▁try ▁{ ▁result ▁= ▁con . executequery ▁xxrep ▁4 ▁( ▁xxup ▁\" update ▁friends ▁xxup ▁set ▁accepted ▁= ▁0 ▁xxup ▁where ▁user _1 ▁= \" ▁+ ▁user 1 id ) ▁+ ▁xxup ▁\" and ▁user _ 2 ▁= ▁\") ▁+ ▁user 2 id )); ▁} ▁catch ▁( java . sql . sqlexception ▁e ) ▁{ ▁e . printstacktrace (); ▁} ▁services . response ▁response ▁= ▁new ▁services . response ( utility . responsecode s . status _ ok ); ▁return ▁response . tojson (); ▁},▁xxbos ▁public ▁void ▁set point index ( int ▁point index ) ▁{ ▁if ▁(( point index ▁>= ▁0) ▁&& ▁( point index ▁< ▁( _ buffer . length ( ▁xxrep ▁4 ▁) ▁{ ▁_ point index ▁= ▁point index ; ▁} ▁},▁xxbos ▁public ▁void ▁change buffer data ( org . rajawali 3 d . buffer info ▁buffer info , ▁java . nio . buffer ▁new data , ▁int ▁index ) ▁{ ▁change buffer data ( buffer info , ▁new data , ▁index , ▁new data . capacity ()); ▁}\n",
       "y: CategoryList\n",
       "before,before,before,before,before\n",
       "Path: /tf/data/datasets/raw/raw_java/data00m_god-r;\n",
       "\n",
       "Test: None, model=SequentialRNN(\n",
       "  (0): MultiBatchEncoder(\n",
       "    (module): TransformerXL(\n",
       "      (encoder): Embedding(30000, 410)\n",
       "      (pos_enc): PositionalEncoding()\n",
       "      (drop_emb): Dropout(p=0.05, inplace=False)\n",
       "      (layers): ModuleList(\n",
       "        (0): DecoderLayer(\n",
       "          (mhra): MultiHeadRelativeAttention(\n",
       "            (attention): Linear(in_features=410, out_features=1230, bias=False)\n",
       "            (out): Linear(in_features=410, out_features=410, bias=False)\n",
       "            (drop_att): Dropout(p=0.05, inplace=False)\n",
       "            (drop_res): Dropout(p=0.05, inplace=False)\n",
       "            (ln): LayerNorm((410,), eps=1e-05, elementwise_affine=True)\n",
       "            (r_attn): Linear(in_features=410, out_features=410, bias=False)\n",
       "          )\n",
       "          (ff): SequentialEx(\n",
       "            (layers): ModuleList(\n",
       "              (0): Linear(in_features=410, out_features=2100, bias=True)\n",
       "              (1): ReLU(inplace=True)\n",
       "              (2): Dropout(p=0.05, inplace=False)\n",
       "              (3): Linear(in_features=2100, out_features=410, bias=True)\n",
       "              (4): Dropout(p=0.05, inplace=False)\n",
       "              (5): MergeLayer()\n",
       "              (6): LayerNorm((410,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (1): DecoderLayer(\n",
       "          (mhra): MultiHeadRelativeAttention(\n",
       "            (attention): Linear(in_features=410, out_features=1230, bias=False)\n",
       "            (out): Linear(in_features=410, out_features=410, bias=False)\n",
       "            (drop_att): Dropout(p=0.05, inplace=False)\n",
       "            (drop_res): Dropout(p=0.05, inplace=False)\n",
       "            (ln): LayerNorm((410,), eps=1e-05, elementwise_affine=True)\n",
       "            (r_attn): Linear(in_features=410, out_features=410, bias=False)\n",
       "          )\n",
       "          (ff): SequentialEx(\n",
       "            (layers): ModuleList(\n",
       "              (0): Linear(in_features=410, out_features=2100, bias=True)\n",
       "              (1): ReLU(inplace=True)\n",
       "              (2): Dropout(p=0.05, inplace=False)\n",
       "              (3): Linear(in_features=2100, out_features=410, bias=True)\n",
       "              (4): Dropout(p=0.05, inplace=False)\n",
       "              (5): MergeLayer()\n",
       "              (6): LayerNorm((410,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (2): DecoderLayer(\n",
       "          (mhra): MultiHeadRelativeAttention(\n",
       "            (attention): Linear(in_features=410, out_features=1230, bias=False)\n",
       "            (out): Linear(in_features=410, out_features=410, bias=False)\n",
       "            (drop_att): Dropout(p=0.05, inplace=False)\n",
       "            (drop_res): Dropout(p=0.05, inplace=False)\n",
       "            (ln): LayerNorm((410,), eps=1e-05, elementwise_affine=True)\n",
       "            (r_attn): Linear(in_features=410, out_features=410, bias=False)\n",
       "          )\n",
       "          (ff): SequentialEx(\n",
       "            (layers): ModuleList(\n",
       "              (0): Linear(in_features=410, out_features=2100, bias=True)\n",
       "              (1): ReLU(inplace=True)\n",
       "              (2): Dropout(p=0.05, inplace=False)\n",
       "              (3): Linear(in_features=2100, out_features=410, bias=True)\n",
       "              (4): Dropout(p=0.05, inplace=False)\n",
       "              (5): MergeLayer()\n",
       "              (6): LayerNorm((410,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (3): DecoderLayer(\n",
       "          (mhra): MultiHeadRelativeAttention(\n",
       "            (attention): Linear(in_features=410, out_features=1230, bias=False)\n",
       "            (out): Linear(in_features=410, out_features=410, bias=False)\n",
       "            (drop_att): Dropout(p=0.05, inplace=False)\n",
       "            (drop_res): Dropout(p=0.05, inplace=False)\n",
       "            (ln): LayerNorm((410,), eps=1e-05, elementwise_affine=True)\n",
       "            (r_attn): Linear(in_features=410, out_features=410, bias=False)\n",
       "          )\n",
       "          (ff): SequentialEx(\n",
       "            (layers): ModuleList(\n",
       "              (0): Linear(in_features=410, out_features=2100, bias=True)\n",
       "              (1): ReLU(inplace=True)\n",
       "              (2): Dropout(p=0.05, inplace=False)\n",
       "              (3): Linear(in_features=2100, out_features=410, bias=True)\n",
       "              (4): Dropout(p=0.05, inplace=False)\n",
       "              (5): MergeLayer()\n",
       "              (6): LayerNorm((410,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (4): DecoderLayer(\n",
       "          (mhra): MultiHeadRelativeAttention(\n",
       "            (attention): Linear(in_features=410, out_features=1230, bias=False)\n",
       "            (out): Linear(in_features=410, out_features=410, bias=False)\n",
       "            (drop_att): Dropout(p=0.05, inplace=False)\n",
       "            (drop_res): Dropout(p=0.05, inplace=False)\n",
       "            (ln): LayerNorm((410,), eps=1e-05, elementwise_affine=True)\n",
       "            (r_attn): Linear(in_features=410, out_features=410, bias=False)\n",
       "          )\n",
       "          (ff): SequentialEx(\n",
       "            (layers): ModuleList(\n",
       "              (0): Linear(in_features=410, out_features=2100, bias=True)\n",
       "              (1): ReLU(inplace=True)\n",
       "              (2): Dropout(p=0.05, inplace=False)\n",
       "              (3): Linear(in_features=2100, out_features=410, bias=True)\n",
       "              (4): Dropout(p=0.05, inplace=False)\n",
       "              (5): MergeLayer()\n",
       "              (6): LayerNorm((410,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (5): DecoderLayer(\n",
       "          (mhra): MultiHeadRelativeAttention(\n",
       "            (attention): Linear(in_features=410, out_features=1230, bias=False)\n",
       "            (out): Linear(in_features=410, out_features=410, bias=False)\n",
       "            (drop_att): Dropout(p=0.05, inplace=False)\n",
       "            (drop_res): Dropout(p=0.05, inplace=False)\n",
       "            (ln): LayerNorm((410,), eps=1e-05, elementwise_affine=True)\n",
       "            (r_attn): Linear(in_features=410, out_features=410, bias=False)\n",
       "          )\n",
       "          (ff): SequentialEx(\n",
       "            (layers): ModuleList(\n",
       "              (0): Linear(in_features=410, out_features=2100, bias=True)\n",
       "              (1): ReLU(inplace=True)\n",
       "              (2): Dropout(p=0.05, inplace=False)\n",
       "              (3): Linear(in_features=2100, out_features=410, bias=True)\n",
       "              (4): Dropout(p=0.05, inplace=False)\n",
       "              (5): MergeLayer()\n",
       "              (6): LayerNorm((410,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (6): DecoderLayer(\n",
       "          (mhra): MultiHeadRelativeAttention(\n",
       "            (attention): Linear(in_features=410, out_features=1230, bias=False)\n",
       "            (out): Linear(in_features=410, out_features=410, bias=False)\n",
       "            (drop_att): Dropout(p=0.05, inplace=False)\n",
       "            (drop_res): Dropout(p=0.05, inplace=False)\n",
       "            (ln): LayerNorm((410,), eps=1e-05, elementwise_affine=True)\n",
       "            (r_attn): Linear(in_features=410, out_features=410, bias=False)\n",
       "          )\n",
       "          (ff): SequentialEx(\n",
       "            (layers): ModuleList(\n",
       "              (0): Linear(in_features=410, out_features=2100, bias=True)\n",
       "              (1): ReLU(inplace=True)\n",
       "              (2): Dropout(p=0.05, inplace=False)\n",
       "              (3): Linear(in_features=2100, out_features=410, bias=True)\n",
       "              (4): Dropout(p=0.05, inplace=False)\n",
       "              (5): MergeLayer()\n",
       "              (6): LayerNorm((410,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (7): DecoderLayer(\n",
       "          (mhra): MultiHeadRelativeAttention(\n",
       "            (attention): Linear(in_features=410, out_features=1230, bias=False)\n",
       "            (out): Linear(in_features=410, out_features=410, bias=False)\n",
       "            (drop_att): Dropout(p=0.05, inplace=False)\n",
       "            (drop_res): Dropout(p=0.05, inplace=False)\n",
       "            (ln): LayerNorm((410,), eps=1e-05, elementwise_affine=True)\n",
       "            (r_attn): Linear(in_features=410, out_features=410, bias=False)\n",
       "          )\n",
       "          (ff): SequentialEx(\n",
       "            (layers): ModuleList(\n",
       "              (0): Linear(in_features=410, out_features=2100, bias=True)\n",
       "              (1): ReLU(inplace=True)\n",
       "              (2): Dropout(p=0.05, inplace=False)\n",
       "              (3): Linear(in_features=2100, out_features=410, bias=True)\n",
       "              (4): Dropout(p=0.05, inplace=False)\n",
       "              (5): MergeLayer()\n",
       "              (6): LayerNorm((410,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (8): DecoderLayer(\n",
       "          (mhra): MultiHeadRelativeAttention(\n",
       "            (attention): Linear(in_features=410, out_features=1230, bias=False)\n",
       "            (out): Linear(in_features=410, out_features=410, bias=False)\n",
       "            (drop_att): Dropout(p=0.05, inplace=False)\n",
       "            (drop_res): Dropout(p=0.05, inplace=False)\n",
       "            (ln): LayerNorm((410,), eps=1e-05, elementwise_affine=True)\n",
       "            (r_attn): Linear(in_features=410, out_features=410, bias=False)\n",
       "          )\n",
       "          (ff): SequentialEx(\n",
       "            (layers): ModuleList(\n",
       "              (0): Linear(in_features=410, out_features=2100, bias=True)\n",
       "              (1): ReLU(inplace=True)\n",
       "              (2): Dropout(p=0.05, inplace=False)\n",
       "              (3): Linear(in_features=2100, out_features=410, bias=True)\n",
       "              (4): Dropout(p=0.05, inplace=False)\n",
       "              (5): MergeLayer()\n",
       "              (6): LayerNorm((410,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (9): DecoderLayer(\n",
       "          (mhra): MultiHeadRelativeAttention(\n",
       "            (attention): Linear(in_features=410, out_features=1230, bias=False)\n",
       "            (out): Linear(in_features=410, out_features=410, bias=False)\n",
       "            (drop_att): Dropout(p=0.05, inplace=False)\n",
       "            (drop_res): Dropout(p=0.05, inplace=False)\n",
       "            (ln): LayerNorm((410,), eps=1e-05, elementwise_affine=True)\n",
       "            (r_attn): Linear(in_features=410, out_features=410, bias=False)\n",
       "          )\n",
       "          (ff): SequentialEx(\n",
       "            (layers): ModuleList(\n",
       "              (0): Linear(in_features=410, out_features=2100, bias=True)\n",
       "              (1): ReLU(inplace=True)\n",
       "              (2): Dropout(p=0.05, inplace=False)\n",
       "              (3): Linear(in_features=2100, out_features=410, bias=True)\n",
       "              (4): Dropout(p=0.05, inplace=False)\n",
       "              (5): MergeLayer()\n",
       "              (6): LayerNorm((410,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (10): DecoderLayer(\n",
       "          (mhra): MultiHeadRelativeAttention(\n",
       "            (attention): Linear(in_features=410, out_features=1230, bias=False)\n",
       "            (out): Linear(in_features=410, out_features=410, bias=False)\n",
       "            (drop_att): Dropout(p=0.05, inplace=False)\n",
       "            (drop_res): Dropout(p=0.05, inplace=False)\n",
       "            (ln): LayerNorm((410,), eps=1e-05, elementwise_affine=True)\n",
       "            (r_attn): Linear(in_features=410, out_features=410, bias=False)\n",
       "          )\n",
       "          (ff): SequentialEx(\n",
       "            (layers): ModuleList(\n",
       "              (0): Linear(in_features=410, out_features=2100, bias=True)\n",
       "              (1): ReLU(inplace=True)\n",
       "              (2): Dropout(p=0.05, inplace=False)\n",
       "              (3): Linear(in_features=2100, out_features=410, bias=True)\n",
       "              (4): Dropout(p=0.05, inplace=False)\n",
       "              (5): MergeLayer()\n",
       "              (6): LayerNorm((410,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (11): DecoderLayer(\n",
       "          (mhra): MultiHeadRelativeAttention(\n",
       "            (attention): Linear(in_features=410, out_features=1230, bias=False)\n",
       "            (out): Linear(in_features=410, out_features=410, bias=False)\n",
       "            (drop_att): Dropout(p=0.05, inplace=False)\n",
       "            (drop_res): Dropout(p=0.05, inplace=False)\n",
       "            (ln): LayerNorm((410,), eps=1e-05, elementwise_affine=True)\n",
       "            (r_attn): Linear(in_features=410, out_features=410, bias=False)\n",
       "          )\n",
       "          (ff): SequentialEx(\n",
       "            (layers): ModuleList(\n",
       "              (0): Linear(in_features=410, out_features=2100, bias=True)\n",
       "              (1): ReLU(inplace=True)\n",
       "              (2): Dropout(p=0.05, inplace=False)\n",
       "              (3): Linear(in_features=2100, out_features=410, bias=True)\n",
       "              (4): Dropout(p=0.05, inplace=False)\n",
       "              (5): MergeLayer()\n",
       "              (6): LayerNorm((410,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (1): PoolingLinearClassifier(\n",
       "    (layers): Sequential(\n",
       "      (0): BatchNorm1d(1230, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (1): Dropout(p=0.05, inplace=False)\n",
       "      (2): Linear(in_features=1230, out_features=50, bias=True)\n",
       "      (3): ReLU(inplace=True)\n",
       "      (4): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (5): Dropout(p=0.1, inplace=False)\n",
       "      (6): Linear(in_features=50, out_features=2, bias=True)\n",
       "    )\n",
       "  )\n",
       "), opt_func=functools.partial(<class 'torch.optim.adam.Adam'>, betas=(0.9, 0.99)), loss_func=FlattenedLoss of CrossEntropyLoss(), metrics=[<function accuracy at 0x7f152760dbf8>], true_wd=True, bn_wd=True, wd=0.01, train_bn=True, path=PosixPath('/tf/data/datasets/raw/raw_java/data00m_god-r'), model_dir='models', callback_fns=[functools.partial(<class 'fastai.basic_train.Recorder'>, add_time=True, silent=False)], callbacks=[...], layer_groups=[Sequential(\n",
       "  (0): Embedding(30000, 410)\n",
       "), Sequential(\n",
       "  (0): DecoderLayer(\n",
       "    (mhra): MultiHeadRelativeAttention(\n",
       "      (attention): Linear(in_features=410, out_features=1230, bias=False)\n",
       "      (out): Linear(in_features=410, out_features=410, bias=False)\n",
       "      (drop_att): Dropout(p=0.05, inplace=False)\n",
       "      (drop_res): Dropout(p=0.05, inplace=False)\n",
       "      (ln): LayerNorm((410,), eps=1e-05, elementwise_affine=True)\n",
       "      (r_attn): Linear(in_features=410, out_features=410, bias=False)\n",
       "    )\n",
       "    (ff): SequentialEx(\n",
       "      (layers): ModuleList(\n",
       "        (0): Linear(in_features=410, out_features=2100, bias=True)\n",
       "        (1): ReLU(inplace=True)\n",
       "        (2): Dropout(p=0.05, inplace=False)\n",
       "        (3): Linear(in_features=2100, out_features=410, bias=True)\n",
       "        (4): Dropout(p=0.05, inplace=False)\n",
       "        (5): MergeLayer()\n",
       "        (6): LayerNorm((410,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (1): DecoderLayer(\n",
       "    (mhra): MultiHeadRelativeAttention(\n",
       "      (attention): Linear(in_features=410, out_features=1230, bias=False)\n",
       "      (out): Linear(in_features=410, out_features=410, bias=False)\n",
       "      (drop_att): Dropout(p=0.05, inplace=False)\n",
       "      (drop_res): Dropout(p=0.05, inplace=False)\n",
       "      (ln): LayerNorm((410,), eps=1e-05, elementwise_affine=True)\n",
       "      (r_attn): Linear(in_features=410, out_features=410, bias=False)\n",
       "    )\n",
       "    (ff): SequentialEx(\n",
       "      (layers): ModuleList(\n",
       "        (0): Linear(in_features=410, out_features=2100, bias=True)\n",
       "        (1): ReLU(inplace=True)\n",
       "        (2): Dropout(p=0.05, inplace=False)\n",
       "        (3): Linear(in_features=2100, out_features=410, bias=True)\n",
       "        (4): Dropout(p=0.05, inplace=False)\n",
       "        (5): MergeLayer()\n",
       "        (6): LayerNorm((410,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (2): DecoderLayer(\n",
       "    (mhra): MultiHeadRelativeAttention(\n",
       "      (attention): Linear(in_features=410, out_features=1230, bias=False)\n",
       "      (out): Linear(in_features=410, out_features=410, bias=False)\n",
       "      (drop_att): Dropout(p=0.05, inplace=False)\n",
       "      (drop_res): Dropout(p=0.05, inplace=False)\n",
       "      (ln): LayerNorm((410,), eps=1e-05, elementwise_affine=True)\n",
       "      (r_attn): Linear(in_features=410, out_features=410, bias=False)\n",
       "    )\n",
       "    (ff): SequentialEx(\n",
       "      (layers): ModuleList(\n",
       "        (0): Linear(in_features=410, out_features=2100, bias=True)\n",
       "        (1): ReLU(inplace=True)\n",
       "        (2): Dropout(p=0.05, inplace=False)\n",
       "        (3): Linear(in_features=2100, out_features=410, bias=True)\n",
       "        (4): Dropout(p=0.05, inplace=False)\n",
       "        (5): MergeLayer()\n",
       "        (6): LayerNorm((410,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (3): DecoderLayer(\n",
       "    (mhra): MultiHeadRelativeAttention(\n",
       "      (attention): Linear(in_features=410, out_features=1230, bias=False)\n",
       "      (out): Linear(in_features=410, out_features=410, bias=False)\n",
       "      (drop_att): Dropout(p=0.05, inplace=False)\n",
       "      (drop_res): Dropout(p=0.05, inplace=False)\n",
       "      (ln): LayerNorm((410,), eps=1e-05, elementwise_affine=True)\n",
       "      (r_attn): Linear(in_features=410, out_features=410, bias=False)\n",
       "    )\n",
       "    (ff): SequentialEx(\n",
       "      (layers): ModuleList(\n",
       "        (0): Linear(in_features=410, out_features=2100, bias=True)\n",
       "        (1): ReLU(inplace=True)\n",
       "        (2): Dropout(p=0.05, inplace=False)\n",
       "        (3): Linear(in_features=2100, out_features=410, bias=True)\n",
       "        (4): Dropout(p=0.05, inplace=False)\n",
       "        (5): MergeLayer()\n",
       "        (6): LayerNorm((410,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (4): ParameterModule()\n",
       "  (5): ParameterModule()\n",
       "), Sequential(\n",
       "  (0): DecoderLayer(\n",
       "    (mhra): MultiHeadRelativeAttention(\n",
       "      (attention): Linear(in_features=410, out_features=1230, bias=False)\n",
       "      (out): Linear(in_features=410, out_features=410, bias=False)\n",
       "      (drop_att): Dropout(p=0.05, inplace=False)\n",
       "      (drop_res): Dropout(p=0.05, inplace=False)\n",
       "      (ln): LayerNorm((410,), eps=1e-05, elementwise_affine=True)\n",
       "      (r_attn): Linear(in_features=410, out_features=410, bias=False)\n",
       "    )\n",
       "    (ff): SequentialEx(\n",
       "      (layers): ModuleList(\n",
       "        (0): Linear(in_features=410, out_features=2100, bias=True)\n",
       "        (1): ReLU(inplace=True)\n",
       "        (2): Dropout(p=0.05, inplace=False)\n",
       "        (3): Linear(in_features=2100, out_features=410, bias=True)\n",
       "        (4): Dropout(p=0.05, inplace=False)\n",
       "        (5): MergeLayer()\n",
       "        (6): LayerNorm((410,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (1): DecoderLayer(\n",
       "    (mhra): MultiHeadRelativeAttention(\n",
       "      (attention): Linear(in_features=410, out_features=1230, bias=False)\n",
       "      (out): Linear(in_features=410, out_features=410, bias=False)\n",
       "      (drop_att): Dropout(p=0.05, inplace=False)\n",
       "      (drop_res): Dropout(p=0.05, inplace=False)\n",
       "      (ln): LayerNorm((410,), eps=1e-05, elementwise_affine=True)\n",
       "      (r_attn): Linear(in_features=410, out_features=410, bias=False)\n",
       "    )\n",
       "    (ff): SequentialEx(\n",
       "      (layers): ModuleList(\n",
       "        (0): Linear(in_features=410, out_features=2100, bias=True)\n",
       "        (1): ReLU(inplace=True)\n",
       "        (2): Dropout(p=0.05, inplace=False)\n",
       "        (3): Linear(in_features=2100, out_features=410, bias=True)\n",
       "        (4): Dropout(p=0.05, inplace=False)\n",
       "        (5): MergeLayer()\n",
       "        (6): LayerNorm((410,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (2): DecoderLayer(\n",
       "    (mhra): MultiHeadRelativeAttention(\n",
       "      (attention): Linear(in_features=410, out_features=1230, bias=False)\n",
       "      (out): Linear(in_features=410, out_features=410, bias=False)\n",
       "      (drop_att): Dropout(p=0.05, inplace=False)\n",
       "      (drop_res): Dropout(p=0.05, inplace=False)\n",
       "      (ln): LayerNorm((410,), eps=1e-05, elementwise_affine=True)\n",
       "      (r_attn): Linear(in_features=410, out_features=410, bias=False)\n",
       "    )\n",
       "    (ff): SequentialEx(\n",
       "      (layers): ModuleList(\n",
       "        (0): Linear(in_features=410, out_features=2100, bias=True)\n",
       "        (1): ReLU(inplace=True)\n",
       "        (2): Dropout(p=0.05, inplace=False)\n",
       "        (3): Linear(in_features=2100, out_features=410, bias=True)\n",
       "        (4): Dropout(p=0.05, inplace=False)\n",
       "        (5): MergeLayer()\n",
       "        (6): LayerNorm((410,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (3): DecoderLayer(\n",
       "    (mhra): MultiHeadRelativeAttention(\n",
       "      (attention): Linear(in_features=410, out_features=1230, bias=False)\n",
       "      (out): Linear(in_features=410, out_features=410, bias=False)\n",
       "      (drop_att): Dropout(p=0.05, inplace=False)\n",
       "      (drop_res): Dropout(p=0.05, inplace=False)\n",
       "      (ln): LayerNorm((410,), eps=1e-05, elementwise_affine=True)\n",
       "      (r_attn): Linear(in_features=410, out_features=410, bias=False)\n",
       "    )\n",
       "    (ff): SequentialEx(\n",
       "      (layers): ModuleList(\n",
       "        (0): Linear(in_features=410, out_features=2100, bias=True)\n",
       "        (1): ReLU(inplace=True)\n",
       "        (2): Dropout(p=0.05, inplace=False)\n",
       "        (3): Linear(in_features=2100, out_features=410, bias=True)\n",
       "        (4): Dropout(p=0.05, inplace=False)\n",
       "        (5): MergeLayer()\n",
       "        (6): LayerNorm((410,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "), Sequential(\n",
       "  (0): DecoderLayer(\n",
       "    (mhra): MultiHeadRelativeAttention(\n",
       "      (attention): Linear(in_features=410, out_features=1230, bias=False)\n",
       "      (out): Linear(in_features=410, out_features=410, bias=False)\n",
       "      (drop_att): Dropout(p=0.05, inplace=False)\n",
       "      (drop_res): Dropout(p=0.05, inplace=False)\n",
       "      (ln): LayerNorm((410,), eps=1e-05, elementwise_affine=True)\n",
       "      (r_attn): Linear(in_features=410, out_features=410, bias=False)\n",
       "    )\n",
       "    (ff): SequentialEx(\n",
       "      (layers): ModuleList(\n",
       "        (0): Linear(in_features=410, out_features=2100, bias=True)\n",
       "        (1): ReLU(inplace=True)\n",
       "        (2): Dropout(p=0.05, inplace=False)\n",
       "        (3): Linear(in_features=2100, out_features=410, bias=True)\n",
       "        (4): Dropout(p=0.05, inplace=False)\n",
       "        (5): MergeLayer()\n",
       "        (6): LayerNorm((410,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (1): DecoderLayer(\n",
       "    (mhra): MultiHeadRelativeAttention(\n",
       "      (attention): Linear(in_features=410, out_features=1230, bias=False)\n",
       "      (out): Linear(in_features=410, out_features=410, bias=False)\n",
       "      (drop_att): Dropout(p=0.05, inplace=False)\n",
       "      (drop_res): Dropout(p=0.05, inplace=False)\n",
       "      (ln): LayerNorm((410,), eps=1e-05, elementwise_affine=True)\n",
       "      (r_attn): Linear(in_features=410, out_features=410, bias=False)\n",
       "    )\n",
       "    (ff): SequentialEx(\n",
       "      (layers): ModuleList(\n",
       "        (0): Linear(in_features=410, out_features=2100, bias=True)\n",
       "        (1): ReLU(inplace=True)\n",
       "        (2): Dropout(p=0.05, inplace=False)\n",
       "        (3): Linear(in_features=2100, out_features=410, bias=True)\n",
       "        (4): Dropout(p=0.05, inplace=False)\n",
       "        (5): MergeLayer()\n",
       "        (6): LayerNorm((410,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (2): DecoderLayer(\n",
       "    (mhra): MultiHeadRelativeAttention(\n",
       "      (attention): Linear(in_features=410, out_features=1230, bias=False)\n",
       "      (out): Linear(in_features=410, out_features=410, bias=False)\n",
       "      (drop_att): Dropout(p=0.05, inplace=False)\n",
       "      (drop_res): Dropout(p=0.05, inplace=False)\n",
       "      (ln): LayerNorm((410,), eps=1e-05, elementwise_affine=True)\n",
       "      (r_attn): Linear(in_features=410, out_features=410, bias=False)\n",
       "    )\n",
       "    (ff): SequentialEx(\n",
       "      (layers): ModuleList(\n",
       "        (0): Linear(in_features=410, out_features=2100, bias=True)\n",
       "        (1): ReLU(inplace=True)\n",
       "        (2): Dropout(p=0.05, inplace=False)\n",
       "        (3): Linear(in_features=2100, out_features=410, bias=True)\n",
       "        (4): Dropout(p=0.05, inplace=False)\n",
       "        (5): MergeLayer()\n",
       "        (6): LayerNorm((410,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (3): DecoderLayer(\n",
       "    (mhra): MultiHeadRelativeAttention(\n",
       "      (attention): Linear(in_features=410, out_features=1230, bias=False)\n",
       "      (out): Linear(in_features=410, out_features=410, bias=False)\n",
       "      (drop_att): Dropout(p=0.05, inplace=False)\n",
       "      (drop_res): Dropout(p=0.05, inplace=False)\n",
       "      (ln): LayerNorm((410,), eps=1e-05, elementwise_affine=True)\n",
       "      (r_attn): Linear(in_features=410, out_features=410, bias=False)\n",
       "    )\n",
       "    (ff): SequentialEx(\n",
       "      (layers): ModuleList(\n",
       "        (0): Linear(in_features=410, out_features=2100, bias=True)\n",
       "        (1): ReLU(inplace=True)\n",
       "        (2): Dropout(p=0.05, inplace=False)\n",
       "        (3): Linear(in_features=2100, out_features=410, bias=True)\n",
       "        (4): Dropout(p=0.05, inplace=False)\n",
       "        (5): MergeLayer()\n",
       "        (6): LayerNorm((410,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "), Sequential(\n",
       "  (0): PoolingLinearClassifier(\n",
       "    (layers): Sequential(\n",
       "      (0): BatchNorm1d(1230, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (1): Dropout(p=0.05, inplace=False)\n",
       "      (2): Linear(in_features=1230, out_features=50, bias=True)\n",
       "      (3): ReLU(inplace=True)\n",
       "      (4): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (5): Dropout(p=0.1, inplace=False)\n",
       "      (6): Linear(in_features=50, out_features=2, bias=True)\n",
       "    )\n",
       "  )\n",
       ")], add_time=True, silent=False, cb_fns_registered=False)\n",
       "alpha: 2.0\n",
       "beta: 1.0], layer_groups=[Sequential(\n",
       "  (0): Embedding(30000, 410)\n",
       "), Sequential(\n",
       "  (0): DecoderLayer(\n",
       "    (mhra): MultiHeadRelativeAttention(\n",
       "      (attention): Linear(in_features=410, out_features=1230, bias=False)\n",
       "      (out): Linear(in_features=410, out_features=410, bias=False)\n",
       "      (drop_att): Dropout(p=0.05, inplace=False)\n",
       "      (drop_res): Dropout(p=0.05, inplace=False)\n",
       "      (ln): LayerNorm((410,), eps=1e-05, elementwise_affine=True)\n",
       "      (r_attn): Linear(in_features=410, out_features=410, bias=False)\n",
       "    )\n",
       "    (ff): SequentialEx(\n",
       "      (layers): ModuleList(\n",
       "        (0): Linear(in_features=410, out_features=2100, bias=True)\n",
       "        (1): ReLU(inplace=True)\n",
       "        (2): Dropout(p=0.05, inplace=False)\n",
       "        (3): Linear(in_features=2100, out_features=410, bias=True)\n",
       "        (4): Dropout(p=0.05, inplace=False)\n",
       "        (5): MergeLayer()\n",
       "        (6): LayerNorm((410,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (1): DecoderLayer(\n",
       "    (mhra): MultiHeadRelativeAttention(\n",
       "      (attention): Linear(in_features=410, out_features=1230, bias=False)\n",
       "      (out): Linear(in_features=410, out_features=410, bias=False)\n",
       "      (drop_att): Dropout(p=0.05, inplace=False)\n",
       "      (drop_res): Dropout(p=0.05, inplace=False)\n",
       "      (ln): LayerNorm((410,), eps=1e-05, elementwise_affine=True)\n",
       "      (r_attn): Linear(in_features=410, out_features=410, bias=False)\n",
       "    )\n",
       "    (ff): SequentialEx(\n",
       "      (layers): ModuleList(\n",
       "        (0): Linear(in_features=410, out_features=2100, bias=True)\n",
       "        (1): ReLU(inplace=True)\n",
       "        (2): Dropout(p=0.05, inplace=False)\n",
       "        (3): Linear(in_features=2100, out_features=410, bias=True)\n",
       "        (4): Dropout(p=0.05, inplace=False)\n",
       "        (5): MergeLayer()\n",
       "        (6): LayerNorm((410,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (2): DecoderLayer(\n",
       "    (mhra): MultiHeadRelativeAttention(\n",
       "      (attention): Linear(in_features=410, out_features=1230, bias=False)\n",
       "      (out): Linear(in_features=410, out_features=410, bias=False)\n",
       "      (drop_att): Dropout(p=0.05, inplace=False)\n",
       "      (drop_res): Dropout(p=0.05, inplace=False)\n",
       "      (ln): LayerNorm((410,), eps=1e-05, elementwise_affine=True)\n",
       "      (r_attn): Linear(in_features=410, out_features=410, bias=False)\n",
       "    )\n",
       "    (ff): SequentialEx(\n",
       "      (layers): ModuleList(\n",
       "        (0): Linear(in_features=410, out_features=2100, bias=True)\n",
       "        (1): ReLU(inplace=True)\n",
       "        (2): Dropout(p=0.05, inplace=False)\n",
       "        (3): Linear(in_features=2100, out_features=410, bias=True)\n",
       "        (4): Dropout(p=0.05, inplace=False)\n",
       "        (5): MergeLayer()\n",
       "        (6): LayerNorm((410,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (3): DecoderLayer(\n",
       "    (mhra): MultiHeadRelativeAttention(\n",
       "      (attention): Linear(in_features=410, out_features=1230, bias=False)\n",
       "      (out): Linear(in_features=410, out_features=410, bias=False)\n",
       "      (drop_att): Dropout(p=0.05, inplace=False)\n",
       "      (drop_res): Dropout(p=0.05, inplace=False)\n",
       "      (ln): LayerNorm((410,), eps=1e-05, elementwise_affine=True)\n",
       "      (r_attn): Linear(in_features=410, out_features=410, bias=False)\n",
       "    )\n",
       "    (ff): SequentialEx(\n",
       "      (layers): ModuleList(\n",
       "        (0): Linear(in_features=410, out_features=2100, bias=True)\n",
       "        (1): ReLU(inplace=True)\n",
       "        (2): Dropout(p=0.05, inplace=False)\n",
       "        (3): Linear(in_features=2100, out_features=410, bias=True)\n",
       "        (4): Dropout(p=0.05, inplace=False)\n",
       "        (5): MergeLayer()\n",
       "        (6): LayerNorm((410,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (4): ParameterModule()\n",
       "  (5): ParameterModule()\n",
       "), Sequential(\n",
       "  (0): DecoderLayer(\n",
       "    (mhra): MultiHeadRelativeAttention(\n",
       "      (attention): Linear(in_features=410, out_features=1230, bias=False)\n",
       "      (out): Linear(in_features=410, out_features=410, bias=False)\n",
       "      (drop_att): Dropout(p=0.05, inplace=False)\n",
       "      (drop_res): Dropout(p=0.05, inplace=False)\n",
       "      (ln): LayerNorm((410,), eps=1e-05, elementwise_affine=True)\n",
       "      (r_attn): Linear(in_features=410, out_features=410, bias=False)\n",
       "    )\n",
       "    (ff): SequentialEx(\n",
       "      (layers): ModuleList(\n",
       "        (0): Linear(in_features=410, out_features=2100, bias=True)\n",
       "        (1): ReLU(inplace=True)\n",
       "        (2): Dropout(p=0.05, inplace=False)\n",
       "        (3): Linear(in_features=2100, out_features=410, bias=True)\n",
       "        (4): Dropout(p=0.05, inplace=False)\n",
       "        (5): MergeLayer()\n",
       "        (6): LayerNorm((410,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (1): DecoderLayer(\n",
       "    (mhra): MultiHeadRelativeAttention(\n",
       "      (attention): Linear(in_features=410, out_features=1230, bias=False)\n",
       "      (out): Linear(in_features=410, out_features=410, bias=False)\n",
       "      (drop_att): Dropout(p=0.05, inplace=False)\n",
       "      (drop_res): Dropout(p=0.05, inplace=False)\n",
       "      (ln): LayerNorm((410,), eps=1e-05, elementwise_affine=True)\n",
       "      (r_attn): Linear(in_features=410, out_features=410, bias=False)\n",
       "    )\n",
       "    (ff): SequentialEx(\n",
       "      (layers): ModuleList(\n",
       "        (0): Linear(in_features=410, out_features=2100, bias=True)\n",
       "        (1): ReLU(inplace=True)\n",
       "        (2): Dropout(p=0.05, inplace=False)\n",
       "        (3): Linear(in_features=2100, out_features=410, bias=True)\n",
       "        (4): Dropout(p=0.05, inplace=False)\n",
       "        (5): MergeLayer()\n",
       "        (6): LayerNorm((410,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (2): DecoderLayer(\n",
       "    (mhra): MultiHeadRelativeAttention(\n",
       "      (attention): Linear(in_features=410, out_features=1230, bias=False)\n",
       "      (out): Linear(in_features=410, out_features=410, bias=False)\n",
       "      (drop_att): Dropout(p=0.05, inplace=False)\n",
       "      (drop_res): Dropout(p=0.05, inplace=False)\n",
       "      (ln): LayerNorm((410,), eps=1e-05, elementwise_affine=True)\n",
       "      (r_attn): Linear(in_features=410, out_features=410, bias=False)\n",
       "    )\n",
       "    (ff): SequentialEx(\n",
       "      (layers): ModuleList(\n",
       "        (0): Linear(in_features=410, out_features=2100, bias=True)\n",
       "        (1): ReLU(inplace=True)\n",
       "        (2): Dropout(p=0.05, inplace=False)\n",
       "        (3): Linear(in_features=2100, out_features=410, bias=True)\n",
       "        (4): Dropout(p=0.05, inplace=False)\n",
       "        (5): MergeLayer()\n",
       "        (6): LayerNorm((410,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (3): DecoderLayer(\n",
       "    (mhra): MultiHeadRelativeAttention(\n",
       "      (attention): Linear(in_features=410, out_features=1230, bias=False)\n",
       "      (out): Linear(in_features=410, out_features=410, bias=False)\n",
       "      (drop_att): Dropout(p=0.05, inplace=False)\n",
       "      (drop_res): Dropout(p=0.05, inplace=False)\n",
       "      (ln): LayerNorm((410,), eps=1e-05, elementwise_affine=True)\n",
       "      (r_attn): Linear(in_features=410, out_features=410, bias=False)\n",
       "    )\n",
       "    (ff): SequentialEx(\n",
       "      (layers): ModuleList(\n",
       "        (0): Linear(in_features=410, out_features=2100, bias=True)\n",
       "        (1): ReLU(inplace=True)\n",
       "        (2): Dropout(p=0.05, inplace=False)\n",
       "        (3): Linear(in_features=2100, out_features=410, bias=True)\n",
       "        (4): Dropout(p=0.05, inplace=False)\n",
       "        (5): MergeLayer()\n",
       "        (6): LayerNorm((410,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "), Sequential(\n",
       "  (0): DecoderLayer(\n",
       "    (mhra): MultiHeadRelativeAttention(\n",
       "      (attention): Linear(in_features=410, out_features=1230, bias=False)\n",
       "      (out): Linear(in_features=410, out_features=410, bias=False)\n",
       "      (drop_att): Dropout(p=0.05, inplace=False)\n",
       "      (drop_res): Dropout(p=0.05, inplace=False)\n",
       "      (ln): LayerNorm((410,), eps=1e-05, elementwise_affine=True)\n",
       "      (r_attn): Linear(in_features=410, out_features=410, bias=False)\n",
       "    )\n",
       "    (ff): SequentialEx(\n",
       "      (layers): ModuleList(\n",
       "        (0): Linear(in_features=410, out_features=2100, bias=True)\n",
       "        (1): ReLU(inplace=True)\n",
       "        (2): Dropout(p=0.05, inplace=False)\n",
       "        (3): Linear(in_features=2100, out_features=410, bias=True)\n",
       "        (4): Dropout(p=0.05, inplace=False)\n",
       "        (5): MergeLayer()\n",
       "        (6): LayerNorm((410,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (1): DecoderLayer(\n",
       "    (mhra): MultiHeadRelativeAttention(\n",
       "      (attention): Linear(in_features=410, out_features=1230, bias=False)\n",
       "      (out): Linear(in_features=410, out_features=410, bias=False)\n",
       "      (drop_att): Dropout(p=0.05, inplace=False)\n",
       "      (drop_res): Dropout(p=0.05, inplace=False)\n",
       "      (ln): LayerNorm((410,), eps=1e-05, elementwise_affine=True)\n",
       "      (r_attn): Linear(in_features=410, out_features=410, bias=False)\n",
       "    )\n",
       "    (ff): SequentialEx(\n",
       "      (layers): ModuleList(\n",
       "        (0): Linear(in_features=410, out_features=2100, bias=True)\n",
       "        (1): ReLU(inplace=True)\n",
       "        (2): Dropout(p=0.05, inplace=False)\n",
       "        (3): Linear(in_features=2100, out_features=410, bias=True)\n",
       "        (4): Dropout(p=0.05, inplace=False)\n",
       "        (5): MergeLayer()\n",
       "        (6): LayerNorm((410,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (2): DecoderLayer(\n",
       "    (mhra): MultiHeadRelativeAttention(\n",
       "      (attention): Linear(in_features=410, out_features=1230, bias=False)\n",
       "      (out): Linear(in_features=410, out_features=410, bias=False)\n",
       "      (drop_att): Dropout(p=0.05, inplace=False)\n",
       "      (drop_res): Dropout(p=0.05, inplace=False)\n",
       "      (ln): LayerNorm((410,), eps=1e-05, elementwise_affine=True)\n",
       "      (r_attn): Linear(in_features=410, out_features=410, bias=False)\n",
       "    )\n",
       "    (ff): SequentialEx(\n",
       "      (layers): ModuleList(\n",
       "        (0): Linear(in_features=410, out_features=2100, bias=True)\n",
       "        (1): ReLU(inplace=True)\n",
       "        (2): Dropout(p=0.05, inplace=False)\n",
       "        (3): Linear(in_features=2100, out_features=410, bias=True)\n",
       "        (4): Dropout(p=0.05, inplace=False)\n",
       "        (5): MergeLayer()\n",
       "        (6): LayerNorm((410,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (3): DecoderLayer(\n",
       "    (mhra): MultiHeadRelativeAttention(\n",
       "      (attention): Linear(in_features=410, out_features=1230, bias=False)\n",
       "      (out): Linear(in_features=410, out_features=410, bias=False)\n",
       "      (drop_att): Dropout(p=0.05, inplace=False)\n",
       "      (drop_res): Dropout(p=0.05, inplace=False)\n",
       "      (ln): LayerNorm((410,), eps=1e-05, elementwise_affine=True)\n",
       "      (r_attn): Linear(in_features=410, out_features=410, bias=False)\n",
       "    )\n",
       "    (ff): SequentialEx(\n",
       "      (layers): ModuleList(\n",
       "        (0): Linear(in_features=410, out_features=2100, bias=True)\n",
       "        (1): ReLU(inplace=True)\n",
       "        (2): Dropout(p=0.05, inplace=False)\n",
       "        (3): Linear(in_features=2100, out_features=410, bias=True)\n",
       "        (4): Dropout(p=0.05, inplace=False)\n",
       "        (5): MergeLayer()\n",
       "        (6): LayerNorm((410,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "), Sequential(\n",
       "  (0): PoolingLinearClassifier(\n",
       "    (layers): Sequential(\n",
       "      (0): BatchNorm1d(1230, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (1): Dropout(p=0.05, inplace=False)\n",
       "      (2): Linear(in_features=1230, out_features=50, bias=True)\n",
       "      (3): ReLU(inplace=True)\n",
       "      (4): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (5): Dropout(p=0.1, inplace=False)\n",
       "      (6): Linear(in_features=50, out_features=2, bias=True)\n",
       "    )\n",
       "  )\n",
       ")], add_time=True, silent=False, cb_fns_registered=False)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learn = text_classifier_learner(data_clas, TransformerXL, drop_mult=0.5)\n",
    "learn.load_encoder('fine_tuned_enc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.lr_find()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.recorder.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.fit_one_cycle(5, 2e-2, moms=(0.8,0.7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.save('first')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.load('first');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.freeze_to(-2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 48.00 MiB (GPU 0; 23.65 GiB total capacity; 20.35 GiB already allocated; 30.56 MiB free; 1.33 GiB cached)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/fastai/basic_train.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(epochs, learn, callbacks, metrics)\u001b[0m\n\u001b[1;32m    100\u001b[0m                 \u001b[0mxb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcb_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 101\u001b[0;31m                 \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcb_handler\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    102\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mcb_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_batch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/fastai/basic_train.py\u001b[0m in \u001b[0;36mloss_batch\u001b[0;34m(model, xb, yb, loss_func, opt, cb_handler)\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_listy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0myb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0myb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0myb\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m     \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mxb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m     \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcb_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_loss_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    546\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 547\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    548\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     91\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_modules\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 92\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    546\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 547\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    548\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/fastai/text/learner.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    262\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbptt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 263\u001b[0;31m             \u001b[0mr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbptt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    264\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m>\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msl\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_len\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    546\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 547\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    548\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/fastai/text/models/transformer.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    221\u001b[0m             \u001b[0mmem\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhidden\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmem_len\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 222\u001b[0;31m             \u001b[0minp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpos_enc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mu\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmem\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    223\u001b[0m             \u001b[0mhids\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    546\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 547\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    548\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/fastai/text/models/transformer.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, mask, **kwargs)\u001b[0m\n\u001b[1;32m    146\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 147\u001b[0;31m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mff\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmhra\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    148\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    546\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 547\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    548\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/fastai/text/models/transformer.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, mask, **kwargs)\u001b[0m\n\u001b[1;32m     44\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mln\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop_res\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply_attention\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/fastai/text/models/transformer.py\u001b[0m in \u001b[0;36m_apply_attention\u001b[0;34m(self, x, r, u, v, mask, mem)\u001b[0m\n\u001b[1;32m    110\u001b[0m         \u001b[0mAC\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwq\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mu\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mwk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 111\u001b[0;31m         \u001b[0mBD\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_line_shift\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwq\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwkr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    112\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscale\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mattn_score\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mAC\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mBD\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0md_head\u001b[0m \u001b[0;34m**\u001b[0m \u001b[0;36m0.5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/fastai/text/models/transformer.py\u001b[0m in \u001b[0;36m_line_shift\u001b[0;34m(x, mask)\u001b[0m\n\u001b[1;32m     83\u001b[0m     \u001b[0mbs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnh\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 84\u001b[0;31m     \u001b[0mx_pad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnew_zeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnh\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     85\u001b[0m     \u001b[0mx_shift\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx_pad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnh\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mp\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview_as\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 38.00 MiB (GPU 0; 23.65 GiB total capacity; 20.35 GiB already allocated; 30.56 MiB free; 1.33 GiB cached)",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-d81c6bd29d71>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlr_find\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/fastai/train.py\u001b[0m in \u001b[0;36mlr_find\u001b[0;34m(learn, start_lr, end_lr, num_it, stop_div, wd)\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[0mcb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLRFinder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlearn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart_lr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend_lr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_it\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstop_div\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0mepochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mceil\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_it\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_dl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m     \u001b[0mlearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart_lr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcb\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwd\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mwd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m def to_fp16(learn:Learner, loss_scale:float=None, max_noskip:int=1000, dynamic:bool=True, clip:float=None,\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/fastai/basic_train.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, epochs, lr, wd, callbacks)\u001b[0m\n\u001b[1;32m    200\u001b[0m         \u001b[0mcallbacks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mcb\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mcb\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallback_fns\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mlistify\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdefaults\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextra_callback_fns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mlistify\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcb_fns_registered\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 202\u001b[0;31m         \u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    203\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    204\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcreate_opt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mFloats\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwd\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mFloats\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m->\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/fastai/basic_train.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(epochs, learn, callbacks, metrics)\u001b[0m\n\u001b[1;32m    110\u001b[0m         \u001b[0mexception\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m         \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 112\u001b[0;31m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mcb_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexception\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m loss_func_name2activ = {'cross_entropy_loss': F.softmax, 'nll_loss': torch.exp, 'poisson_nll_loss': torch.exp,\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/fastai/callback.py\u001b[0m in \u001b[0;36mon_train_end\u001b[0;34m(self, exception)\u001b[0m\n\u001b[1;32m    321\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mon_train_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexception\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbool\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mException\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m->\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    322\u001b[0m         \u001b[0;34m\"Handle end of training, `exception` is an `Exception` or False if no exceptions during training.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 323\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'train_end'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexception\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mexception\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    324\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    325\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/fastai/callback.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, cb_name, call_mets, **kwargs)\u001b[0m\n\u001b[1;32m    249\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcall_mets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mmet\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_and_update\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcb_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 251\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mcb\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_and_update\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcb_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    252\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    253\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mset_dl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdl\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mDataLoader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/fastai/callback.py\u001b[0m in \u001b[0;36m_call_and_update\u001b[0;34m(self, cb, cb_name, **kwargs)\u001b[0m\n\u001b[1;32m    239\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_and_update\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcb_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m->\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    240\u001b[0m         \u001b[0;34m\"Call `cb_name` on `cb` and update the inner state.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 241\u001b[0;31m         \u001b[0mnew\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mifnone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34mf'on_{cb_name}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    242\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnew\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    243\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/fastai/callbacks/lr_finder.py\u001b[0m in \u001b[0;36mon_train_end\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m     33\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mon_train_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mAny\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m->\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0;34m\"Cleanup learn model weights disturbed during LRFinder exploration.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'tmp'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpurge\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'reset'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mcb\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/fastai/basic_train.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(self, file, device, strict, with_opt, purge, remove_module)\u001b[0m\n\u001b[1;32m    267\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mdevice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'cuda'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    268\u001b[0m         \u001b[0msource\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_dir\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;34mf'{file}.pth'\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mis_pathlike\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 269\u001b[0;31m         \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msource\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    270\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'model'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'opt'\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    271\u001b[0m             \u001b[0mmodel_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'model'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, **pickle_load_args)\u001b[0m\n\u001b[1;32m    384\u001b[0m         \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    385\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 386\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_load\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpickle_module\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpickle_load_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    387\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    388\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mnew_fd\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36m_load\u001b[0;34m(f, map_location, pickle_module, **pickle_load_args)\u001b[0m\n\u001b[1;32m    571\u001b[0m     \u001b[0munpickler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle_module\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mUnpickler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpickle_load_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    572\u001b[0m     \u001b[0munpickler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpersistent_load\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpersistent_load\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 573\u001b[0;31m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0munpickler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    574\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    575\u001b[0m     \u001b[0mdeserialized_storage_keys\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle_module\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpickle_load_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36mpersistent_load\u001b[0;34m(saved_id)\u001b[0m\n\u001b[1;32m    534\u001b[0m                 \u001b[0mobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    535\u001b[0m                 \u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_torch_load_uninitialized\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 536\u001b[0;31m                 \u001b[0mdeserialized_objects\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mroot_key\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrestore_location\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    537\u001b[0m             \u001b[0mstorage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdeserialized_objects\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mroot_key\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    538\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mview_metadata\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36mrestore_location\u001b[0;34m(storage, location)\u001b[0m\n\u001b[1;32m    404\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmap_location\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    405\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mrestore_location\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstorage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 406\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mdefault_restore_location\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstorage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmap_location\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    407\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    408\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mrestore_location\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstorage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36mdefault_restore_location\u001b[0;34m(storage, location)\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mdefault_restore_location\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstorage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_package_registry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 119\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstorage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    120\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mresult\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36m_cuda_deserialize\u001b[0;34m(obj, location)\u001b[0m\n\u001b[1;32m     97\u001b[0m             \u001b[0mstorage_type\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mstorage_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    100\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 48.00 MiB (GPU 0; 23.65 GiB total capacity; 20.35 GiB already allocated; 30.56 MiB free; 1.33 GiB cached)"
     ]
    }
   ],
   "source": [
    "learn.lr_find()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.recorder.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.fit_one_cycle(1, slice(1e-2/(2.6**4),1e-2), moms=(0.8,0.7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.save('second')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.load('second');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.freeze_to(-3)\n",
    "learn.fit_one_cycle(1, slice(5e-3/(2.6**4),5e-3), moms=(0.8,0.7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.save('third')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.load('third');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.unfreeze()\n",
    "learn.fit_one_cycle(2, slice(1e-3/(2.6**4),1e-3), moms=(0.8,0.7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.predict(\"I really loved that movie, it was awesome!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
